#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import random
from datetime import datetime, timedelta, timezone

import streamlit as st
import pandas as pd

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

from supabase import create_client, Client

# -----------------------------
# Streamlit ê¸°ë³¸ ì„¤ì •
# -----------------------------
st.set_page_config(
    page_title="YouTubeê²€ìƒ‰ê¸°",
    page_icon="ğŸ”",
    layout="wide",
)

# ìƒë‹¨ ì—¬ë°± ì¡°ì • + DataEditor ì•„ì´ì½˜(3ì  ë©”ë‰´ ë“±) ìˆ¨ê¸°ê¸°
st.markdown(
    """
<style>
.block-container { padding-top: 3rem !important; }
/* DataEditor ë‚´ ì•„ì´ì½˜ ë²„íŠ¼(3ì  ë©”ë‰´ ë“±) ìˆ¨ê¸°ê¸° */
[data-testid="stDataFrame"] button[kind="icon"] {
    display: none !important;
}
</style>
""",
    unsafe_allow_html=True,
)

# -----------------------------
# ë¡œê·¸ì¸ ì„¤ì •
# -----------------------------
LOGIN_ID_ENV = os.getenv("LOGIN_ID", "")
LOGIN_PW_ENV = os.getenv("LOGIN_PW", "")

st.session_state.setdefault("logged_in", False)
st.session_state.setdefault("login_id", LOGIN_ID_ENV or "")
st.session_state.setdefault("login_pw", LOGIN_PW_ENV or "")

def check_login(id_text: str, pw_text: str) -> bool:
    if LOGIN_ID_ENV and LOGIN_PW_ENV:
        return (id_text == LOGIN_ID_ENV) and (pw_text == LOGIN_PW_ENV)
    return True

if not st.session_state["logged_in"]:
    st.markdown("### ğŸ”’ YouTubeê²€ìƒ‰ê¸° ë¡œê·¸ì¸")

    id_input = st.text_input("ë¡œê·¸ì¸ ID", value=st.session_state["login_id"])
    pw_input = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password", value=st.session_state["login_pw"])

    col_l, col_r = st.columns([1, 3])
    with col_l:
        login_btn = st.button("ë¡œê·¸ì¸", type="primary", use_container_width=True)

    if login_btn:
        st.session_state["login_id"] = id_input
        st.session_state["login_pw"] = pw_input
        if check_login(id_input, pw_input):
            st.session_state["logged_in"] = True
            st.success("ë¡œê·¸ì¸ ì„±ê³µ!")
            st.rerun()
        else:
            st.error("ë¡œê·¸ì¸ ì‹¤íŒ¨. ID ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    st.stop()

# ë¡œê·¸ì¸ ì´í›„ í™”ë©´ ì œëª© (ì‘ê²Œ)
st.markdown("### ğŸ” YouTubeê²€ìƒ‰ê¸°")

# -----------------------------
# ê³µí†µ ìƒìˆ˜/í™˜ê²½
# -----------------------------
KST = timezone(timedelta(hours=9))
WEEKDAY_KO = ["ì›”ìš”ì¼","í™”ìš”ì¼","ìˆ˜ìš”ì¼","ëª©ìš”ì¼","ê¸ˆìš”ì¼","í† ìš”ì¼","ì¼ìš”ì¼"]

COUNTRY_LANG_MAP = {
    "ë¯¸êµ­": ("US", "en"),
    "ì˜êµ­": ("GB", "en"),
    "í•œêµ­": ("KR", "ko"),
    "ì¼ë³¸": ("JP", "ja"),
    "ì¸ë„": ("IN", "en"),
    "ë¸Œë¼ì§ˆ": ("BR", "pt"),
    "ìºë‚˜ë‹¤": ("CA", "en"),
    "ë…ì¼": ("DE", "de"),
    "í”„ë‘ìŠ¤": ("FR", "fr"),
    "ë©•ì‹œì½”": ("MX", "es"),
    "í˜¸ì£¼": ("AU", "en"),
    "ìŠ¤í˜ì¸": ("ES", "es"),
    "ì´íƒˆë¦¬ì•„": ("IT", "it"),
    "ë„¤ëœë€ë“œ": ("NL", "nl"),
    "í„°í‚¤": ("TR", "tr"),
    "ì¸ë„ë„¤ì‹œì•„": ("ID", "id"),
    "íƒœêµ­": ("TH", "th"),
    "ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„": ("SA", "ar"),
    "ì•„ëì—ë¯¸ë¦¬íŠ¸": ("AE", "ar"),
}
COUNTRY_LIST = list(COUNTRY_LANG_MAP.keys())

# íŠ¸ë Œë“œ ì¹´í…Œê³ ë¦¬ (videoCategoryId)
TREND_CATEGORY_MAP = {
    "ì „ì²´": None,
    "ì˜í™”/ì• ë‹ˆë©”ì´ì…˜": "1",
    "ìë™ì°¨/êµí†µ": "2",
    "ìŒì•…": "10",
    "ìŠ¤í¬ì¸ ": "17",
    "ê²Œì„": "20",
    "ì¸ë¬¼/ë¸”ë¡œê·¸": "22",
    "ì½”ë¯¸ë””": "23",
    "ì—”í„°í…Œì¸ë¨¼íŠ¸": "24",
    "ë‰´ìŠ¤/ì •ì¹˜": "25",
    "ë…¸í•˜ìš°/ìŠ¤íƒ€ì¼": "26",
    "êµìœ¡": "27",
    "ê³¼í•™/ê¸°ìˆ ": "28",
}

# -----------------------------
# Supabase í´ë¼ì´ì–¸íŠ¸
# -----------------------------
@st.cache_resource
def get_supabase_client() -> Client | None:
    url = st.secrets.get("SUPABASE_URL")
    key = st.secrets.get("SUPABASE_SERVICE_KEY")
    if not url or not key:
        return None
    return create_client(url, key)

SUPABASE_BUCKET = st.secrets.get("SUPABASE_BUCKET", "yts-config")
supabase = get_supabase_client()

def _load_json(filename: str, default):
    if supabase is None:
        return default
    try:
        res = supabase.storage.from_(SUPABASE_BUCKET).download(filename)
        if res is None:
            return default
        if isinstance(res, bytes):
            text = res.decode("utf-8")
        else:
            text = str(res)
        return json.loads(text)
    except Exception:
        return default

def _save_json(filename: str, data):
    if supabase is None:
        return
    try:
        payload = json.dumps(data, ensure_ascii=False, indent=2).encode("utf-8")
        supabase.storage.from_(SUPABASE_BUCKET).upload(
            path=filename,
            file=payload,
            file_options={"content-type": "application/json", "x-upsert": "true"},
        )
    except Exception as e:
        st.warning(f"Supabase ì €ì¥ ì˜¤ë¥˜({filename}): {e}")

# -----------------------------
# ê²½ë¡œ/íŒŒì¼ëª… ì •ì˜
# -----------------------------
KEYWORD_LOG_PATH  = "yts_keyword_log.json"
QUOTA_PATH        = "yts_quota_usage.json"

# -----------------------------
# API í‚¤
# -----------------------------
def get_current_api_key() -> str:
    keys = st.secrets.get("YOUTUBE_API_KEYS")
    if isinstance(keys, list) and keys:
        return str(keys[0]).strip()
    if isinstance(keys, str) and keys.strip():
        first = keys.strip().splitlines()[0]
        return first.strip()

    single = st.secrets.get("YOUTUBE_API_KEY")
    if isinstance(single, str) and single.strip():
        return single.strip()

    return ""

def get_youtube_client():
    key = get_current_api_key()
    if not key:
        raise RuntimeError(
            "YouTube API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
            "â–¶ .streamlit/secrets.toml ì— YOUTUBE_API_KEYS ë˜ëŠ” YOUTUBE_API_KEY ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
        )
    try:
        return build("youtube", "v3", developerKey=key, cache_discovery=False)
    except TypeError:
        return build("youtube", "v3", developerKey=key)

# -----------------------------
# ì¿¼í„° ê¸°ë¡
# -----------------------------
def _load_quota_map():
    return _load_json(QUOTA_PATH, {})

def _save_quota_map(data: dict):
    _save_json(QUOTA_PATH, data)

def quota_today_key():
    return datetime.now(KST).strftime("%Y-%m-%d")

def get_today_quota_total() -> int:
    data = _load_quota_map()
    return int(data.get(quota_today_key(), 0))

def add_quota_usage(units: int):
    if units <= 0:
        return
    data = _load_quota_map()
    key = quota_today_key()
    data[key] = int(data.get(key, 0)) + int(units)
    _save_quota_map(data)

# -----------------------------
# ìµœê·¼ ê²€ìƒ‰ í‚¤ì›Œë“œ
# -----------------------------
def _load_keyword_log():
    return _load_json(KEYWORD_LOG_PATH, [])

def _save_keyword_log(entries: list):
    _save_json(KEYWORD_LOG_PATH, entries)

def append_keyword_log(query: str):
    q = (query or "").strip()
    if not q:
        return
    entries = _load_keyword_log()
    now = datetime.now(KST).isoformat(timespec="seconds")
    entries.append({"ts": now, "q": q})
    _save_keyword_log(entries)

def get_recent_keywords(limit: int = 30):
    entries = _load_keyword_log()
    out = []
    for item in entries:
        ts = item.get("ts")
        q  = item.get("q")
        if not ts or not q:
            continue
        try:
            dt = datetime.fromisoformat(ts)
        except Exception:
            continue
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=KST)
        out.append((dt, q))
    out.sort(key=lambda x: x[0], reverse=True)
    return out[:limit]

# -----------------------------
# ì‹œê°„/í˜•ì‹ ìœ í‹¸
# -----------------------------
def parse_published_at_to_kst(published_iso: str) -> datetime:
    dt_utc = datetime.fromisoformat(published_iso.replace("Z", "+00:00"))
    return dt_utc.astimezone(KST)

def human_elapsed_days_hours(later: datetime, earlier: datetime) -> tuple[int, int]:
    delta = later - earlier
    if delta.total_seconds() < 0:
        return 0, 0
    days = delta.days
    hours = delta.seconds // 3600
    return days, hours

def published_after_from_label(label: str):
    label = label.strip()
    now_utc = datetime.now(timezone.utc)
    if label == "ì œí•œì—†ìŒ":
        return None
    if label.endswith("ì¼"):
        days = int(label[:-1])
        dt = now_utc - timedelta(days=days)
    else:
        return None
    return dt.isoformat(timespec="seconds").replace("+00:00", "Z")

def parse_duration_iso8601(iso_dur: str) -> int:
    h = m = s = 0
    if not iso_dur or not iso_dur.startswith("PT"):
        return 0
    num = ""
    for ch in iso_dur[2:]:
        if ch.isdigit():
            num += ch
        else:
            if ch == "H" and num:
                h = int(num); num = ""
            elif ch == "M" and num:
                m = int(num); num = ""
            elif ch == "S" and num:
                s = int(num); num = ""
    return h*3600 + m*60 + s

def format_duration_hms(seconds: int) -> str:
    if seconds <= 0:
        return "0:00"
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    if h > 0:
        return f"{h}:{m:02d}:{s:02d}"
    return f"{m}:{s:02d}"

def duration_filter_ok(seconds: int, label: str) -> bool:
    if label == "ì „ì²´": return True
    if label == "ì‡¼ì¸ ": return seconds < 60
    if label == "ë¡±í¼": return seconds >= 60
    if label == "1~20ë¶„": return 60 <= seconds < 20*60
    if label == "20~40ë¶„": return 20*60 <= seconds < 40*60
    if label == "40~60ë¶„": return 40*60 <= seconds < 60*60
    if label == "60ë¶„ì´ìƒ": return seconds >= 60*60
    return True

def parse_min_views(text: str) -> int:
    digits = text.replace(",", "").replace(" ", "").replace("ë§Œ", "0000")
    try:
        return int(digits)
    except Exception:
        return 0

# -----------------------------
# ë“±ê¸‰ ê³„ì‚° (A~H)
# -----------------------------
def calc_grade(clicks_per_hour: int) -> str:
    v = clicks_per_hour
    if v >= 5000: return "A"
    if v >= 2000: return "B"
    if v >= 1000: return "C"
    if v >= 500:  return "D"
    if v >= 300:  return "E"
    if v >= 100:  return "F"
    if v >= 50:   return "G"
    return "H"

# -----------------------------
# YouTube API í˜¸ì¶œ í•¨ìˆ˜
# -----------------------------
def search_videos(
    query: str,
    min_views: int,
    api_period_label: str,
    duration_label: str,
    max_fetch: int,
    region_code: str | None,
    lang_code: str | None,
):
    youtube = get_youtube_client()
    published_after = published_after_from_label(api_period_label)

    cost_used = 0
    breakdown = {"search.list": 0, "videos.list": 0}
    max_fetch = max(1, min(int(max_fetch or 100), 5000))

    results_tmp = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        kwargs = dict(
            q=query,
            part="id",
            type="video",
            maxResults=take,
        )
        if published_after:
            kwargs["publishedAfter"] = published_after
        if region_code:
            kwargs["regionCode"] = region_code
        if lang_code:
            kwargs["relevanceLanguage"] = lang_code
        if next_token:
            kwargs["pageToken"] = next_token

        try:
            search_response = youtube.search().list(**kwargs).execute()
            cost_used += 100; breakdown["search.list"] += 100
        except HttpError as e:
            raise RuntimeError(f"Search API ì˜¤ë¥˜: {e}")

        page_ids = [
            it["id"]["videoId"]
            for it in search_response.get("items", [])
            if "id" in it and "videoId" in it["id"]
        ]
        if not page_ids:
            break

        try:
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(page_ids),
            ).execute()
            cost_used += 1; breakdown["videos.list"] += 1
        except HttpError as e:
            raise RuntimeError(f"Videos API ì˜¤ë¥˜: {e}")

        items = video_response.get("items", [])
        for item in items:
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}

            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

            thumbs = snip.get("thumbnails", {}) or {}
            thumb_url = (
                (thumbs.get("maxres") or {}).get("url")
                or (thumbs.get("standard") or {}).get("url")
                or (thumbs.get("high") or {}).get("url")
                or (thumbs.get("medium") or {}).get("url")
                or (thumbs.get("default") or {}).get("url")
                or ""
            )

            if view_count < min_views:
                continue
            if not duration_filter_ok(duration_sec, duration_label):
                continue

            results_tmp.append({
                "title": title,
                "views": view_count,
                "published_at_iso": published_at_iso,
                "url": url,
                "duration_sec": duration_sec,
                "channel_title": snip.get("channelTitle", ""),
                "thumbnail_url": thumb_url,
            })

        fetched += len(page_ids)
        next_token = search_response.get("nextPageToken")
        if not next_token:
            break

    return results_tmp, cost_used, breakdown

def search_channels_by_keyword(
    keyword: str,
    max_results: int,
    region_code: str | None,
    lang_code: str | None,
):
    youtube = get_youtube_client()
    take = max(1, min(max_results, 50))
    kwargs = dict(
        q=keyword,
        part="id",
        type="channel",
        maxResults=take,
    )
    if region_code:
        kwargs["regionCode"] = region_code
    if lang_code:
        kwargs["relevanceLanguage"] = lang_code

    try:
        search_response = youtube.search().list(**kwargs).execute()
        cost_used = 100
    except HttpError as e:
        raise RuntimeError(f"Channel search API ì˜¤ë¥˜: {e}")

    ch_ids = [
        it["id"]["channelId"]
        for it in search_response.get("items", [])
        if "id" in it and "channelId" in it["id"]
    ]
    if not ch_ids:
        return [], cost_used, {"search.list": 100, "channels.list": 0}

    try:
        ch_resp = youtube.channels().list(
            part="snippet,statistics",
            id=",".join(ch_ids),
        ).execute()
        cost_used += 1
    except HttpError as e:
        raise RuntimeError(f"Channels API ì˜¤ë¥˜: {e}")

    results = []
    for c in ch_resp.get("items", []):
        cid = c.get("id", "")
        sn = c.get("snippet", {}) or {}
        stt = c.get("statistics", {}) or {}
        subs = int(stt.get("subscriberCount", 0)) if stt.get("subscriberCount") is not None else None
        total_views = int(stt.get("viewCount", 0))
        videos = int(stt.get("videoCount", 0))
        url = f"https://www.youtube.com/channel/{cid}" if cid else ""

        thumbs = sn.get("thumbnails", {}) or {}
        thumb_url = (
            (thumbs.get("high") or {}).get("url")
            or (thumbs.get("medium") or {}).get("url")
            or (thumbs.get("default") or {}).get("url")
            or ""
        )

        results.append({
            "channel_title": sn.get("title", ""),
            "subs": subs,
            "total_views": total_views,
            "videos": videos,
            "url": url,
            "thumbnail_url": thumb_url,
        })

    results.sort(key=lambda r: (r["subs"] or 0), reverse=True)
    return results, cost_used, {"search.list": 100, "channels.list": 1}

def search_videos_in_channel_by_name(
    channel_name: str,
    min_views: int,
    api_period_label: str,
    duration_label: str,
    max_fetch: int,
    region_code: str | None,
    lang_code: str | None,
):
    youtube = get_youtube_client()
    published_after = published_after_from_label(api_period_label)
    cost_used = 0
    breakdown = {"search.list": 0, "videos.list": 0}

    kwargs_ch = dict(
        q=channel_name,
        part="id,snippet",
        type="channel",
        maxResults=1,
    )
    if region_code:
        kwargs_ch["regionCode"] = region_code
    if lang_code:
        kwargs_ch["relevanceLanguage"] = lang_code

    try:
        ch_resp = youtube.search().list(**kwargs_ch).execute()
        cost_used += 100; breakdown["search.list"] += 100
    except HttpError as e:
        raise RuntimeError(f"ì±„ë„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

    items = ch_resp.get("items", [])
    if not items:
        return [], cost_used, breakdown

    channel_id = items[0]["id"]["channelId"]

    max_fetch = max(1, min(int(max_fetch or 100), 5000))
    results_tmp = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        kwargs = dict(
            part="id",
            type="video",
            channelId=channel_id,
            maxResults=take,
            order="date",
        )
        if published_after:
            kwargs["publishedAfter"] = published_after
        if region_code:
            kwargs["regionCode"] = region_code
        if lang_code:
            kwargs["relevanceLanguage"] = lang_code
        if next_token:
            kwargs["pageToken"] = next_token

        try:
            v_search = youtube.search().list(**kwargs).execute()
            cost_used += 100; breakdown["search.list"] += 100
        except HttpError as e:
            raise RuntimeError(f"ì±„ë„ ì˜ìƒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

        page_ids = [
            it["id"]["videoId"]
            for it in v_search.get("items", [])
            if "id" in it and "videoId" in it["id"]
        ]
        if not page_ids:
            break

        try:
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(page_ids),
            ).execute()
            cost_used += 1; breakdown["videos.list"] += 1
        except HttpError as e:
            raise RuntimeError(f"Videos API ì˜¤ë¥˜: {e}")

        items2 = video_response.get("items", [])
        for item in items2:
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}

            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

            thumbs = snip.get("thumbnails", {}) or {}
            thumb_url = (
                (thumbs.get("maxres") or {}).get("url")
                or (thumbs.get("standard") or {}).get("url")
                or (thumbs.get("high") or {}).get("url")
                or (thumbs.get("medium") or {}).get("url")
                or (thumbs.get("default") or {}).get("url")
                or ""
            )

            if view_count < min_views:
                continue
            if not duration_filter_ok(duration_sec, duration_label):
                continue

            results_tmp.append({
                "title": title,
                "views": view_count,
                "published_at_iso": published_at_iso,
                "url": url,
                "duration_sec": duration_sec,
                "channel_title": snip.get("channelTitle", ""),
                "thumbnail_url": thumb_url,
            })

        fetched += len(page_ids)
        next_token = v_search.get("nextPageToken")
        if not next_token:
            break

    return results_tmp, cost_used, breakdown

def search_trending_videos(
    max_results: int,
    region_code: str | None,
    video_category_id: str | None,
):
    youtube = get_youtube_client()
    take = max(1, min(max_results, 50))
    kwargs = dict(
        part="snippet,statistics,contentDetails",
        chart="mostPopular",
        maxResults=take,
    )
    if region_code:
        kwargs["regionCode"] = region_code
    if video_category_id:
        kwargs["videoCategoryId"] = video_category_id

    try:
        resp = youtube.videos().list(**kwargs).execute()
        cost_used = 1
    except HttpError as e:
        raise RuntimeError(f"íŠ¸ë Œë“œ API ì˜¤ë¥˜: {e}")

    results = []
    for item in resp.get("items", []):
        vid = item.get("id", "")
        snip = item.get("snippet", {}) or {}
        stats = item.get("statistics", {}) or {}
        cdet = item.get("contentDetails", {}) or {}

        title = snip.get("title", "")
        published_at_iso = snip.get("publishedAt", "")
        view_count = int(stats.get("viewCount", 0))
        url = f"https://www.youtube.com/watch?v={vid}"
        duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

        thumbs = snip.get("thumbnails", {}) or {}
        thumb_url = (
            (thumbs.get("maxres") or {}).get("url")
            or (thumbs.get("standard") or {}).get("url")
            or (thumbs.get("high") or {}).get("url")
            or (thumbs.get("medium") or {}).get("url")
            or (thumbs.get("default") or {}).get("url")
            or ""
        )

        results.append({
            "title": title,
            "views": view_count,
            "published_at_iso": published_at_iso,
            "url": url,
            "duration_sec": duration_sec,
            "channel_title": snip.get("channelTitle", ""),
            "thumbnail_url": thumb_url,
        })
    return results, cost_used, {"videos.list": 1}

# -----------------------------
# Sidebar UI
# -----------------------------
st.sidebar.header("ê²€ìƒ‰")

# ëœë¤ ê²€ìƒ‰ ë²„íŠ¼ (ë§¨ ìœ„)
do_random_search = st.sidebar.button("ğŸ² ëœë¤ê²€ìƒ‰", use_container_width=True, key="btn_random")

# ê³µí†µ ê²€ìƒ‰ì–´
search_query = st.sidebar.text_input("ê²€ìƒ‰ì–´", "")

# ê²€ìƒ‰ ë²„íŠ¼ 3ê°œ (ì„¸ë¡œ ë‚˜ì—´)
do_general_search = st.sidebar.button("ğŸ” ì¼ë°˜ê²€ìƒ‰", use_container_width=True, key="btn_general")
do_channel_videos_search = st.sidebar.button("ğŸ¬ ì±„ë„ì˜ìƒê²€ìƒ‰", use_container_width=True, key="btn_channel_videos")
do_channel_list_search = st.sidebar.button("ğŸ“ˆ í‚¤ì›Œë“œì±„ë„ê²€ìƒ‰", use_container_width=True, key="btn_channel_list")

# íŠ¸ë Œë“œ ê²€ìƒ‰ì€ disclosure ì•ˆ
with st.sidebar.expander("ğŸ”¥ íŠ¸ë Œë“œ ê²€ìƒ‰", expanded=False):
    trend_category_label = st.selectbox(
        "ì¹´í…Œê³ ë¦¬",
        list(TREND_CATEGORY_MAP.keys()),
        index=0,
        key="trend_category_label",
    )
    do_trend_search = st.button("íŠ¸ë Œë“œ ê²€ìƒ‰ ì‹¤í–‰", use_container_width=True, key="btn_trend")

st.sidebar.markdown("---")

# ë³´ê¸° ëª¨ë“œ (ê¸°ë³¸: ê·¸ë¦¬ë“œ ë·°)
view_mode_label = st.sidebar.radio(
    "ë³´ê¸° ëª¨ë“œ",
    options=["ê·¸ë¦¬ë“œ ë·°", "ë¦¬ìŠ¤íŠ¸ ë·°", "ì‡¼ì¸  ë·°"],
    index=0,
    horizontal=True,
)

if view_mode_label == "ê·¸ë¦¬ë“œ ë·°":
    view_mode = "grid"
elif view_mode_label == "ë¦¬ìŠ¤íŠ¸ ë·°":
    view_mode = "list"
else:
    view_mode = "shorts"

# ì •ë ¬ ë°©ì‹ (í•­ìƒ í¼ì³ì§„ ìƒíƒœ, ê¸°ë³¸ ë“±ê¸‰/ì˜¤ë¦„ì°¨ìˆœ)
st.session_state.setdefault("sort_key", "ë“±ê¸‰")
st.session_state.setdefault("sort_asc", True)

with st.sidebar.expander("ì •ë ¬ ë°©ì‹", expanded=True):
    sort_key = st.selectbox(
        "ì •ë ¬ ê¸°ì¤€",
        ["ë“±ê¸‰", "ì˜ìƒì¡°íšŒìˆ˜", "ì‹œê°„ë‹¹í´ë¦­", "ì—…ë¡œë“œì‹œê°", "êµ¬ë…ììˆ˜", "ì±„ë„ì¡°íšŒìˆ˜", "ì±„ë„ì˜ìƒìˆ˜"],
        index=0,
        key="sort_key_ui",
    )
    sort_dir = st.radio(
        "ì •ë ¬ ë°©í–¥",
        ["ì˜¤ë¦„ì°¨ìˆœ", "ë‚´ë¦¼ì°¨ìˆœ"],
        index=0 if st.session_state["sort_asc"] else 1,
        horizontal=True,
        key="sort_dir_ui",
    )
    st.session_state["sort_key"] = sort_key
    st.session_state["sort_asc"] = (sort_dir == "ì˜¤ë¦„ì°¨ìˆœ")

st.sidebar.markdown("---")

# ì„¸ë¶€ í•„í„° (í•­ìƒ í¼ì¹¨)
with st.sidebar.expander("âš™ ì„¸ë¶€ í•„í„°", expanded=True):
    api_period = st.selectbox(
        "ì„œë²„ ê²€ìƒ‰ê¸°ê°„ (YouTube API)",
        ["ì œí•œì—†ìŒ","90ì¼","150ì¼","365ì¼","730ì¼","1095ì¼","1825ì¼","3650ì¼"],
        index=1,
        key="api_period",
    )
    upload_period = st.selectbox(
        "ì—…ë¡œë“œ ê¸°ê°„(í´ë¼ì´ì–¸íŠ¸ í•„í„°)",
        ["ì œí•œì—†ìŒ","1ì¼","3ì¼","7ì¼","14ì¼","30ì¼","60ì¼","90ì¼","180ì¼","365ì¼"],
        index=6,
        key="upload_period",
    )
    min_views_label = st.selectbox(
        "ìµœì†Œ ì¡°íšŒìˆ˜",
        ["5,000","10,000","25,000","50,000","100,000","200,000","500,000","1,000,000"],
        index=0,
        key="min_views_label",
    )
    duration_label = st.selectbox(
        "ì˜ìƒ ê¸¸ì´",
        ["ì „ì²´","ì‡¼ì¸ ","ë¡±í¼","1~20ë¶„","20~40ë¶„","40~60ë¶„","60ë¶„ì´ìƒ"],
        index=0,
        key="duration_label",
    )
    max_fetch = st.number_input(
        "ëª¨ë“  ê²€ìƒ‰ì—ì„œ ê°€ì ¸ì˜¬ ìµœëŒ€ ê°œìˆ˜",
        1, 5000, 50, step=10,
        key="max_fetch",
    )
    country_name = st.selectbox("ê²€ìƒ‰ìš© êµ­ê°€/ì–¸ì–´", COUNTRY_LIST, index=0, key="country_for_search")
    region_code, lang_code = COUNTRY_LANG_MAP[country_name]

status_placeholder = st.empty()

# -----------------------------
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# -----------------------------
if "results_df" not in st.session_state:
    st.session_state.results_df = None
    st.session_state.last_search_time = None
    st.session_state.search_mode = None

def apply_client_filters(df: pd.DataFrame, upload_period: str, min_views_label: str) -> pd.DataFrame:
    if upload_period != "ì œí•œì—†ìŒ" and "ì—…ë¡œë“œì‹œê°" in df.columns:
        days = int(upload_period.replace("ì¼",""))
        cutoff = datetime.now(KST) - timedelta(days=days)
        df = df[df["ì—…ë¡œë“œì‹œê°"] >= cutoff]
    min_views = parse_min_views(min_views_label)
    if "ì˜ìƒì¡°íšŒìˆ˜" in df.columns:
        df = df[df["ì˜ìƒì¡°íšŒìˆ˜"] >= min_views]
    return df

def sort_dataframe(df: pd.DataFrame, mode: str, sort_key: str, ascending: bool) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    if sort_key not in df.columns:
        if mode in ("general", "trend", "random_trend", "channel_videos"):
            key_fallback = "ë“±ê¸‰" if "ë“±ê¸‰" in df.columns else None
        else:
            key_fallback = "êµ¬ë…ììˆ˜" if "êµ¬ë…ììˆ˜" in df.columns else None
        if not key_fallback:
            return df
        sort_key = key_fallback

    if sort_key == "ë“±ê¸‰":
        order = ["A","B","C","D","E","F","G","H"]
        cat = pd.Categorical(df["ë“±ê¸‰"], categories=order, ordered=True)
        df = df.assign(_grade_cat=cat)
        df = df.sort_values("_grade_cat", ascending=ascending, kind="mergesort")
        return df.drop(columns=["_grade_cat"])

    if sort_key in ["ì˜ìƒì¡°íšŒìˆ˜","ì‹œê°„ë‹¹í´ë¦­"]:
        return df.sort_values(sort_key, ascending=ascending, kind="mergesort")

    if sort_key == "ì—…ë¡œë“œì‹œê°":
        return df.sort_values("ì—…ë¡œë“œì‹œê°", ascending=ascending, kind="mergesort")

    if sort_key in ["êµ¬ë…ììˆ˜","ì±„ë„ì¡°íšŒìˆ˜","ì±„ë„ì˜ìƒìˆ˜"]:
        tmp = df[sort_key].astype(str).str.replace(",","").str.replace(" ","")
        num = pd.to_numeric(tmp, errors="coerce").fillna(0)
        df = df.assign(_num=num)
        df = df.sort_values("_num", ascending=ascending, kind="mergesort")
        return df.drop(columns=["_num"])

    return df.sort_values(sort_key, ascending=ascending, kind="mergesort")

# -----------------------------
# ê²€ìƒ‰ ì‹¤í–‰ ë¡œì§
# -----------------------------
try:
    mode_triggered = None
    if do_random_search:
        mode_triggered = "random_trend"
    elif do_general_search:
        mode_triggered = "general"
    elif do_trend_search:
        mode_triggered = "trend"
    elif do_channel_videos_search:
        mode_triggered = "channel_videos"
    elif do_channel_list_search:
        mode_triggered = "channel_list"

    if mode_triggered is not None:
        search_dt = datetime.now(KST)

        # ----- ëœë¤ íŠ¸ë Œë“œ ê²€ìƒ‰ -----
        if mode_triggered == "random_trend":
            rand_country_label = random.choice(COUNTRY_LIST)
            rand_region_code, _ = COUNTRY_LANG_MAP[rand_country_label]
            rand_cat_label = random.choice(list(TREND_CATEGORY_MAP.keys()))
            rand_cat_id = TREND_CATEGORY_MAP[rand_cat_label]

            append_keyword_log(f"[random]{rand_country_label}/{rand_cat_label}")
            status_placeholder.info(
                f"ëœë¤ íŠ¸ë Œë“œ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘... (êµ­ê°€: {rand_country_label}, ì¹´í…Œê³ ë¦¬: {rand_cat_label})"
            )
            raw_results, cost_used, breakdown = search_trending_videos(
                max_results=max_fetch,
                region_code=rand_region_code,
                video_category_id=rand_cat_id,
            )
            add_quota_usage(cost_used)

            if not raw_results:
                st.session_state.results_df = None
                st.session_state.search_mode = "random_trend"
                status_placeholder.info("ëœë¤ íŠ¸ë Œë“œ ê²°ê³¼ 0ê±´")
            else:
                rows = []
                for r in raw_results:
                    pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                    d, h = human_elapsed_days_hours(search_dt, pub_kst)
                    total_hours = max(1, d*24 + h)
                    cph = int(round(r["views"] / total_hours))
                    rows.append({
                        "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),
                        "ì±„ë„ëª…": r["channel_title"],
                        "ë“±ê¸‰": calc_grade(cph),
                        "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                        "ì‹œê°„ë‹¹í´ë¦­": cph,
                        "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                        "ì—…ë¡œë“œì‹œê°": pub_kst,
                        "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                        "ì œëª©": r["title"],
                        "ë§í¬URL": r["url"],
                    })
                df = pd.DataFrame(rows)
                if not df.empty:
                    df = apply_client_filters(df, upload_period, min_views_label)
                st.session_state.results_df = df
                st.session_state.last_search_time = search_dt
                st.session_state.search_mode = "random_trend"
                status_placeholder.success(
                    f"[ëœë¤ íŠ¸ë Œë“œ] ê²°ê³¼: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                )

        # ----- ì¼ë°˜ ê²€ìƒ‰ -----
        elif mode_triggered == "general":
            base_query = (search_query or "").strip()
            if not base_query:
                st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                append_keyword_log(base_query)
                status_placeholder.info("ì¼ë°˜ ì˜ìƒ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
                raw_results, cost_used, breakdown = search_videos(
                    query=base_query,
                    min_views=parse_min_views(min_views_label),
                    api_period_label=api_period,
                    duration_label=duration_label,
                    max_fetch=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
                add_quota_usage(cost_used)

                if not raw_results:
                    st.session_state.results_df = None
                    st.session_state.search_mode = "general"
                    status_placeholder.info("ì„œë²„ ê²°ê³¼ 0ê±´")
                else:
                    rows = []
                    for r in raw_results:
                        pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                        d, h = human_elapsed_days_hours(search_dt, pub_kst)
                        total_hours = max(1, d*24 + h)
                        cph = int(round(r["views"] / total_hours))
                        rows.append({
                            "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),
                            "ì±„ë„ëª…": r["channel_title"],
                            "ë“±ê¸‰": calc_grade(cph),
                            "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                            "ì‹œê°„ë‹¹í´ë¦­": cph,
                            "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                            "ì—…ë¡œë“œì‹œê°": pub_kst,
                            "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                            "ì œëª©": r["title"],
                            "ë§í¬URL": r["url"],
                        })
                    df = pd.DataFrame(rows)
                    if not df.empty:
                        df = apply_client_filters(df, upload_period, min_views_label)
                    st.session_state.results_df = df
                    st.session_state.last_search_time = search_dt
                    st.session_state.search_mode = "general"
                    status_placeholder.success(
                        f"[ì¼ë°˜ ê²€ìƒ‰] ì„œë²„ ê²°ê³¼: {len(raw_results):,}ê±´ / í•„í„° í›„: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                    )

        # ----- íŠ¸ë Œë“œ ê²€ìƒ‰ (ì„ íƒ ì¹´í…Œê³ ë¦¬) -----
        elif mode_triggered == "trend":
            trend_cat_id = TREND_CATEGORY_MAP.get(trend_category_label)
            append_keyword_log(f"[trend]{trend_category_label}")
            status_placeholder.info("íŠ¸ë Œë“œ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
            raw_results, cost_used, breakdown = search_trending_videos(
                max_results=max_fetch,
                region_code=region_code,
                video_category_id=trend_cat_id,
            )
            add_quota_usage(cost_used)

            if not raw_results:
                st.session_state.results_df = None
                st.session_state.search_mode = "trend"
                status_placeholder.info("íŠ¸ë Œë“œ ê²°ê³¼ 0ê±´")
            else:
                rows = []
                for r in raw_results:
                    pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                    d, h = human_elapsed_days_hours(search_dt, pub_kst)
                    total_hours = max(1, d*24 + h)
                    cph = int(round(r["views"] / total_hours))
                    rows.append({
                        "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),
                        "ì±„ë„ëª…": r["channel_title"],
                        "ë“±ê¸‰": calc_grade(cph),
                        "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                        "ì‹œê°„ë‹¹í´ë¦­": cph,
                        "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                        "ì—…ë¡œë“œì‹œê°": pub_kst,
                        "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                        "ì œëª©": r["title"],
                        "ë§í¬URL": r["url"],
                    })
                df = pd.DataFrame(rows)
                if not df.empty:
                    df = apply_client_filters(df, upload_period, min_views_label)
                st.session_state.results_df = df
                st.session_state.last_search_time = search_dt
                st.session_state.search_mode = "trend"
                status_placeholder.success(
                    f"[íŠ¸ë Œë“œ ê²€ìƒ‰] ê²°ê³¼: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                )

        # ----- ì±„ë„ ì˜ìƒ ê²€ìƒ‰ -----
        elif mode_triggered == "channel_videos":
            ch_name = (search_query or "").strip()
            if not ch_name:
                st.warning("ì±„ë„ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                append_keyword_log(f"[channel_videos]{ch_name}")
                status_placeholder.info("ì±„ë„ ì˜ìƒ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
                raw_results, cost_used, breakdown = search_videos_in_channel_by_name(
                    channel_name=ch_name,
                    min_views=parse_min_views(min_views_label),
                    api_period_label=api_period,
                    duration_label=duration_label,
                    max_fetch=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
                add_quota_usage(cost_used)

                if not raw_results:
                    st.session_state.results_df = None
                    st.session_state.search_mode = "channel_videos"
                    status_placeholder.info("ì±„ë„ ì˜ìƒ ê²°ê³¼ 0ê±´")
                else:
                    rows = []
                    for r in raw_results:
                        pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                        d, h = human_elapsed_days_hours(search_dt, pub_kst)
                        total_hours = max(1, d*24 + h)
                        cph = int(round(r["views"] / total_hours))
                        rows.append({
                            "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),
                            "ì±„ë„ëª…": r["channel_title"],
                            "ë“±ê¸‰": calc_grade(cph),
                            "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                            "ì‹œê°„ë‹¹í´ë¦­": cph,
                            "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                            "ì—…ë¡œë“œì‹œê°": pub_kst,
                            "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                            "ì œëª©": r["title"],
                            "ë§í¬URL": r["url"],
                        })
                    df = pd.DataFrame(rows)
                    if not df.empty:
                        df = apply_client_filters(df, upload_period, min_views_label)
                    st.session_state.results_df = df
                    st.session_state.last_search_time = search_dt
                    st.session_state.search_mode = "channel_videos"
                    status_placeholder.success(
                        f"[ì±„ë„ ì˜ìƒ ê²€ìƒ‰] ì„œë²„ ê²°ê³¼: {len(raw_results):,}ê±´ / í•„í„° í›„: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                    )

        # ----- í‚¤ì›Œë“œ ì±„ë„ ê²€ìƒ‰ -----
        elif mode_triggered == "channel_list":
            ch_kw = (search_query or "").strip()
            if not ch_kw:
                st.warning("ì±„ë„ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                append_keyword_log(f"[channel]{ch_kw}")
                status_placeholder.info("ì±„ë„ ëª©ë¡ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
                ch_results, cost_used, breakdown = search_channels_by_keyword(
                    keyword=ch_kw,
                    max_results=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
                add_quota_usage(cost_used)

                if not ch_results:
                    st.session_state.results_df = None
                    st.session_state.search_mode = "channel_list"
                    status_placeholder.info("ì±„ë„ ê²°ê³¼ 0ê±´")
                else:
                    df_rows = []
                    for r in ch_results:
                        subs = r["subs"]
                        subs_text = f"{subs:,}" if isinstance(subs, int) else "-"
                        df_rows.append({
                            "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),
                            "ì±„ë„ëª…": r["channel_title"],
                            "êµ¬ë…ììˆ˜": subs_text,
                            "ì±„ë„ì¡°íšŒìˆ˜": f"{r['total_views']:,}",
                            "ì±„ë„ì˜ìƒìˆ˜": f"{r['videos']:,}",
                            "ë§í¬URL": r["url"],
                        })
                    df = pd.DataFrame(df_rows)
                    st.session_state.results_df = df
                    st.session_state.last_search_time = search_dt
                    st.session_state.search_mode = "channel_list"
                    status_placeholder.success(
                        f"[ì±„ë„ ëª©ë¡ ê²€ìƒ‰] ê²°ê³¼: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                    )

except Exception as e:
    st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
    st.session_state.results_df = None

# -----------------------------
# ê²°ê³¼ í‘œì‹œ
# -----------------------------
df = st.session_state.results_df
mode = st.session_state.search_mode

if df is None or df.empty:
    st.info("ì•„ì§ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢Œì¸¡ì—ì„œ ì¡°ê±´ì„ ì„¤ì •í•˜ê³  ë²„íŠ¼ì„ ëˆŒëŸ¬ ê²€ìƒ‰í•˜ì„¸ìš”.")
else:
    df_display = df.copy()
    if "ë§í¬URL" in df_display.columns:
        df_display["ë§í¬"] = df_display["ë§í¬URL"]
        df_display = df_display.drop(columns=["ë§í¬URL"])

    df_display = sort_dataframe(
        df_display,
        mode=mode or "",
        sort_key=st.session_state["sort_key"],
        ascending=st.session_state["sort_asc"],
    )

    # í—¤ë”
    if mode == "general":
        st.subheader("ğŸ“Š ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸")
    elif mode in ("trend", "random_trend"):
        st.subheader("ğŸ”¥ íŠ¸ë Œë“œ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸")
    elif mode == "channel_videos":
        st.subheader("ğŸ¬ ì±„ë„ ì˜ìƒ ë¦¬ìŠ¤íŠ¸")
    elif mode == "channel_list":
        st.subheader("ğŸ“º ì±„ë„ê²€ìƒ‰ ë¦¬ìŠ¤íŠ¸")
    else:
        st.subheader("ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸")

    # ---------------- ì‡¼ì¸  ë·° ----------------
    if view_mode == "shorts":
        # ì˜ìƒ ê³„ì—´ (ì¼ë°˜/íŠ¸ë Œë“œ/ëœë¤íŠ¸ë Œë“œ/ì±„ë„ì˜ìƒ)
        if mode in ("general", "trend", "random_trend", "channel_videos"):
            thumbs = df_display["ì¸ë„¤ì¼"].astype(str).tolist()
            html_items = []
            for url in thumbs:
                if not url:
                    continue
                html_items.append(
                    '<div class="shorts-item">'
                    f'  <div class="shorts-frame" style="background-image:url(\'{url}\');"></div>'
                    '</div>'
                )
            html = (
                "<style>"
                ".shorts-container{display:flex;flex-wrap:wrap;justify-content:center;gap:4px 4px;}"
                ".shorts-item{flex:0 0 23%;max-width:170px;}"
                ".shorts-frame{position:relative;width:100%;height:0;padding-bottom:177%;"
                "overflow:hidden;border-radius:10px;background:#000;"
                "background-size:cover;background-position:center center;background-repeat:no-repeat;}"
                "@media (max-width:480px){"
                ".shorts-item{flex:0 0 48%;max-width:none;}"
                ".shorts-container{gap:4px 4px;}"
                "}"
                "</style>"
                f'<div class="shorts-container">{"".join(html_items)}</div>'
            )
            st.markdown(html, unsafe_allow_html=True)

        # ì±„ë„ ë¦¬ìŠ¤íŠ¸: ì•„ì´ì½˜ ê·¸ë¦¬ë“œ (100x100px)
        elif mode == "channel_list":
            thumbs = df_display["ì¸ë„¤ì¼"].astype(str).tolist()
            html_items = []
            for url in thumbs:
                if not url:
                    continue
                html_items.append(
                    '<div class="shorts-item">'
                    f'  <img src="{url}" class="channel-icon"/>'
                    "</div>"
                )
            html = (
                "<style>"
                ".shorts-container{display:flex;flex-wrap:wrap;justify-content:center;gap:6px 6px;}"
                ".shorts-item{flex:0 0 22%;max-width:100px;}"
                ".channel-icon{width:100px;height:100px;object-fit:cover;border-radius:50%;display:block;}"
                "@media (max-width:480px){"
                ".shorts-item{flex:0 0 25%;}"
                ".channel-icon{width:80px;height:80px;}"
                "}"
                "</style>"
                f'<div class="shorts-container">{"".join(html_items)}</div>'
            )
            st.markdown(html, unsafe_allow_html=True)

        st.caption("ì‡¼ì¸  ë·°: ì´ë¯¸ì§€ë¥¼ ëˆŒëŸ¬ë„ ë³„ë„ ë™ì‘ì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # ---------------- ê·¸ë¦¬ë“œ ë·° ----------------
    elif view_mode == "grid":
        n_cols = 3
        cols = st.columns(n_cols)

        for idx, (_, row) in enumerate(df_display.iterrows()):
            col = cols[idx % n_cols]
            with col:
                if "ì¸ë„¤ì¼" in df_display.columns and isinstance(row["ì¸ë„¤ì¼"], str) and row["ì¸ë„¤ì¼"]:
                    st.image(row["ì¸ë„¤ì¼"], use_column_width=True)

                if mode == "channel_list":
                    title = row.get("ì±„ë„ëª…", "")
                    subs = row.get("êµ¬ë…ììˆ˜", "")
                    total_views = row.get("ì±„ë„ì¡°íšŒìˆ˜", "")
                    video_count = row.get("ì±„ë„ì˜ìƒìˆ˜", "")
                    link = row.get("ë§í¬", "")
                    st.markdown(f"**{title}**")
                    st.caption(f"êµ¬ë…ì: {subs} Â· ì¡°íšŒìˆ˜: {total_views} Â· ì˜ìƒìˆ˜: {video_count}")
                    if link:
                        st.markdown(f"[ì±„ë„ ì—´ê¸°]({link})")
                else:
                    title = row.get("ì œëª©", "")
                    ch = row.get("ì±„ë„ëª…", "")
                    views = row.get("ì˜ìƒì¡°íšŒìˆ˜", "")
                    grade = row.get("ë“±ê¸‰", "")
                    link = row.get("ë§í¬", "")
                    st.markdown(f"**{title}**")
                    st.caption(f"ë“±ê¸‰ {grade} Â· {ch} Â· ì¡°íšŒìˆ˜ {views:,}")
                    if link:
                        st.markdown(f"[ì˜ìƒ ì—´ê¸°]({link})")

            if (idx + 1) % n_cols == 0 and (idx + 1) < len(df_display):
                cols = st.columns(n_cols)

        st.caption("ğŸ‘‰ í…ìŠ¤íŠ¸ ë§í¬ë¥¼ ëˆŒëŸ¬ ìƒˆ íƒ­ì—ì„œ ì˜ìƒ ë˜ëŠ” ì±„ë„ì„ ì—´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # ---------------- ë¦¬ìŠ¤íŠ¸ ë·° ----------------
    else:
        if mode in ("general", "trend", "random_trend", "channel_videos"):
            base_order = [
                "ë“±ê¸‰",
                "ì¸ë„¤ì¼",
                "ì±„ë„ëª…",
                "ì˜ìƒì¡°íšŒìˆ˜",
                "ì‹œê°„ë‹¹í´ë¦­",
                "ì˜ìƒê¸¸ì´",
                "ì—…ë¡œë“œì‹œê°",
                "ê²½ê³¼ì‹œê°„",
                "ì œëª©",
                "ë§í¬",
            ]
        else:
            base_order = [
                "ì¸ë„¤ì¼",
                "ì±„ë„ëª…",
                "êµ¬ë…ììˆ˜",
                "ì±„ë„ì¡°íšŒìˆ˜",
                "ì±„ë„ì˜ìƒìˆ˜",
                "ë§í¬",
            ]
        column_order = [c for c in base_order if c in df_display.columns]

        column_config = {}
        if "ë§í¬" in df_display.columns:
            column_config["ë§í¬"] = st.column_config.LinkColumn(
                "ì—´ê¸°",
                display_text="ì—´ê¸°",
            )
        if "ì¸ë„¤ì¼" in df_display.columns:
            column_config["ì¸ë„¤ì¼"] = st.column_config.ImageColumn(
                "ì¸ë„¤ì¼",
                help="ì¸ë„¤ì¼ ì´ë¯¸ì§€",
                width="small",
            )

        if mode == "general":
            editor_key = "video_results_editor_general"
        elif mode in ("trend", "random_trend"):
            editor_key = "video_results_editor_trend"
        elif mode == "channel_videos":
            editor_key = "video_results_editor_channel_videos"
        elif mode == "channel_list":
            editor_key = "channel_results_editor_keyword"
        else:
            editor_key = "results_editor_default"

        st.data_editor(
            df_display,
            use_container_width=True,
            height=620,
            hide_index=True,
            column_order=column_order if column_order else None,
            column_config=column_config,
            key=editor_key,
            disabled=True,
            num_rows="fixed",
        )

        st.caption("ğŸ‘‰ 'ì—´ê¸°' ë§í¬ë¥¼ ëˆ„ë¥´ë©´ ìƒˆ íƒ­ì—ì„œ ì˜ìƒ ë˜ëŠ” ì±„ë„ì´ ì—´ë¦½ë‹ˆë‹¤.")

# -----------------------------
# ì‚¬ì´ë“œë°” í•˜ë‹¨
# -----------------------------
st.sidebar.markdown("---")

quota_today = get_today_quota_total()
st.sidebar.caption(f"ì˜¤ëŠ˜ ì‚¬ìš© ì¿¼í„°: {quota_today:,} units")

recents = get_recent_keywords(7)
if recents:
    keywords = [q for _, q in recents]
    labels = [f"`{k}`" for k in keywords]
    st.sidebar.caption("ìµœê·¼ í‚¤ì›Œë“œ: " + " Â· ".join(labels))
else:
    st.sidebar.caption("ìµœê·¼ í‚¤ì›Œë“œ: ì—†ìŒ")

if st.sidebar.button("ë¡œê·¸ì•„ì›ƒ", use_container_width=True):
    st.session_state["logged_in"] = False
    st.rerun()
