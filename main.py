#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
from datetime import datetime, timedelta, timezone

import streamlit as st
import pandas as pd

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

from supabase import create_client, Client

st.set_page_config(
    page_title="YouTube ê²€ìƒ‰ê¸° (Streamlit)",
    page_icon="ğŸ”",
    layout="wide",
)

st.markdown(
    """
    <style>
    .block-container { padding-top: 3rem !important; }
    </style>
    """,
    unsafe_allow_html=True,
)

st.title("ğŸ” YouTube ê²€ìƒ‰ê¸° (Streamlit)")

KST = timezone(timedelta(hours=9))
ENV_KEY_NAME = "YOUTUBE_API_KEY"
WEEKDAY_KO = ["ì›”ìš”ì¼","í™”ìš”ì¼","ìˆ˜ìš”ì¼","ëª©ìš”ì¼","ê¸ˆìš”ì¼","í† ìš”ì¼","ì¼ìš”ì¼"]

COUNTRY_LANG_MAP = {
    "í•œêµ­": ("KR", "ko"),
    "ì¼ë³¸": ("JP", "ja"),
    "ë¯¸êµ­": ("US", "en"),
    "ì˜êµ­": ("GB", "en"),
    "ë…ì¼": ("DE", "de"),
    "í”„ë‘ìŠ¤": ("FR", "fr"),
    "ìŠ¤í˜ì¸": ("ES", "es"),
    "ì´íƒˆë¦¬ì•„": ("IT", "it"),
    "ë¸Œë¼ì§ˆ": ("BR", "pt"),
    "ì¸ë„": ("IN", "en"),
    "í˜¸ì£¼": ("AU", "en"),
}
COUNTRY_LIST = list(COUNTRY_LANG_MAP.keys())

# ----------------------------
# Supabase ì—°ë™ (ì„¤ì •/ë¡œê·¸ ì €ì¥ìš©)
# ----------------------------
@st.cache_resource
def get_supabase_client() -> Client | None:
    url = st.secrets.get("SUPABASE_URL")
    key = st.secrets.get("SUPABASE_SERVICE_KEY")
    if not url or not key:
        return None
    return create_client(url, key)

SUPABASE_BUCKET = st.secrets.get("SUPABASE_BUCKET", "yts-config")
supabase = get_supabase_client()

def _load_json(filename: str, default):
    if supabase is None:
        return default
    try:
        res = supabase.storage.from_(SUPABASE_BUCKET).download(filename)
        if res is None:
            return default
        if isinstance(res, bytes):
            text = res.decode("utf-8")
        else:
            text = str(res)
        return json.loads(text)
    except Exception:
        return default

def _save_json(filename: str, data):
    if supabase is None:
        return
    try:
        payload = json.dumps(data, ensure_ascii=False, indent=2).encode("utf-8")
        supabase.storage.from_(SUPABASE_BUCKET).upload(
            path=filename,
            file=payload,
            file_options={"content-type": "application/json", "x-upsert": "true"},
        )
    except Exception as e:
        st.warning(f"Supabase ì €ì¥ ì˜¤ë¥˜({filename}): {e}")

# ì„¤ì •/ë¡œê·¸ íŒŒì¼ ì´ë¦„
KEYWORD_LOG_PATH  = "yts_keyword_log.json"
QUOTA_PATH        = "yts_quota_usage.json"

# ----------------------------
# API í‚¤: secretsì—ì„œë§Œ ì‚¬ìš©
# ----------------------------
YOUTUBE_API_KEYS = st.secrets.get("YOUTUBE_API_KEYS", [])
if isinstance(YOUTUBE_API_KEYS, str):
    YOUTUBE_API_KEYS = [YOUTUBE_API_KEYS]
SINGLE_API_KEY = st.secrets.get("YOUTUBE_API_KEY", "")

def get_current_api_key() -> str:
    """
    - st.secrets["YOUTUBE_API_KEYS"] â†’ ë¦¬ìŠ¤íŠ¸ë©´ ì²« ë²ˆì§¸ í‚¤ ì‚¬ìš©
    - ì—†ìœ¼ë©´ st.secrets["YOUTUBE_API_KEY"] ì‚¬ìš©
    """
    if YOUTUBE_API_KEYS:
        return YOUTUBE_API_KEYS[0]
    if SINGLE_API_KEY:
        return SINGLE_API_KEY
    return ""

# ----------------------------
# ì¿¼í„° ê´€ë¦¬
# ----------------------------
def _load_quota_map():
    return _load_json(QUOTA_PATH, {})

def _save_quota_map(data: dict):
    _save_json(QUOTA_PATH, data)

def quota_today_key():
    return datetime.now(KST).strftime("%Y-%m-%d")

def get_today_quota_total() -> int:
    data = _load_quota_map()
    return int(data.get(quota_today_key(), 0))

def add_quota_usage(units: int):
    if units <= 0:
        return
    data = _load_quota_map()
    key = quota_today_key()
    data[key] = int(data.get(key, 0)) + int(units)
    _save_quota_map(data)

# ----------------------------
# í‚¤ì›Œë“œ ë¡œê·¸
# ----------------------------
def _load_keyword_log():
    return _load_json(KEYWORD_LOG_PATH, [])

def _save_keyword_log(entries: list):
    _save_json(KEYWORD_LOG_PATH, entries)

def append_keyword_log(query: str):
    q = (query or "").strip()
    if not q:
        return
    entries = _load_keyword_log()
    now = datetime.now(KST).isoformat(timespec="seconds")
    entries.append({"ts": now, "q": q})
    _save_keyword_log(entries)

def get_recent_keywords(limit: int = 30):
    entries = _load_keyword_log()
    out = []
    for item in entries:
        ts = item.get("ts")
        q  = item.get("q")
        if not ts or not q:
            continue
        try:
            dt = datetime.fromisoformat(ts)
        except Exception:
            continue
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=KST)
        out.append((dt, q))
    out.sort(key=lambda x: x[0], reverse=True)
    return out[:limit]

# ----------------------------
# ì‹œê°„/í˜•ì‹ ìœ í‹¸
# ----------------------------
def format_k_datetime(dt_aw: datetime) -> str:
    if dt_aw.tzinfo is None:
        dt_aw = dt_aw.replace(tzinfo=KST)
    dt = dt_aw.astimezone(KST)
    wd = WEEKDAY_KO[dt.weekday()]
    h24 = dt.hour
    ampm = "ì˜¤ì „" if h24 < 12 else "ì˜¤í›„"
    h12 = h24 % 12 or 12
    return f"{dt.month}ì›”{dt.day}ì¼ {wd} {ampm}{h12}ì‹œ {dt.minute}ë¶„"

def parse_published_at_to_kst(published_iso: str) -> datetime:
    dt_utc = datetime.fromisoformat(published_iso.replace("Z", "+00:00"))
    return dt_utc.astimezone(KST)

def human_elapsed_days_hours(later: datetime, earlier: datetime) -> tuple[int, int]:
    delta = later - earlier
    if delta.total_seconds() < 0:
        return 0, 0
    days = delta.days
    hours = delta.seconds // 3600
    return days, hours

def published_after_from_label(label: str):
    label = label.strip()
    now_utc = datetime.now(timezone.utc)
    if label == "ì œí•œì—†ìŒ":
        return None
    if label.endswith("ì¼"):
        days = int(label[:-1])
        dt = now_utc - timedelta(days=days)
    else:
        return None
    return dt.isoformat(timespec="seconds").replace("+00:00", "Z")

def parse_duration_iso8601(iso_dur: str) -> int:
    h = m = s = 0
    if not iso_dur or not iso_dur.startswith("PT"):
        return 0
    num = ""
    for ch in iso_dur[2:]:
        if ch.isdigit():
            num += ch
        else:
            if ch == "H" and num:
                h = int(num); num = ""
            elif ch == "M" and num:
                m = int(num); num = ""
            elif ch == "S" and num:
                s = int(num); num = ""
    return h*3600 + m*60 + s

def format_duration_hms(seconds: int) -> str:
    if seconds <= 0:
        return "0:00"
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    if h > 0:
        return f"{h}:{m:02d}:{s:02d}"
    return f"{m}:{s:02d}"

def duration_filter_ok(seconds: int, label: str) -> bool:
    if label == "ì „ì²´": return True
    if label == "ì‡¼ì¸ ": return seconds < 60
    if label == "ë¡±í¼": return seconds >= 60
    if label == "1~20ë¶„": return 60 <= seconds < 20*60
    if label == "20~40ë¶„": return 20*60 <= seconds < 40*60
    if label == "40~60ë¶„": return 40*60 <= seconds < 60*60
    if label == "60ë¶„ì´ìƒ": return seconds >= 60*60
    return True

def parse_min_views(text: str) -> int:
    digits = text.replace(",", "").replace(" ", "").replace("ë§Œ", "0000")
    try:
        return int(digits)
    except Exception:
        return 0

# ----------------------------
# YouTube í´ë¼ì´ì–¸íŠ¸
# ----------------------------
def get_youtube_client():
    key = get_current_api_key()
    if not key:
        raise RuntimeError("YouTube API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. (st.secretsì— YOUTUBE_API_KEYS ë˜ëŠ” YOUTUBE_API_KEY ì„¤ì • í•„ìš”)")
    try:
        return build("youtube", "v3", developerKey=key, cache_discovery=False)
    except TypeError:
        return build("youtube", "v3", developerKey=key)

# ----------------------------
# ì˜ìƒ ê²€ìƒ‰ (í‚¤ì›Œë“œ ê¸°ë°˜ - ì¼ë°˜ê²€ìƒ‰)
# ----------------------------
def search_videos(
    query: str,
    min_views: int,
    api_period_label: str,
    duration_label: str,
    max_fetch: int,
    region_code: str | None,
    lang_code: str | None,
):
    youtube = get_youtube_client()
    published_after = published_after_from_label(api_period_label)

    cost_used = 0
    breakdown = {"search.list": 0, "videos.list": 0}
    max_fetch = max(1, min(int(max_fetch or 100), 5000))

    results_tmp = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        kwargs = dict(
            q=query,
            part="id",
            type="video",
            maxResults=take,
        )
        if published_after:
            kwargs["publishedAfter"] = published_after
        if region_code:
            kwargs["regionCode"] = region_code
        if lang_code:
            kwargs["relevanceLanguage"] = lang_code
        if next_token:
            kwargs["pageToken"] = next_token

        try:
            search_response = youtube.search().list(**kwargs).execute()
            cost_used += 100; breakdown["search.list"] += 100
        except HttpError as e:
            raise RuntimeError(f"Search API ì˜¤ë¥˜: {e}")

        page_ids = [
            it["id"]["videoId"]
            for it in search_response.get("items", [])
            if "id" in it and "videoId" in it["id"]
        ]
        if not page_ids:
            break

        try:
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(page_ids),
            ).execute()
            cost_used += 1; breakdown["videos.list"] += 1
        except HttpError as e:
            raise RuntimeError(f"Videos API ì˜¤ë¥˜: {e}")

        items = video_response.get("items", [])
        for item in items:
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}

            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

            if view_count < min_views:
                continue
            if not duration_filter_ok(duration_sec, duration_label):
                continue

            results_tmp.append({
                "title": title,
                "views": view_count,
                "published_at_iso": published_at_iso,
                "url": url,
                "duration_sec": duration_sec,
                "channel_title": snip.get("channelTitle", ""),
            })

        fetched += len(page_ids)
        next_token = search_response.get("nextPageToken")
        if not next_token:
            break

    return results_tmp, cost_used, breakdown

# ----------------------------
# íŠ¸ë Œë“œ ê²€ìƒ‰ (ìµœê·¼ ì¸ê¸° ì˜ìƒ - ê²€ìƒ‰ì–´ X)
# ----------------------------
def search_trending_videos(
    min_views: int,
    duration_label: str,
    max_fetch: int,
    region_code: str | None,
):
    youtube = get_youtube_client()

    cost_used = 0
    breakdown = {"videos.list": 0}
    max_fetch = max(1, min(int(max_fetch or 100), 200))  # íŠ¸ë Œë“œëŠ” 200ê°œ ì •ë„ë©´ ì¶©ë¶„

    results_tmp = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        kwargs = dict(
            part="snippet,statistics,contentDetails",
            chart="mostPopular",
            maxResults=take,
        )
        if region_code:
            kwargs["regionCode"] = region_code
        if next_token:
            kwargs["pageToken"] = next_token

        try:
            resp = youtube.videos().list(**kwargs).execute()
            cost_used += 1
            breakdown["videos.list"] += 1
        except HttpError as e:
            raise RuntimeError(f"íŠ¸ë Œë“œ API ì˜¤ë¥˜: {e}")

        items = resp.get("items", [])
        if not items:
            break

        for item in items:
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}

            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

            if view_count < min_views:
                continue
            if not duration_filter_ok(duration_sec, duration_label):
                continue

            results_tmp.append({
                "title": title,
                "views": view_count,
                "published_at_iso": published_at_iso,
                "url": url,
                "duration_sec": duration_sec,
                "channel_title": snip.get("channelTitle", ""),
            })

        fetched += len(items)
        next_token = resp.get("nextPageToken")
        if not next_token:
            break

    return results_tmp, cost_used, breakdown

# ----------------------------
# ì±„ë„ í‚¤ì›Œë“œë¡œ ì±„ë„ ì°¾ê¸°
# ----------------------------
def search_channels_by_keyword(
    keyword: str,
    max_results: int,
    region_code: str | None,
    lang_code: str | None,
):
    youtube = get_youtube_client()
    take = max(1, min(max_results, 50))
    kwargs = dict(
        q=keyword,
        part="id",
        type="channel",
        maxResults=take,
    )
    if region_code:
        kwargs["regionCode"] = region_code
    if lang_code:
        kwargs["relevanceLanguage"] = lang_code

    try:
        search_response = youtube.search().list(**kwargs).execute()
        cost_used = 100
    except HttpError as e:
        raise RuntimeError(f"Channel search API ì˜¤ë¥˜: {e}")

    ch_ids = [
        it["id"]["channelId"]
        for it in search_response.get("items", [])
        if "id" in it and "channelId" in it["id"]
    ]
    if not ch_ids:
        return [], cost_used, {"search.list": 100, "channels.list": 0}

    try:
        ch_resp = youtube.channels().list(
            part="snippet,statistics",
            id=",".join(ch_ids),
        ).execute()
        cost_used += 1
    except HttpError as e:
        raise RuntimeError(f"Channels API ì˜¤ë¥˜: {e}")

    results = []
    for c in ch_resp.get("items", []):
        cid = c.get("id", "")
        sn = c.get("snippet", {}) or {}
        stt = c.get("statistics", {}) or {}
        subs = int(stt.get("subscriberCount", 0)) if stt.get("subscriberCount") is not None else None
        total_views = int(stt.get("viewCount", 0))
        videos = int(stt.get("videoCount", 0))
        url = f"https://www.youtube.com/channel/{cid}" if cid else ""
        results.append({
            "channel_title": sn.get("title", ""),
            "subs": subs,
            "total_views": total_views,
            "videos": videos,
            "url": url,
        })

    results.sort(key=lambda r: (r["subs"] or 0), reverse=True)
    return results, cost_used, {"search.list": 100, "channels.list": 1}

# ----------------------------
# ì±„ë„ ê²€ìƒ‰ì–´(ì±„ë„ ì´ë¦„)ë¡œ ì±„ë„ ì˜ìƒ ê²€ìƒ‰
# ----------------------------
def search_videos_in_channel_by_name(
    channel_name: str,
    min_views: int,
    api_period_label: str,
    duration_label: str,
    max_fetch: int,
    region_code: str | None,
    lang_code: str | None,
):
    youtube = get_youtube_client()
    cost_used = 0
    breakdown = {"search.list": 0, "videos.list": 0}

    # 1) ì±„ë„ ê²€ìƒ‰ (ì´ë¦„ìœ¼ë¡œ)
    kwargs_ch = dict(
        q=channel_name,
        part="id,snippet",
        type="channel",
        maxResults=1,
    )
    if region_code:
        kwargs_ch["regionCode"] = region_code
    if lang_code:
        kwargs_ch["relevanceLanguage"] = lang_code

    try:
        ch_search = youtube.search().list(**kwargs_ch).execute()
        cost_used += 100; breakdown["search.list"] += 100
    except HttpError as e:
        raise RuntimeError(f"ì±„ë„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

    items = ch_search.get("items", [])
    if not items:
        return [], cost_used, breakdown

    channel_id = items[0]["id"]["channelId"]
    channel_title = (items[0].get("snippet") or {}).get("title", channel_name)

    # 2) í•´ë‹¹ ì±„ë„ì˜ ì˜ìƒ ê²€ìƒ‰
    published_after = published_after_from_label(api_period_label)
    max_fetch = max(1, min(int(max_fetch or 100), 5000))

    results_tmp = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        kwargs = dict(
            part="id",
            type="video",
            channelId=channel_id,
            maxResults=take,
            order="date",
        )
        if published_after:
            kwargs["publishedAfter"] = published_after
        if region_code:
            kwargs["regionCode"] = region_code
        if lang_code:
            kwargs["relevanceLanguage"] = lang_code
        if next_token:
            kwargs["pageToken"] = next_token

        try:
            v_search = youtube.search().list(**kwargs).execute()
            cost_used += 100; breakdown["search.list"] += 100
        except HttpError as e:
            raise RuntimeError(f"ì±„ë„ ì˜ìƒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

        page_ids = [
            it["id"]["videoId"]
            for it in v_search.get("items", [])
            if "id" in it and "videoId" in it["id"]
        ]
        if not page_ids:
            break

        try:
            video_resp = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(page_ids),
            ).execute()
            cost_used += 1; breakdown["videos.list"] += 1
        except HttpError as e:
            raise RuntimeError(f"Videos API ì˜¤ë¥˜: {e}")

        for item in video_resp.get("items", []):
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}

            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

            if view_count < min_views:
                continue
            if not duration_filter_ok(duration_sec, duration_label):
                continue

            results_tmp.append({
                "title": title,
                "views": view_count,
                "published_at_iso": published_at_iso,
                "url": url,
                "duration_sec": duration_sec,
                "channel_title": channel_title,
            })

        fetched += len(page_ids)
        next_token = v_search.get("nextPageToken")
        if not next_token:
            break

    return results_tmp, cost_used, breakdown

# ----------------------------
# ë“±ê¸‰ ê³„ì‚°
# ----------------------------
def calc_grade(clicks_per_hour: int) -> str:
    v = clicks_per_hour
    if v >= 5000: return "S"
    if v >= 2000: return "A+"
    if v >= 1000: return "A"
    if v >= 500:  return "B"
    if v >= 300:  return "C"
    if v >= 100:  return "D"
    if v >= 50:   return "E"
    return "F"

# ==================================================================
# ì‚¬ì´ë“œë°” UI
# ==================================================================

st.sidebar.header("ê²€ìƒ‰")

# 1) ì¼ë°˜ ê²€ìƒ‰ (í•­ìƒ í¼ì³ì ¸ ìˆëŠ” ì˜ì—­)
query = st.sidebar.text_input("ğŸ” ì¼ë°˜ ê²€ìƒ‰ì–´", "", placeholder="ì˜ˆ: ì›”ë“œì»µ ê²½ì œí•™")
btn_general = st.sidebar.button("ì¼ë°˜ ê²€ìƒ‰ ì‹¤í–‰", use_container_width=True)

# Separator
try:
    st.sidebar.divider()
except Exception:
    st.sidebar.markdown("---")

# 2) ë‚˜ë¨¸ì§€ ê²€ìƒ‰ë°©ì‹: íŠ¸ë Œë“œ / ì±„ë„ í‚¤ì›Œë“œ / ì±„ë„ ì˜ìƒ â†’ ëª¨ë‘ expanderë¡œ
with st.sidebar.expander("ğŸ”¥ íŠ¸ë Œë“œ ê²€ìƒ‰", expanded=False):
    st.caption("í˜„ì¬ êµ­ê°€ ê¸°ì¤€ìœ¼ë¡œ YouTube ì¸ê¸° ë™ì˜ìƒì„ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    btn_trend = st.button("íŠ¸ë Œë“œ ê°€ì ¸ì˜¤ê¸°", use_container_width=True, key="btn_trend")

with st.sidebar.expander("ğŸ“ˆ ì±„ë„ í‚¤ì›Œë“œë¡œ ì±„ë„ ì°¾ê¸°", expanded=False):
    channel_keyword = st.text_input("ì±„ë„ í‚¤ì›Œë“œ", "", placeholder="ì˜ˆ: ì¶•êµ¬ í•˜ì´ë¼ì´íŠ¸", key="channel_keyword")
    btn_channel_find = st.button("ì±„ë„ ê²€ìƒ‰ ì‹¤í–‰", use_container_width=True, key="btn_channel_find")

with st.sidebar.expander("ğŸ ì±„ë„ ì´ë¦„ìœ¼ë¡œ ì±„ë„ ì˜ìƒ ê²€ìƒ‰", expanded=False):
    channel_name = st.text_input("ì±„ë„ ê²€ìƒ‰ì–´(ì±„ë„ ì´ë¦„)", "", placeholder="ì˜ˆ: SPOTV", key="channel_name")
    btn_channel_videos = st.button("ì±„ë„ ì˜ìƒ ë¶ˆëŸ¬ì˜¤ê¸°", use_container_width=True, key="btn_channel_videos")

st.sidebar.markdown("---")

with st.sidebar.expander("âš™ ì„¸ë¶€ í•„í„°", expanded=False):
    api_period = st.selectbox(
        "ì„œë²„ ê²€ìƒ‰ê¸°ê°„(ì¼ë°˜ê²€ìƒ‰/ì±„ë„ì˜ìƒ)",
        ["ì œí•œì—†ìŒ","90ì¼","150ì¼","365ì¼","730ì¼","1095ì¼","1825ì¼","3650ì¼"],
        index=1,
    )
    upload_period = st.selectbox(
        "ì—…ë¡œë“œ ê¸°ê°„(í´ë¼ì´ì–¸íŠ¸ í•„í„°)",
        ["ì œí•œì—†ìŒ","1ì¼","3ì¼","7ì¼","14ì¼","30ì¼","60ì¼","90ì¼","180ì¼","365ì¼"],
        index=6,
    )
    min_views_label = st.selectbox(
        "ìµœì†Œ ì¡°íšŒìˆ˜",
        ["5,000","10,000","25,000","50,000","100,000","200,000","500,000","1,000,000"],
        index=0,
    )
    duration_label = st.selectbox(
        "ì˜ìƒ ê¸¸ì´",
        ["ì „ì²´","ì‡¼ì¸ ","ë¡±í¼","1~20ë¶„","20~40ë¶„","40~60ë¶„","60ë¶„ì´ìƒ"],
        index=0,
    )
    max_fetch = st.number_input("ê°€ì ¸ì˜¬ ìµœëŒ€ ê°œìˆ˜", 1, 5000, 50, step=10)
    country_name = st.selectbox("êµ­ê°€/ì–¸ì–´", COUNTRY_LIST, index=0)
    region_code, lang_code = COUNTRY_LANG_MAP[country_name]

st.sidebar.markdown("---")

with st.sidebar.expander("â± ìµœê·¼ ê²€ìƒ‰ í‚¤ì›Œë“œ", expanded=False):
    recents = get_recent_keywords(30)
    if not recents:
        st.write("ìµœê·¼ ê²€ìƒ‰ ì—†ìŒ")
    else:
        for dt, q in recents:
            st.write(f"- {dt.strftime('%m-%d %H:%M')} â€” `{q}`")

st.sidebar.markdown("---")
st.sidebar.metric("ì˜¤ëŠ˜ ì‚¬ìš©í•œ ì¿¼í„°", f"{get_today_quota_total():,} units")

# ==================================================================
# ë©”ì¸ ì˜ì—­
# ==================================================================

status_placeholder = st.empty()

if "results_df" not in st.session_state:
    st.session_state.results_df = None
    st.session_state.last_search_time = None
    st.session_state.search_type = None  # "video_general","video_trend","channel_find","channel_videos"

def apply_client_filters(df: pd.DataFrame, upload_period: str, min_views_label: str) -> pd.DataFrame:
    if upload_period != "ì œí•œì—†ìŒ" and "ì—…ë¡œë“œì‹œê°" in df.columns:
        days = int(upload_period.replace("ì¼",""))
        cutoff = datetime.now(KST) - timedelta(days=days)
        df = df[df["ì—…ë¡œë“œì‹œê°"] >= cutoff]
    min_views = parse_min_views(min_views_label)
    if "ì˜ìƒì¡°íšŒìˆ˜" in df.columns:
        df = df[df["ì˜ìƒì¡°íšŒìˆ˜"] >= min_views]
    return df

# ì–´ë–¤ ë²„íŠ¼ì´ ëˆŒë ¸ëŠ”ì§€ í™•ì¸
mode = None
if btn_general:
    mode = "video_general"
elif btn_trend:
    mode = "video_trend"
elif btn_channel_find:
    mode = "channel_find"
elif btn_channel_videos:
    mode = "channel_videos"

if mode is not None:
    try:
        # ---------------- ì¼ë°˜ ê²€ìƒ‰ ----------------
        if mode == "video_general":
            base_query = (query or "").strip()
            if not base_query:
                st.warning("ì¼ë°˜ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                append_keyword_log(base_query)
                status_placeholder.info("ì¼ë°˜ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")

                raw_results, cost_used, breakdown = search_videos(
                    query=base_query,
                    min_views=parse_min_views(min_views_label),
                    api_period_label=api_period,
                    duration_label=duration_label,
                    max_fetch=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )

                if not raw_results:
                    st.session_state.results_df = None
                    st.session_state.search_type = "video_general"
                    status_placeholder.info("ì„œë²„ ê²°ê³¼ 0ê±´")
                else:
                    search_dt = datetime.now(KST)
                    rows = []
                    for r in raw_results:
                        pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                        d, h = human_elapsed_days_hours(search_dt, pub_kst)
                        total_hours = max(1, d*24 + h)
                        cph = int(round(r["views"] / total_hours))
                        rows.append({
                            "ì±„ë„ëª…": r["channel_title"],
                            "ë“±ê¸‰": calc_grade(cph),
                            "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                            "ì‹œê°„ë‹¹í´ë¦­": cph,
                            "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                            "ì—…ë¡œë“œì‹œê°": pub_kst,
                            "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                            "ì œëª©": r["title"],
                            "URL": r["url"],
                        })
                    df = pd.DataFrame(rows)
                    if not df.empty:
                        df = apply_client_filters(df, upload_period, min_views_label)
                    st.session_state.results_df = df
                    st.session_state.last_search_time = search_dt
                    st.session_state.search_type = "video_general"
                    status_placeholder.success(
                        f"ì„œë²„ ê²°ê³¼: {len(raw_results):,}ê±´ / í•„í„° í›„: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                    )
                add_quota_usage(cost_used)

        # ---------------- íŠ¸ë Œë“œ ê²€ìƒ‰ ----------------
        elif mode == "video_trend":
            append_keyword_log("[trend]")
            status_placeholder.info("íŠ¸ë Œë“œ ì¸ê¸° ì˜ìƒ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")

            raw_results, cost_used, breakdown = search_trending_videos(
                min_views=parse_min_views(min_views_label),
                duration_label=duration_label,
                max_fetch=max_fetch,
                region_code=region_code,
            )

            if not raw_results:
                st.session_state.results_df = None
                st.session_state.search_type = "video_trend"
                status_placeholder.info("íŠ¸ë Œë“œ ê²°ê³¼ 0ê±´")
            else:
                search_dt = datetime.now(KST)
                rows = []
                for r in raw_results:
                    pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                    d, h = human_elapsed_days_hours(search_dt, pub_kst)
                    total_hours = max(1, d*24 + h)
                    cph = int(round(r["views"] / total_hours))
                    rows.append({
                        "ì±„ë„ëª…": r["channel_title"],
                        "ë“±ê¸‰": calc_grade(cph),
                        "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                        "ì‹œê°„ë‹¹í´ë¦­": cph,
                        "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                        "ì—…ë¡œë“œì‹œê°": pub_kst,
                        "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                        "ì œëª©": r["title"],
                        "URL": r["url"],
                    })
                df = pd.DataFrame(rows)
                if not df.empty:
                    df = apply_client_filters(df, upload_period, min_views_label)
                st.session_state.results_df = df
                st.session_state.last_search_time = search_dt
                st.session_state.search_type = "video_trend"
                status_placeholder.success(
                    f"íŠ¸ë Œë“œ ê²°ê³¼: {len(raw_results):,}ê±´ / í•„í„° í›„: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                )
            add_quota_usage(cost_used)

        # ---------------- ì±„ë„ í‚¤ì›Œë“œë¡œ ì±„ë„ ì°¾ê¸° ----------------
        elif mode == "channel_find":
            ch_kw = (channel_keyword or "").strip()
            if not ch_kw:
                st.warning("ì±„ë„ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                append_keyword_log(f"[channel-find]{ch_kw}")
                status_placeholder.info("ì±„ë„ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
                ch_results, cost_used, breakdown = search_channels_by_keyword(
                    keyword=ch_kw,
                    max_results=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
                rows = []
                for r in ch_results:
                    subs = r["subs"]
                    subs_text = f"{subs:,}" if isinstance(subs, int) else "-"
                    rows.append({
                        "ì±„ë„ëª…": r["channel_title"],
                        "êµ¬ë…ììˆ˜": subs_text,
                        "ì±„ë„ì¡°íšŒìˆ˜": f"{r['total_views']:,}",
                        "ì±„ë„ì˜ìƒìˆ˜": f"{r['videos']:,}",
                        "URL": r["url"],
                    })
                df = pd.DataFrame(rows)
                st.session_state.results_df = df
                st.session_state.last_search_time = datetime.now(KST)
                st.session_state.search_type = "channel_find"
                status_placeholder.success(
                    f"ì±„ë„ ê²€ìƒ‰ ê²°ê³¼: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                )
                add_quota_usage(cost_used)

        # ---------------- ì±„ë„ ì´ë¦„ìœ¼ë¡œ ì±„ë„ ì˜ìƒ ê²€ìƒ‰ ----------------
        elif mode == "channel_videos":
            ch_name = (channel_name or "").strip()
            if not ch_name:
                st.warning("ì±„ë„ ê²€ìƒ‰ì–´(ì±„ë„ ì´ë¦„)ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                append_keyword_log(f"[channel-video]{ch_name}")
                status_placeholder.info("ì±„ë„ ì˜ìƒ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
                raw_results, cost_used, breakdown = search_videos_in_channel_by_name(
                    channel_name=ch_name,
                    min_views=parse_min_views(min_views_label),
                    api_period_label=api_period,
                    duration_label=duration_label,
                    max_fetch=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
                if not raw_results:
                    st.session_state.results_df = None
                    st.session_state.search_type = "channel_videos"
                    status_placeholder.info("ì±„ë„ ì˜ìƒ ê²°ê³¼ 0ê±´")
                else:
                    search_dt = datetime.now(KST)
                    rows = []
                    for r in raw_results:
                        pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                        d, h = human_elapsed_days_hours(search_dt, pub_kst)
                        total_hours = max(1, d*24 + h)
                        cph = int(round(r["views"] / total_hours))
                        rows.append({
                            "ì±„ë„ëª…": r["channel_title"],
                            "ë“±ê¸‰": calc_grade(cph),
                            "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                            "ì‹œê°„ë‹¹í´ë¦­": cph,
                            "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                            "ì—…ë¡œë“œì‹œê°": pub_kst,
                            "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                            "ì œëª©": r["title"],
                            "URL": r["url"],
                        })
                    df = pd.DataFrame(rows)
                    if not df.empty:
                        df = apply_client_filters(df, upload_period, min_views_label)
                    st.session_state.results_df = df
                    st.session_state.last_search_time = search_dt
                    st.session_state.search_type = "channel_videos"
                    status_placeholder.success(
                        f"ì±„ë„ ì˜ìƒ ê²°ê³¼: {len(raw_results):,}ê±´ / í•„í„° í›„: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                    )
                add_quota_usage(cost_used)

    except Exception as e:
        st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        st.session_state.results_df = None

# ==================================================================
# ê²°ê³¼ í‘œì‹œ
# ==================================================================
df = st.session_state.results_df
search_type = st.session_state.search_type

if df is None or df.empty:
    st.info("ì•„ì§ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢Œì¸¡ì—ì„œ ì¡°ê±´ì„ ì„¤ì •í•˜ê³  **ê²€ìƒ‰ ë²„íŠ¼**ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
else:
    # ê²€ìƒ‰ íƒ€ì…ë³„ ì œëª©
    video_title_map = {
        "video_general": "ğŸ“Š ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸",
        "video_trend": "ğŸ“ˆ íŠ¸ë Œë“œ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸",
        "channel_videos": "ğŸ ì±„ë„ ì˜ìƒ ë¦¬ìŠ¤íŠ¸",
    }

    if search_type in ("video_general", "video_trend", "channel_videos"):
        df_display = df.copy()
        df_display["ë§í¬"] = df_display["URL"]
        df_display = df_display.drop(columns=["URL"])
        st.subheader(video_title_map.get(search_type, "ğŸ“Š ì˜ìƒ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸"))
        st.data_editor(
            df_display,
            use_container_width=True,
            height=500,
            hide_index=True,
            column_config={
                "ë§í¬": st.column_config.LinkColumn("ì—´ê¸°", display_text="ì—´ê¸°"),
            },
        )
    elif search_type == "channel_find":
        df_display = df.copy()
        df_display["ë§í¬"] = df_display["URL"]
        df_display = df_display.drop(columns=["URL"])
        st.subheader("ğŸ“‚ ì±„ë„ê²€ìƒ‰ ë¦¬ìŠ¤íŠ¸")
        st.data_editor(
            df_display,
            use_container_width=True,
            height=500,
            hide_index=True,
            column_config={
                "ë§í¬": st.column_config.LinkColumn("ì±„ë„ ì—´ê¸°", display_text="ì—´ê¸°"),
            },
        )

    st.caption("ì—´ê¸° ë§í¬ë¥¼ ëˆ„ë¥´ë©´ ìƒˆ íƒ­ì—ì„œ ì˜ìƒ ë˜ëŠ” ì±„ë„ì´ ì—´ë¦½ë‹ˆë‹¤.")
