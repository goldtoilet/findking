#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import os
import json
import shutil
from datetime import datetime, timedelta, timezone
from pathlib import Path
import random

import streamlit as st

# --- ì„ íƒ ì˜ì¡´ì„±(ì¸ë„¤ì¼ ë¯¸ë¦¬ë³´ê¸°ìš©, ì—†ì–´ë„ ë™ì‘) ---
try:
    import requests
    PIL_AVAILABLE = True
except Exception:
    PIL_AVAILABLE = False

# --- YouTube Data API ---
try:
    from googleapiclient.discovery import build
    from googleapiclient.errors import HttpError
except Exception:
    build = None
    HttpError = Exception

# ============================
# ì €ì¥ ìœ„ì¹˜(iCloud) â€“ Tk ë²„ì „ê³¼ ë™ì¼
# ============================
ICLOUD_ROOT = Path.home() / "Library" / "Mobile Documents" / "com~apple~CloudDocs"
BASE_DIR = ICLOUD_ROOT / "youtubesearch"
BASE_DIR.mkdir(parents=True, exist_ok=True)

def _p(name: str) -> str:
    return str(BASE_DIR / name)

def _migrate(old: str, new: str):
    old = os.path.expanduser(old)
    if os.path.exists(old) and not os.path.exists(new):
        try:
            os.rename(old, new)
        except Exception:
            try:
                shutil.copy2(old, new)
            except Exception:
                pass

CONFIG_PATH       = _p("yts_config.json")
HISTORY_PATH      = _p("yts_search_history.json")
KEYWORD_LOG_PATH  = _p("yts_keyword_log.json")
QUOTA_PATH        = _p("yts_quota_usage.json")

# ê³¼ê±° dotíŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜
_migrate("~/.yts_config.json",         CONFIG_PATH)
_migrate("~/.yts_search_history.json", HISTORY_PATH)
_migrate("~/.yts_keyword_log.json",    KEYWORD_LOG_PATH)

# ----------------------------
# í™˜ê²½/ìƒìˆ˜
# ----------------------------
ENV_KEY_NAME = "YOUTUBE_API_KEY"
KST = timezone(timedelta(hours=9))
WEEKDAY_KO = ["ì›”ìš”ì¼","í™”ìš”ì¼","ìˆ˜ìš”ì¼","ëª©ìš”ì¼","ê¸ˆìš”ì¼","í† ìš”ì¼","ì¼ìš”ì¼"]

# êµ­ê°€/ì–¸ì–´ ì„ íƒ - UI ë¼ë²¨ â†’ (regionCode, relevanceLanguage)
COUNTRY_LANG_MAP = {
    "í•œêµ­": ("KR", "ko"),
    "ì¼ë³¸": ("JP", "ja"),
    "ë¯¸êµ­": ("US", "en"),
    "ì˜êµ­": ("GB", "en"),
    "ë…ì¼": ("DE", "de"),
    "í”„ë‘ìŠ¤": ("FR", "fr"),
    "ë¸Œë¼ì§ˆ": ("BR", "pt"),
    "ì¸ë„": ("IN", "en"),
    "ì¸ë„ë„¤ì‹œì•„": ("ID", "id"),
    "ë² íŠ¸ë‚¨": ("VN", "vi"),
    "íƒœêµ­": ("TH", "th"),
    "í•„ë¦¬í•€": ("PH", "en"),
}
COUNTRY_LIST = list(COUNTRY_LANG_MAP.keys())

# ----------------------------
# ê³µìš© JSON I/O
# ----------------------------
def _load_json(path, default):
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return default
    return default

def _save_json(path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# ----------------------------
# ì¿¼í„° ì €ì¥/ë¡œë“œ
# ----------------------------
def _load_quota_map():
    return _load_json(QUOTA_PATH, {})

def _save_quota_map(data: dict):
    _save_json(QUOTA_PATH, data)

def quota_today_key():
    return datetime.now(KST).strftime("%Y-%m-%d")

def get_today_quota_total() -> int:
    data = _load_quota_map()
    return int(data.get(quota_today_key(), 0))

def add_quota_usage(units: int):
    if units <= 0:
        return
    data = _load_quota_map()
    key = quota_today_key()
    data[key] = int(data.get(key, 0)) + int(units)
    _save_quota_map(data)

# ----------------------------
# API í‚¤ ê´€ë¦¬ (ê°„ë‹¨ ë²„ì „; Tkì™€ ë™ì¼í•œ config ì‚¬ìš©)
# ----------------------------
_DEFAULT_API_KEYS = []

def _load_api_keys_config():
    data = _load_json(CONFIG_PATH, {})
    keys = [k.strip() for k in (data.get("api_keys") or []) if k.strip()]
    if not keys and _DEFAULT_API_KEYS:
        keys = _DEFAULT_API_KEYS[:]
    sel = data.get("selected_index", 0)
    sel = max(0, min(sel, len(keys)-1)) if keys else 0
    return {"api_keys": keys, "selected_index": sel}

def _save_api_keys_config(keys, selected_index: int):
    keys = [k.strip() for k in keys if k.strip()]
    selected_index = max(0, min(selected_index, len(keys)-1)) if keys else 0
    _save_json(CONFIG_PATH, {"api_keys": keys, "selected_index": selected_index})

API_KEYS_STATE = {
    "keys": [],
    "index": 0,
}

def _apply_env_key(key: str):
    if key:
        os.environ[ENV_KEY_NAME] = key
    else:
        os.environ.pop(ENV_KEY_NAME, None)

def init_api_keys_state():
    cfg = _load_api_keys_config()
    API_KEYS_STATE["keys"] = cfg["api_keys"]
    API_KEYS_STATE["index"] = cfg["selected_index"]
    if API_KEYS_STATE["keys"]:
        _apply_env_key(API_KEYS_STATE["keys"][API_KEYS_STATE["index"]])
    else:
        _apply_env_key("")

def get_current_api_key() -> str:
    if not API_KEYS_STATE["keys"]:
        return ""
    return API_KEYS_STATE["keys"][API_KEYS_STATE["index"]]

def save_api_keys_from_user(keys: list[str], selected_index: int = 0):
    if not keys:
        _save_api_keys_config([], 0)
        API_KEYS_STATE["keys"] = []
        API_KEYS_STATE["index"] = 0
        _apply_env_key("")
        return
    _save_api_keys_config(keys, selected_index)
    cfg = _load_api_keys_config()
    API_KEYS_STATE["keys"] = cfg["api_keys"]
    API_KEYS_STATE["index"] = cfg["selected_index"]
    _apply_env_key(API_KEYS_STATE["keys"][API_KEYS_STATE["index"]])

# ----------------------------
# YouTube í´ë¼ì´ì–¸íŠ¸
# ----------------------------
def get_youtube_client():
    if build is None:
        raise RuntimeError(
            "google-api-python-clientê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
            "í„°ë¯¸ë„ì—ì„œ ì•„ë˜ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”:\n\n"
            "pip install google-api-python-client"
        )
    key = get_current_api_key()
    if not key:
        raise RuntimeError("API í‚¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ API í‚¤ë¥¼ ì…ë ¥/ì €ì¥í•˜ì„¸ìš”.")
    try:
        return build("youtube", "v3", developerKey=key, cache_discovery=False)
    except TypeError:
        return build("youtube", "v3", developerKey=key)

# ----------------------------
# ê¸°ë¡/ë¡œê·¸
# ----------------------------
def _load_history_raw():
    return _load_json(HISTORY_PATH, {})

def _save_history_raw(data: dict):
    try:
        _save_json(HISTORY_PATH, data)
    except Exception:
        pass

def add_to_history(query: str, limit_per_day: int = 100):
    q = (query or "").strip()
    if not q:
        return
    data = _load_history_raw()
    today = datetime.now(KST).strftime("%Y-%m-%d")
    lst = data.get(today, [])
    lst = [x for x in lst if x != q]
    lst.insert(0, q)
    data[today] = lst[:limit_per_day]
    _save_history_raw(data)

def _load_keyword_log():
    return _load_json(KEYWORD_LOG_PATH, [])

def _save_keyword_log(entries: list):
    try:
        _save_json(KEYWORD_LOG_PATH, entries)
    except Exception:
        pass

def append_keyword_log(query: str):
    q = (query or "").strip()
    if not q:
        return
    entries = _load_keyword_log()
    now = datetime.now(KST).isoformat(timespec="seconds")
    entries.append({"ts": now, "q": q})
    _save_keyword_log(entries)

def get_recent_keywords(days: int = 14, limit: int = 50):
    cutoff = datetime.now(KST) - timedelta(days=days)
    out = []
    for item in _load_keyword_log():
        ts = item.get("ts"); q = item.get("q")
        if not ts or not q:
            continue
        try:
            dt = datetime.fromisoformat(ts)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=KST)
            dt_kst = dt.astimezone(KST)
        except Exception:
            continue
        if dt_kst >= cutoff:
            out.append((dt_kst, q))
    out.sort(key=lambda x: x[0], reverse=True)
    return out[:limit]

# ----------------------------
# ì‹œê°„/ê¸¸ì´ ìœ í‹¸
# ----------------------------
def format_k_datetime_simple(dt_aw: datetime) -> str:
    if dt_aw.tzinfo is None:
        dt_aw = dt_aw.replace(tzinfo=KST)
    dt = dt_aw.astimezone(KST)
    return f"{dt.month}ì›” {dt.day}ì¼ {dt.hour}ì‹œ {dt.minute}ë¶„"

def parse_published_at_to_kst(published_iso: str) -> datetime:
    dt_utc = datetime.fromisoformat(published_iso.replace("Z", "+00:00"))
    return dt_utc.astimezone(KST)

def human_elapsed_days_hours(later: datetime, earlier: datetime) -> (int, int):
    delta = later - earlier
    if delta.total_seconds() < 0:
        return 0, 0
    days = delta.days
    hours = delta.seconds // 3600
    return days, hours

def published_after_from_label(label: str):
    label = label.strip()
    now_utc = datetime.now(timezone.utc)
    if label.endswith("ì¼"):
        days = int(label[:-1]); dt = now_utc - timedelta(days=days)
    else:
        return None
    return dt.isoformat(timespec="seconds").replace("+00:00", "Z")

def cutoff_dt_from_label_kst(label: str) -> datetime:
    label = label.strip()
    now_kst = datetime.now(KST)
    if label.endswith("ì¼"):
        return now_kst - timedelta(days=int(label[:-1]))
    return now_kst

def parse_duration_iso8601(iso_dur: str) -> int:
    h = m = s = 0
    if not iso_dur or not iso_dur.startswith("PT"):
        return 0
    num = ""
    for ch in iso_dur[2:]:
        if ch.isdigit():
            num += ch
        else:
            if ch == "H" and num:
                h = int(num); num = ""
            elif ch == "M" and num:
                m = int(num); num = ""
            elif ch == "S" and num:
                s = int(num); num = ""
    return h*3600 + m*60 + s

def format_duration_hms(seconds: int) -> str:
    if seconds <= 0:
        return "0:00"
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    if h > 0:
        return f"{h}:{m:02d}:{s:02d}"
    return f"{m}:{s:02d}"

def duration_filter_ok(seconds: int, label: str) -> bool:
    if label == "ì „ì²´": return True
    if label == "ì‡¼ì¸ ": return seconds < 60
    if label == "ë¡±í¼": return seconds >= 60
    if label == "1ë¶„~20ë¶„": return 60 <= seconds < 20*60
    if label == "20ë¶„~40ë¶„": return 20*60 <= seconds < 40*60
    if label == "40ë¶„~60ë¶„": return 40*60 <= seconds < 60*60
    if label == "60ë¶„ì´ìƒ": return seconds >= 60*60
    return True

def _chunked(seq, n):
    for i in range(0, len(seq), n):
        yield seq[i:i+n]

# ----------------------------
# YouTube API í˜¸ì¶œ: í‚¤ì›Œë“œ ì˜ìƒ ê²€ìƒ‰
# ----------------------------
def search_videos(query: str, min_views: int, period_label: str, duration_label: str,
                  max_fetch: int = 200,
                  region_code: str | None = None, lang_code: str | None = None):
    youtube = get_youtube_client()
    published_after = published_after_from_label(period_label)

    cost_used = 0
    max_fetch = max(1, min(int(max_fetch or 200), 5000))

    results_tmp = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        try:
            kwargs = dict(q=query, part="id", type="video", maxResults=take)
            if published_after:
                kwargs["publishedAfter"] = published_after
            if region_code:
                kwargs["regionCode"] = region_code
            if lang_code:
                kwargs["relevanceLanguage"] = lang_code
            if next_token:
                kwargs["pageToken"] = next_token

            search_response = youtube.search().list(**kwargs).execute()
            cost_used += 100
        except HttpError as e:
            raise RuntimeError(f"Search API ì˜¤ë¥˜: {e}")

        page_ids = [it["id"]["videoId"] for it in search_response.get("items", [])
                    if "id" in it and "videoId" in it["id"]]
        if not page_ids:
            break

        try:
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(page_ids)
            ).execute()
            cost_used += 1
        except HttpError as e:
            raise RuntimeError(f"Videos API ì˜¤ë¥˜: {e}")

        items = video_response.get("items", [])
        for item in items:
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}
            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            thumbs = snip.get("thumbnails", {})
            thumb_url = (thumbs.get("high", {}) or {}).get("url") \
                        or (thumbs.get("medium", {}) or {}).get("url") \
                        or (thumbs.get("default", {}) or {}).get("url") \
                        or ""
            seconds = parse_duration_iso8601(cdet.get("duration", ""))

            if not duration_filter_ok(seconds, duration_label): 
                continue
            if view_count < min_views: 
                continue

            results_tmp.append({
                "title": title,
                "views": view_count,
                "published_at_iso": published_at_iso,
                "url": url,
                "thumbnail_url": thumb_url,
                "duration_sec": seconds,
                "channel_id": snip.get("channelId", ""),
                "channel_title": snip.get("channelTitle", ""),
            })

        fetched += len(page_ids)
        next_token = search_response.get("nextPageToken")
        if not next_token:
            break

    if not results_tmp:
        return [], cost_used

    # ì±„ë„ í†µê³„
    channel_ids = {r["channel_id"] for r in results_tmp if r.get("channel_id")}
    channels_map = {}
    try:
        for batch in _chunked(list(channel_ids), 50):
            ch_resp = youtube.channels().list(
                part="snippet,statistics",
                id=",".join(batch)
            ).execute()
            cost_used += 1
            for c in ch_resp.get("items", []):
                cid = c.get("id")
                cstats = c.get("statistics", {}) or {}
                subs = cstats.get("subscriberCount")
                subs_int = int(subs) if subs is not None else None
                channels_map[cid] = {
                    "title": (c.get("snippet", {}) or {}).get("title", ""),
                    "subs": subs_int,
                    "views": int(cstats.get("viewCount", 0)),
                    "videos": int(cstats.get("videoCount", 0)),
                }
    except HttpError:
        channels_map = {}

    results = []
    for r in results_tmp:
        cinfo = channels_map.get(r["channel_id"], {})
        r.update({
            "channel_subs": cinfo.get("subs"),
            "channel_total_views": cinfo.get("views", 0),
            "channel_video_count": cinfo.get("videos", 0),
            "channel_title": cinfo.get("title", r.get("channel_title", "")),
        })
        results.append(r)

    results.sort(key=lambda x: x["views"], reverse=True)
    return results, cost_used

# ----------------------------
# YouTube API: ì±„ë„ ë‚´ë¶€ ì˜ìƒ ê²€ìƒ‰
# ----------------------------
def search_videos_in_channel_by_name(channel_query: str, min_views: int, period_label: str,
                                     duration_label: str, max_fetch: int = 200,
                                     region_code: str | None = None, lang_code: str | None = None):
    youtube = get_youtube_client()
    published_after = published_after_from_label(period_label)

    cost_used = 0
    max_fetch = max(1, min(int(max_fetch or 200), 5000))

    # 1) ì±„ë„ ì°¾ê¸°
    try:
        kwargs_ch = dict(part="id,snippet", q=channel_query, type="channel", maxResults=1)
        if region_code: kwargs_ch["regionCode"] = region_code
        if lang_code:   kwargs_ch["relevanceLanguage"] = lang_code
        ch_resp = youtube.search().list(**kwargs_ch).execute()
        cost_used += 100
    except HttpError as e:
        raise RuntimeError(f"ì±„ë„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

    items = ch_resp.get("items", [])
    if not items:
        return [], cost_used
    channel_id = items[0]["id"]["channelId"]

    # 2) í•´ë‹¹ ì±„ë„ì˜ ì˜ìƒë“¤
    results_tmp = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        try:
            kwargs = dict(part="id", type="video", channelId=channel_id, maxResults=take, order="date")
            if published_after:
                kwargs["publishedAfter"] = published_after
            if region_code:
                kwargs["regionCode"] = region_code
            if lang_code:
                kwargs["relevanceLanguage"] = lang_code
            if next_token:
                kwargs["pageToken"] = next_token

            v_search = youtube.search().list(**kwargs).execute()
            cost_used += 100
        except HttpError as e:
            raise RuntimeError(f"ì±„ë„ ì˜ìƒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

        page_ids = [it["id"]["videoId"] for it in v_search.get("items", [])
                    if "id" in it and "videoId" in it["id"]]
        if not page_ids:
            break

        try:
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(page_ids)
            ).execute()
            cost_used += 1
        except HttpError as e:
            raise RuntimeError(f"Videos API ì˜¤ë¥˜: {e}")

        items2 = video_response.get("items", [])
        for item in items2:
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}
            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            thumbs = snip.get("thumbnails", {})
            thumb_url = (thumbs.get("high", {}) or {}).get("url") \
                        or (thumbs.get("medium", {}) or {}).get("url") \
                        or (thumbs.get("default", {}) or {}).get("url") \
                        or ""
            seconds = parse_duration_iso8601(cdet.get("duration", ""))

            if not duration_filter_ok(seconds, duration_label): 
                continue
            if view_count < min_views: 
                continue

            results_tmp.append({
                "title": title,
                "views": view_count,
                "published_at_iso": published_at_iso,
                "url": url,
                "thumbnail_url": thumb_url,
                "duration_sec": seconds,
                "channel_id": channel_id,
                "channel_title": snip.get("channelTitle", ""),
            })

        fetched += len(page_ids)
        next_token = v_search.get("nextPageToken")
        if not next_token:
            break

    if not results_tmp:
        return [], cost_used

    # ì±„ë„ ë©”íƒ€
    channels_map = {}
    try:
        ch_resp2 = youtube.channels().list(
            part="snippet,statistics",
            id=channel_id
        ).execute()
        cost_used += 1
        for c in ch_resp2.get("items", []):
            cid = c.get("id")
            cstats = c.get("statistics", {}) or {}
            subs = cstats.get("subscriberCount")
            subs_int = int(subs) if subs is not None else None
            channels_map[cid] = {
                "title": (c.get("snippet", {}) or {}).get("title", ""),
                "subs": subs_int,
                "views": int(cstats.get("viewCount", 0)),
                "videos": int(cstats.get("videoCount", 0)),
            }
    except HttpError:
        channels_map = {}

    results = []
    for r in results_tmp:
        cinfo = channels_map.get(r["channel_id"], {})
        r.update({
            "channel_subs": cinfo.get("subs"),
            "channel_total_views": cinfo.get("views", 0),
            "channel_video_count": cinfo.get("videos", 0),
            "channel_title": cinfo.get("title", r.get("channel_title", "")),
        })
        results.append(r)

    results.sort(key=lambda x: x["views"], reverse=True)
    return results, cost_used

# ----------------------------
# YouTube API: ì±„ë„ í‚¤ì›Œë“œë¡œ ì±„ë„ ì°¾ê¸°
# ----------------------------
def search_channels_by_keyword(keyword: str, max_fetch: int = 50,
                               region_code: str | None = None, lang_code: str | None = None):
    youtube = get_youtube_client()
    cost_used = 0
    max_fetch = max(1, min(int(max_fetch or 50), 200))

    results = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        try:
            kwargs = dict(part="id,snippet", q=keyword, type="channel", maxResults=take)
            if region_code: kwargs["regionCode"] = region_code
            if lang_code:   kwargs["relevanceLanguage"] = lang_code
            if next_token:
                kwargs["pageToken"] = next_token

            resp = youtube.search().list(**kwargs).execute()
            cost_used += 100
        except HttpError as e:
            raise RuntimeError(f"ì±„ë„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

        items = resp.get("items", [])
        if not items:
            break

        channel_ids = [it["id"]["channelId"] for it in items
                       if "id" in it and "channelId" in it["id"]]

        try:
            ch_resp = youtube.channels().list(
                part="snippet,statistics",
                id=",".join(channel_ids)
            ).execute()
            cost_used += 1
        except HttpError as e:
            raise RuntimeError(f"ì±„ë„ ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")

        for c in ch_resp.get("items", []):
            cid = c.get("id")
            snip = c.get("snippet", {}) or {}
            stats = c.get("statistics", {}) or {}
            subs = stats.get("subscriberCount")
            subs_int = int(subs) if subs is not None else None
            results.append({
                "channel_id": cid,
                "channel_title": snip.get("title", ""),
                "description": snip.get("description", ""),
                "subs": subs_int,
                "total_views": int(stats.get("viewCount", 0)),
                "video_count": int(stats.get("videoCount", 0)),
                "url": f"https://www.youtube.com/channel/{cid}",
            })

        fetched += len(items)
        next_token = resp.get("nextPageToken")
        if not next_token:
            break

    # êµ¬ë…ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    results.sort(key=lambda x: x.get("subs") or 0, reverse=True)
    return results, cost_used

# ===================================================================
# ì•„ë˜ë¶€í„° Streamlit UI
# ===================================================================

st.set_page_config(page_title="YouTube ê²€ìƒ‰ê¸° (Streamlit)", page_icon="ğŸ¬", layout="wide")

# API í‚¤ ìƒíƒœ ì´ˆê¸°í™”
init_api_keys_state()

# ì„¸ì…˜ ìƒíƒœ ê¸°ë³¸ê°’
if "results" not in st.session_state:
    st.session_state["results"] = []
if "result_type" not in st.session_state:
    st.session_state["result_type"] = None
if "quota_last_cost" not in st.session_state:
    st.session_state["quota_last_cost"] = 0

# ----------------------------
# ì‚¬ì´ë“œë°” (ì™¼ìª½)
# ----------------------------
with st.sidebar:
    st.markdown("### ğŸ” ê²€ìƒ‰ ì„¤ì •")

    # ì¼ë°˜ ê²€ìƒ‰ì–´
    query = st.text_input("ì¼ë°˜ ê²€ìƒ‰ì–´", value="")

    col_btn1, col_btn2 = st.columns(2)
    do_general = col_btn1.button("ì¼ë°˜ ê²€ìƒ‰", use_container_width=True)
    do_trend   = col_btn2.button("íŠ¸ë Œë“œ ê²€ìƒ‰", use_container_width=True)  # ì…ë ¥ì¹¸ ì—†ëŠ” íŠ¸ë Œë“œ ë²„íŠ¼

    st.markdown("---")

    # ì±„ë„ í‚¤ì›Œë“œë¡œ ì±„ë„ ì°¾ê¸°
    ch_keyword = st.text_input("ì±„ë„ í‚¤ì›Œë“œ (ì±„ë„ ì°¾ê¸°)", value="")
    do_channel_find = st.button("ì±„ë„ í‚¤ì›Œë“œë¡œ ì±„ë„ì°¾ê¸°", use_container_width=True)

    # ì±„ë„ ê²€ìƒ‰ì–´ë¡œ ì±„ë„ ì˜ìƒ ê²€ìƒ‰
    ch_exact = st.text_input("ì±„ë„ ê²€ìƒ‰ì–´ (ì±„ë„ ì´ë¦„)", value="")
    do_channel_videos = st.button("ì±„ë„ ì˜ìƒ ê²€ìƒ‰", use_container_width=True)

    st.markdown("---")

    with st.expander("ğŸ“Œ ê²€ìƒ‰ ì˜µì…˜ (ê¸°ê°„/ê¸¸ì´/ì§€ì—­)", expanded=False):
        period_options = ["30ì¼", "90ì¼", "365ì¼"]
        period_label = st.selectbox("ê²€ìƒ‰ê¸°ê°„(ì„œë²„)", period_options, index=1)

        client_period_options = ["30ì¼", "90ì¼", "365ì¼", "3650ì¼"]
        client_period = st.selectbox("ì—…ë¡œë“œ ê¸°ê°„(í•„í„°)", client_period_options, index=1)

        dur_options = ["ì „ì²´","ì‡¼ì¸ ","ë¡±í¼","1ë¶„~20ë¶„","20ë¶„~40ë¶„","40ë¶„~60ë¶„","60ë¶„ì´ìƒ"]
        dur_label = st.selectbox("ì˜ìƒ ê¸¸ì´", dur_options, index=0)

        min_views_str = st.selectbox("ìµœì†Œ ì¡°íšŒìˆ˜", ["5,000","10,000","50,000","100,000","500,000","1,000,000"], index=0)
        max_fetch = st.number_input("ê°€ì ¸ì˜¬ ìµœëŒ€ ê°œìˆ˜", min_value=10, max_value=500, value=50, step=10)

        country_label = st.selectbox("êµ­ê°€/ì–¸ì–´", COUNTRY_LIST, index=0)
        region_code, lang_code = COUNTRY_LANG_MAP.get(country_label, ("KR","ko"))

    st.markdown("---")

    # ì•„ë˜ìª½: API í‚¤ + ìµœê·¼ ê²€ìƒ‰ í‚¤ì›Œë“œ
    st.markdown("### ğŸ”‘ YouTube API í‚¤ (ì•„ë˜)")

    existing_keys = API_KEYS_STATE["keys"]
    keys_text_default = "\n".join(existing_keys) if existing_keys else ""
    api_keys_text = st.text_area(
        "API í‚¤ ëª©ë¡ (í•œ ì¤„ì— í•œ ê°œ)",
        value=keys_text_default,
        height=80
    )
    if st.button("API í‚¤ ì €ì¥", use_container_width=True):
        keys = [line.strip() for line in api_keys_text.splitlines() if line.strip()]
        save_api_keys_from_user(keys, 0)
        st.success("API í‚¤ ëª©ë¡ì„ ì €ì¥í•˜ê³  1ë²ˆ í‚¤ë¥¼ í™œì„±í™”í–ˆìŠµë‹ˆë‹¤. (config.json)")

    st.markdown("---")
    st.markdown("### ğŸ•’ ìµœê·¼ ê²€ìƒ‰ í‚¤ì›Œë“œ")

    recent = get_recent_keywords(days=14, limit=20)
    if recent:
        for dt_kst, q in recent:
            st.write(f"- {format_k_datetime_simple(dt_kst)} Â· {q}")
    else:
        st.write("ìµœê·¼ ê¸°ë¡ ì—†ìŒ")

    st.markdown("---")
    today_total = get_today_quota_total()
    st.caption(f"ì˜¤ëŠ˜ ì‚¬ìš©í•œ YouTube API ì¿¼í„° ì¶”ì •: {today_total} units\n"
               f"(ë§ˆì§€ë§‰ ê²€ìƒ‰: {st.session_state['quota_last_cost']} units)")

# ----------------------------
# ë©”ì¸ ì˜ì—­ (ì˜¤ë¥¸ìª½)
# ----------------------------
st.title("ğŸ¬ YouTube ê²€ìƒ‰ê¸° (Streamlit ë²„ì „)")

# ì–´ë–¤ ë²„íŠ¼ì´ ëˆŒë ¸ëŠ”ì§€ì— ë”°ë¼ ê²€ìƒ‰ ì‹¤í–‰
error_msg = None

def _parse_min_views(txt: str) -> int:
    return int(txt.replace(",", "").replace(" ", ""))

def _filter_by_client_period_and_duration(items, client_period_label, dur_label):
    cutoff = cutoff_dt_from_label_kst(client_period_label)
    out = []
    now_kst = datetime.now(KST)
    for r in items:
        pub_kst = parse_published_at_to_kst(r["published_at_iso"])
        if pub_kst < cutoff:
            continue
        if not duration_filter_ok(r["duration_sec"], dur_label):
            continue
        # ì‹œê°„ë‹¹ í´ë¦­ìˆ˜ ê³„ì‚°
        d, h = human_elapsed_days_hours(now_kst, pub_kst)
        total_hours = max(1, d*24 + h)
        r["clicks_per_hour"] = int(round(r["views"] / total_hours))
        r["published_kst"] = pub_kst
        out.append(r)
    return out

try:
    if do_general:
        if not query.strip():
            error_msg = "ì¼ë°˜ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
        else:
            min_views = _parse_min_views(min_views_str)
            results, cost = search_videos(
                query=query.strip(),
                min_views=min_views,
                period_label=period_label,
                duration_label="ì „ì²´",  # ì„œë²„ ì¿¼ë¦¬ëŠ” ì „ì²´, í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë‹¤ì‹œ í•„í„°
                max_fetch=max_fetch,
                region_code=region_code,
                lang_code=lang_code
            )
            add_quota_usage(cost)
            st.session_state["quota_last_cost"] = cost

            add_to_history(query.strip())
            append_keyword_log(query.strip())

            filtered = _filter_by_client_period_and_duration(results, client_period, dur_label)

            st.session_state["results"] = filtered
            st.session_state["result_type"] = "general"

    elif do_trend:
        if not query.strip():
            error_msg = "íŠ¸ë Œë“œ ê²€ìƒ‰ë„ ê¸°ë³¸ ê²€ìƒ‰ì–´ëŠ” í•„ìš”í•©ë‹ˆë‹¤."
        else:
            min_views = _parse_min_views(min_views_str)
            results, cost = search_videos(
                query=query.strip(),
                min_views=min_views,
                period_label=period_label,
                duration_label="ì „ì²´",
                max_fetch=max_fetch,
                region_code=region_code,
                lang_code=lang_code
            )
            add_quota_usage(cost)
            st.session_state["quota_last_cost"] = cost

            # íŠ¸ë Œë“œ í‘œì‹œìš© íƒœê·¸
            add_to_history(f"[trend]{query.strip()}")
            append_keyword_log(f"[trend]{query.strip()}")

            filtered = _filter_by_client_period_and_duration(results, client_period, dur_label)

            st.session_state["results"] = filtered
            st.session_state["result_type"] = "trend"

    elif do_channel_find:
        if not ch_keyword.strip():
            error_msg = "ì±„ë„ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”."
        else:
            results, cost = search_channels_by_keyword(
                keyword=ch_keyword.strip(),
                max_fetch=max_fetch,
                region_code=region_code,
                lang_code=lang_code
            )
            add_quota_usage(cost)
            st.session_state["quota_last_cost"] = cost

            add_to_history(f"[channel-find]{ch_keyword.strip()}")
            append_keyword_log(f"[channel-find]{ch_keyword.strip()}")

            st.session_state["results"] = results
            st.session_state["result_type"] = "channel_find"

    elif do_channel_videos:
        if not ch_exact.strip():
            error_msg = "ì±„ë„ ê²€ìƒ‰ì–´(ì±„ë„ ì´ë¦„)ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
        else:
            min_views = _parse_min_views(min_views_str)
            results, cost = search_videos_in_channel_by_name(
                channel_query=ch_exact.strip(),
                min_views=min_views,
                period_label=period_label,
                duration_label="ì „ì²´",
                max_fetch=max_fetch,
                region_code=region_code,
                lang_code=lang_code
            )
            add_quota_usage(cost)
            st.session_state["quota_last_cost"] = cost

            add_to_history(f"[channel]{ch_exact.strip()}")
            append_keyword_log(f"[channel]{ch_exact.strip()}")

            filtered = _filter_by_client_period_and_duration(results, client_period, dur_label)

            st.session_state["results"] = filtered
            st.session_state["result_type"] = "channel_videos"

except Exception as e:
    error_msg = str(e)

if error_msg:
    st.error(error_msg)

# ----------------------------
# ê²°ê³¼ ì œëª© / ë¦¬ìŠ¤íŠ¸ í‘œì‹œ
# ----------------------------
result_type = st.session_state.get("result_type")
results = st.session_state.get("results") or []

title_map = {
    "general": "ğŸ“„ ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸",
    "trend": "ğŸ“ˆ íŠ¸ë Œë“œ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸",
    "channel_find": "ğŸ“‚ ì±„ë„ê²€ìƒ‰ ë¦¬ìŠ¤íŠ¸",
    "channel_videos": "ğŸ ì±„ë„ ì˜ìƒ ë¦¬ìŠ¤íŠ¸",
}

if result_type is None or not results:
    st.info("ì™¼ìª½ì—ì„œ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ê³  ë²„íŠ¼ì„ ëˆŒëŸ¬ ê²€ìƒ‰ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
else:
    st.subheader(title_map.get(result_type, "ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸"))

    if result_type in ("general", "trend", "channel_videos"):
        # ì˜ìƒ ê²°ê³¼ í…Œì´ë¸”
        import pandas as pd
        rows = []
        for r in results:
            pub_kst = r.get("published_kst") or parse_published_at_to_kst(r["published_at_iso"])
            rows.append({
                "ì œëª©": r["title"],
                "ì±„ë„ëª…": r.get("channel_title", ""),
                "ì¡°íšŒìˆ˜": r["views"],
                "ì‹œê°„ë‹¹ í´ë¦­ìˆ˜": r.get("clicks_per_hour", None),
                "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                "ì—…ë¡œë“œì¼(KST)": pub_kst.strftime("%Y-%m-%d"),
                "URL": r["url"],
            })
        df = pd.DataFrame(rows)
        st.dataframe(df, use_container_width=True, height=600)

        # ìƒë‹¨ ëª‡ ê°œ ì¸ë„¤ì¼
        if PIL_AVAILABLE and results:
            st.markdown("#### ëŒ€í‘œ ì¸ë„¤ì¼ (ìƒìœ„ 3ê°œ)")
            thumb_cols = st.columns(min(3, len(results)))
            for i, col in enumerate(thumb_cols):
                r = results[i]
                url = r.get("thumbnail_url")
                if url:
                    with col:
                        st.image(url, use_column_width=True)
                        st.caption(r["title"][:40] + ("..." if len(r["title"]) > 40 else ""))

    elif result_type == "channel_find":
        # ì±„ë„ ë¦¬ìŠ¤íŠ¸
        import pandas as pd
        rows = []
        for c in results:
            rows.append({
                "ì±„ë„ëª…": c["channel_title"],
                "êµ¬ë…ììˆ˜": c["subs"],
                "ì´ì¡°íšŒìˆ˜": c["total_views"],
                "ì˜ìƒê°œìˆ˜": c["video_count"],
                "URL": c["url"],
                "ì„¤ëª…": (c["description"] or "")[:120] + ("..." if len(c["description"] or "") > 120 else "")
            })
        df = pd.DataFrame(rows)
        st.dataframe(df, use_container_width=True, height=600)

