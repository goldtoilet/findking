
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import json
import shutil
from datetime import datetime, timedelta, timezone
from pathlib import Path

import requests
import pandas as pd
import streamlit as st

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# ============================
# ê³µí†µ ìƒìˆ˜/í™˜ê²½
# ============================
KST = timezone(timedelta(hours=9))
WEEKDAY_KO = ["ì›”ìš”ì¼","í™”ìš”ì¼","ìˆ˜ìš”ì¼","ëª©ìš”ì¼","ê¸ˆìš”ì¼","í† ìš”ì¼","ì¼ìš”ì¼"]

# iCloud ê²½ë¡œ (ë¡œì»¬ ë§¥ì—ì„œ ì‹¤í–‰ ì‹œ)
ICLOUD_ROOT = Path.home() / "Library" / "Mobile Documents" / "com~apple~CloudDocs"
BASE_DIR = ICLOUD_ROOT / "youtubesearch"
BASE_DIR.mkdir(parents=True, exist_ok=True)

def _p(name: str) -> str:
    return str(BASE_DIR / name)

def _migrate(old: str, new: str):
    old = os.path.expanduser(old)
    if os.path.exists(old) and not os.path.exists(new):
        try:
            os.rename(old, new)
        except Exception:
            try:
                shutil.copy2(old, new)
            except Exception:
                pass

CONFIG_PATH       = _p("yts_config.json")
HISTORY_PATH      = _p("yts_search_history.json")
KEYWORD_LOG_PATH  = _p("yts_keyword_log.json")
QUOTA_PATH        = _p("yts_quota_usage.json")

_migrate("~/.yts_config.json",         CONFIG_PATH)
_migrate("~/.yts_search_history.json", HISTORY_PATH)
_migrate("~/.yts_keyword_log.json",    KEYWORD_LOG_PATH)

# ----------------------------
# êµ­ê°€/ì–¸ì–´ ì„ íƒ
# ----------------------------
COUNTRY_LANG_MAP = {
    "í•œêµ­": ("KR", "ko"),
    "ì¼ë³¸": ("JP", "ja"),
    "ì¤‘êµ­": ("CN", "zh"),
    "ëŒ€ë§Œ": ("TW", "zh"),
    "í™ì½©": ("HK", "zh"),
    "ì‹±ê°€í¬ë¥´": ("SG", "en"),
    "ë§ë ˆì´ì‹œì•„": ("MY", "ms"),
    "íƒœêµ­": ("TH", "th"),
    "ë² íŠ¸ë‚¨": ("VN", "vi"),
    "ì¸ë„": ("IN", "en"),
    "ì¸ë„ë„¤ì‹œì•„": ("ID", "id"),
    "í•„ë¦¬í•€": ("PH", "en"),
    "ë¯¸êµ­": ("US", "en"),
    "ìºë‚˜ë‹¤": ("CA", "en"),
    "ë©•ì‹œì½”": ("MX", "es"),
    "ë¸Œë¼ì§ˆ": ("BR", "pt"),
    "ì•„ë¥´í—¨í‹°ë‚˜": ("AR", "es"),
    "ì¹ ë ˆ": ("CL", "es"),
    "ì½œë¡¬ë¹„ì•„": ("CO", "es"),
    "íŽ˜ë£¨": ("PE", "es"),
    "ì˜êµ­": ("GB", "en"),
    "ë…ì¼": ("DE", "de"),
    "í”„ëž‘ìŠ¤": ("FR", "fr"),
    "ì´íƒˆë¦¬ì•„": ("IT", "it"),
    "ìŠ¤íŽ˜ì¸": ("ES", "es"),
    "í¬ë¥´íˆ¬ê°ˆ": ("PT", "pt"),
    "ë„¤ëœëž€ë“œ": ("NL", "nl"),
    "ë²¨ê¸°ì—": ("BE", "nl"),
    "ìŠ¤ì›¨ë´": ("SE", "sv"),
    "ë…¸ë¥´ì›¨ì´": ("NO", "no"),
    "ë´ë§ˆí¬": ("DK", "da"),
    "í•€ëž€ë“œ": ("FI", "fi"),
    "ìŠ¤ìœ„ìŠ¤": ("CH", "de"),
    "ì˜¤ìŠ¤íŠ¸ë¦¬ì•„": ("AT", "de"),
    "ì•„ì¼ëžœë“œ": ("IE", "en"),
    "í´ëž€ë“œ": ("PL", "pl"),
    "ì²´ì½”": ("CZ", "cs"),
    "ë£¨ë§ˆë‹ˆì•„": ("RO", "ro"),
    "í—ê°€ë¦¬": ("HU", "hu"),
    "ê·¸ë¦¬ìŠ¤": ("GR", "el"),
    "í„°í‚¤": ("TR", "tr"),
    "í˜¸ì£¼": ("AU", "en"),
    "ë‰´ì§ˆëžœë“œ": ("NZ", "en"),
    "ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„": ("SA", "ar"),
    "ì•„ëžì—ë¯¸ë¦¬íŠ¸": ("AE", "ar"),
    "ì´ìŠ¤ë¼ì—˜": ("IL", "he"),
    "ë‚¨ì•„í”„ë¦¬ì¹´ê³µí™”êµ­": ("ZA", "en"),
    "ë‚˜ì´ì§€ë¦¬ì•„": ("NG", "en"),
    "ì´ì§‘íŠ¸": ("EG", "ar"),
    "ì¼€ëƒ": ("KE", "en"),
    "ëŸ¬ì‹œì•„": ("RU", "ru"),
    "ìš°í¬ë¼ì´ë‚˜": ("UA", "uk"),
}
COUNTRY_LIST = list(COUNTRY_LANG_MAP.keys())

# ----------------------------
# ê³µìš© JSON I/O
# ----------------------------
def _load_json(path, default):
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return default
    return default

def _save_json(path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# ----------------------------
# ì¿¼í„° ì €ìž¥/ë¡œë“œ
# ----------------------------
def _load_quota_map():
    return _load_json(QUOTA_PATH, {})

def _save_quota_map(data: dict):
    _save_json(QUOTA_PATH, data)

def quota_today_key():
    return datetime.now(KST).strftime("%Y-%m-%d")

def get_today_quota_total() -> int:
    data = _load_quota_map()
    return int(data.get(quota_today_key(), 0))

def add_quota_usage(units: int):
    if units <= 0:
        return
    data = _load_quota_map()
    key = quota_today_key()
    data[key] = int(data.get(key, 0)) + int(units)
    _save_quota_map(data)

# ----------------------------
# API í‚¤ ê´€ë¦¬
# ----------------------------
ENV_KEY_NAME = "YOUTUBE_API_KEY"

# ê¸°ë³¸ê°’: ë¹„ì›Œë‘ê³ , ì‚¬ìš©ìžê°€ UIì—ì„œ ìž…ë ¥
_DEFAULT_API_KEYS = []

def _load_api_keys_config():
    data = _load_json(CONFIG_PATH, {})
    keys = [k.strip() for k in (data.get("api_keys") or []) if k.strip()]
    if not keys:
        keys = _DEFAULT_API_KEYS[:]  # ë¹„ì–´ìžˆìœ¼ë©´ ê·¸ëƒ¥ ë¹ˆ ë¦¬ìŠ¤íŠ¸
    sel = data.get("selected_index", 0)
    sel = max(0, min(sel, len(keys)-1)) if keys else 0
    return {"api_keys": keys, "selected_index": sel}

def _save_api_keys_config(keys, selected_index: int):
    keys = [k.strip() for k in keys if k.strip()]
    if keys:
        selected_index = max(0, min(selected_index, len(keys)-1))
    else:
        selected_index = 0
    _save_json(CONFIG_PATH, {"api_keys": keys, "selected_index": selected_index})

API_KEYS_STATE = {
    "keys": [],
    "index": 0,
}

def _apply_env_key(key: str):
    if key:
        os.environ[ENV_KEY_NAME] = key
    else:
        os.environ.pop(ENV_KEY_NAME, None)

def init_api_keys_state():
    cfg = _load_api_keys_config()
    API_KEYS_STATE["keys"] = cfg["api_keys"]
    API_KEYS_STATE["index"] = cfg["selected_index"]
    if API_KEYS_STATE["keys"]:
        _apply_env_key(API_KEYS_STATE["keys"][API_KEYS_STATE["index"]])
    else:
        _apply_env_key("")

def get_current_api_key() -> str:
    if not API_KEYS_STATE["keys"]:
        return ""
    return API_KEYS_STATE["keys"][API_KEYS_STATE["index"]]

def select_api_key(index: int):
    if not API_KEYS_STATE["keys"]:
        return
    API_KEYS_STATE["index"] = max(0, min(index, len(API_KEYS_STATE["keys"])-1))
    _apply_env_key(API_KEYS_STATE["keys"][API_KEYS_STATE["index"]])
    _save_api_keys_config(API_KEYS_STATE["keys"], API_KEYS_STATE["index"])

def save_api_keys_from_user(keys):
    if not keys:
        return
    _save_api_keys_config(keys, 0)
    cfg = _load_api_keys_config()
    API_KEYS_STATE["keys"] = cfg["api_keys"]
    API_KEYS_STATE["index"] = cfg["selected_index"]
    if API_KEYS_STATE["keys"]:
        _apply_env_key(API_KEYS_STATE["keys"][API_KEYS_STATE["index"]])
    else:
        _apply_env_key("")

def clear_all_api_keys():
    _save_json(CONFIG_PATH, {"api_keys": [], "selected_index": 0})
    API_KEYS_STATE["keys"] = []
    API_KEYS_STATE["index"] = 0
    os.environ.pop(ENV_KEY_NAME, None)

def get_youtube_client():
    if not API_KEYS_STATE["keys"]:
        raise RuntimeError("API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ í‚¤ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
    key = get_current_api_key()
    if not key:
        raise RuntimeError("ì„ íƒëœ API í‚¤ê°€ ë¹„ì–´ ìžˆìŠµë‹ˆë‹¤.")
    try:
        return build("youtube", "v3", developerKey=key, cache_discovery=False)
    except TypeError:
        return build("youtube", "v3", developerKey=key)

# ----------------------------
# ì‹œê°„/ê¸¸ì´ ìœ í‹¸
# ----------------------------
def format_k_datetime_aw(dt_aw: datetime) -> str:
    if dt_aw.tzinfo is None:
        dt_aw = dt_aw.replace(tzinfo=KST)
    dt = dt_aw.astimezone(KST)
    wd = WEEKDAY_KO[dt.weekday()]
    hour_24 = dt.hour
    ampm = "ì˜¤ì „" if hour_24 < 12 else "ì˜¤í›„"
    hour_12 = hour_24 % 12 or 12
    return f"{dt.month}ì›”{dt.day}ì¼ {wd} {ampm}{hour_12}ì‹œ {dt.minute}ë¶„"

def format_k_datetime_simple(dt_aw: datetime) -> str:
    if dt_aw.tzinfo is None:
        dt_aw = dt_aw.replace(tzinfo=KST)
    dt = dt_aw.astimezone(KST)
    return f"{dt.month}ì›” {dt.day}ì¼ {dt.hour}ì‹œ {dt.minute}ë¶„"

def parse_published_at_to_kst(published_iso: str) -> datetime:
    dt_utc = datetime.fromisoformat(published_iso.replace("Z", "+00:00"))
    return dt_utc.astimezone(KST)

def human_elapsed_days_hours(later: datetime, earlier: datetime) -> (int, int):
    delta = later - earlier
    if delta.total_seconds() < 0:
        return 0, 0
    days = delta.days
    hours = delta.seconds // 3600
    return days, hours

def published_after_from_label(label: str):
    label = label.strip()
    if label == "ì œí•œì—†ìŒ":
        return None
    now_utc = datetime.now(timezone.utc)
    if label.endswith("ì‹œê°„"):
        hours = int(label[:-2])
        dt = now_utc - timedelta(hours=hours)
    elif label.endswith("ì¼"):
        days = int(label[:-1])
        dt = now_utc - timedelta(days=days)
    else:
        return None
    return dt.isoformat(timespec="seconds").replace("+00:00", "Z")

def cutoff_dt_from_label_kst(label: str) -> datetime:
    label = label.strip()
    now_kst = datetime.now(KST)
    if label.endswith("ì‹œê°„"):
        return now_kst - timedelta(hours=int(label[:-2]))
    if label.endswith("ì¼"):
        return now_kst - timedelta(days=int(label[:-1]))
    return now_kst

def parse_duration_iso8601(iso_dur: str) -> int:
    h = m = s = 0
    if not iso_dur or not iso_dur.startswith("PT"):
        return 0
    num = ""
    for ch in iso_dur[2:]:
        if ch.isdigit():
            num += ch
        else:
            if ch == "H" and num:
                h = int(num); num = ""
            elif ch == "M" and num:
                m = int(num); num = ""
            elif ch == "S" and num:
                s = int(num); num = ""
    return h*3600 + m*60 + s

def format_duration_hms(seconds: int) -> str:
    if seconds <= 0:
        return "0:00"
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    if h > 0:
        return f"{h}:{m:02d}:{s:02d}"
    return f"{m}:{s:02d}"

def duration_filter_ok(seconds: int, label: str) -> bool:
    if label == "ì „ì²´": return True
    if label == "ì‡¼ì¸ ": return seconds < 60
    if label == "ë¡±í¼": return seconds >= 60
    if label == "1ë¶„~20ë¶„": return 60 <= seconds < 20*60
    if label == "20ë¶„~40ë¶„": return 20*60 <= seconds < 40*60
    if label == "40ë¶„~60ë¶„": return 40*60 <= seconds < 60*60
    if label == "60ë¶„ì´ìƒ": return seconds >= 60*60
    return True

def parse_min_views(text: str) -> int:
    digits = text.replace(",", "").replace(" ", "").replace("ë§Œ", "0000")
    try:
        return int(digits)
    except Exception:
        return 0

# ----------------------------
# ê²€ìƒ‰ ížˆìŠ¤í† ë¦¬ / í‚¤ì›Œë“œ ë¡œê·¸
# ----------------------------
def _load_history_raw():
    return _load_json(HISTORY_PATH, {})

def _save_history_raw(data: dict):
    try:
        _save_json(HISTORY_PATH, data)
    except Exception:
        pass

def add_to_history(query: str, limit_per_day: int = 100):
    q = (query or "").strip()
    if not q:
        return
    data = _load_history_raw()
    today = datetime.now(KST).strftime("%Y-%m-%d")
    lst = data.get(today, [])
    lst = [x for x in lst if x != q]
    lst.insert(0, q)
    data[today] = lst[:limit_per_day]
    _save_history_raw(data)

def _load_keyword_log():
    return _load_json(KEYWORD_LOG_PATH, [])

def _save_keyword_log(entries: list):
    try:
        _save_json(KEYWORD_LOG_PATH, entries)
    except Exception:
        pass

def append_keyword_log(query: str):
    q = (query or "").strip()
    if not q:
        return
    entries = _load_keyword_log()
    now = datetime.now(KST).isoformat(timespec="seconds")
    entries.append({"ts": now, "q": q})
    _save_keyword_log(entries)

def get_recent_keywords(days: int = 14):
    cutoff = datetime.now(KST) - timedelta(days=days)
    out = []
    for item in _load_keyword_log():
        ts = item.get("ts"); q = item.get("q")
        if not ts or not q:
            continue
        try:
            dt = datetime.fromisoformat(ts)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=KST)
            dt_kst = dt.astimezone(KST)
        except Exception:
            continue
        if dt_kst >= cutoff:
            out.append((dt_kst, q))
    out.sort(key=lambda x: x[0], reverse=True)
    return out

# ----------------------------
# YouTube API ê²€ìƒ‰ í•¨ìˆ˜
# ----------------------------
def _chunked(seq, n):
    for i in range(0, len(seq), n):
        yield seq[i:i+n]

def search_videos(query: str, min_views: int, period_label: str, duration_label: str,
                  max_fetch: int = 200,
                  region_code: str | None = None, lang_code: str | None = None):
    youtube = get_youtube_client()
    published_after = published_after_from_label(period_label)
    cost_used = 0
    breakdown = {"search.list": 0, "videos.list": 0, "channels.list": 0}
    max_fetch = max(1, min(int(max_fetch or 200), 5000))

    results_tmp = []
    channel_ids = set()
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        try:
            kwargs = dict(q=query, part="id", type="video", maxResults=take)
            if published_after:
                kwargs["publishedAfter"] = published_after
            if region_code:
                kwargs["regionCode"] = region_code
            if lang_code:
                kwargs["relevanceLanguage"] = lang_code
            if next_token:
                kwargs["pageToken"] = next_token

            search_response = youtube.search().list(**kwargs).execute()
            cost_used += 100; breakdown["search.list"] += 100
        except HttpError as e:
            raise RuntimeError(f"Search API ì˜¤ë¥˜: {e}")

        page_ids = [
            it["id"]["videoId"] for it in search_response.get("items", [])
            if "id" in it and "videoId" in it["id"]
        ]
        if not page_ids:
            break

        try:
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(page_ids)
            ).execute()
            cost_used += 1; breakdown["videos.list"] += 1
        except HttpError as e:
            raise RuntimeError(f"Videos API ì˜¤ë¥˜: {e}")

        items = video_response.get("items", [])
        for item in items:
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}
            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            thumbs = snip.get("thumbnails", {})
            thumb_url = (
                (thumbs.get("maxres", {}) or {}).get("url") or
                (thumbs.get("standard", {}) or {}).get("url") or
                (thumbs.get("high", {}) or {}).get("url") or
                (thumbs.get("medium", {}) or {}).get("url") or
                (thumbs.get("default", {}) or {}).get("url") or
                ""
            )
            seconds = parse_duration_iso8601(cdet.get("duration", ""))

            if not duration_filter_ok(seconds, duration_label):
                continue
            if view_count < min_views:
                continue

            channel_id = snip.get("channelId", "")
            if channel_id:
                channel_ids.add(channel_id)

            results_tmp.append({
                "title": title,
                "views": view_count,
                "published_at_iso": published_at_iso,
                "url": url,
                "thumbnail_url": thumb_url,
                "duration_sec": seconds,
                "channel_id": channel_id,
                "channel_title": snip.get("channelTitle", ""),
            })

        fetched += len(page_ids)
        next_token = search_response.get("nextPageToken")
        if not next_token:
            break

    if not results_tmp:
        return [], cost_used, breakdown

    # ì±„ë„ ë©”íƒ€
    channels_map = {}
    try:
        for batch in _chunked(list(channel_ids), 50):
            ch_resp = youtube.channels().list(
                part="snippet,statistics",
                id=",".join(batch)
            ).execute()
            cost_used += 1; breakdown["channels.list"] += 1
            for c in ch_resp.get("items", []):
                cid = c.get("id")
                cstats = c.get("statistics", {}) or {}
                subs = cstats.get("subscriberCount")
                subs_int = int(subs) if subs is not None else None
                channels_map[cid] = {
                    "title": (c.get("snippet", {}) or {}).get("title", ""),
                    "subs": subs_int,
                    "views": int(cstats.get("viewCount", 0)),
                    "videos": int(cstats.get("videoCount", 0)),
                }
    except HttpError:
        channels_map = {}

    results = []
    for r in results_tmp:
        cinfo = channels_map.get(r["channel_id"], {})
        r.update({
            "channel_subs": cinfo.get("subs"),
            "channel_total_views": cinfo.get("views", 0),
            "channel_video_count": cinfo.get("videos", 0),
            "channel_title": cinfo.get("title", r.get("channel_title", "")),
        })
        results.append(r)

    results.sort(key=lambda x: x["views"], reverse=True)
    return results, cost_used, breakdown

def search_videos_in_channel_by_name(channel_query: str, min_views: int, period_label: str, duration_label: str,
                                     max_fetch: int = 200,
                                     region_code: str | None = None, lang_code: str | None = None):
    youtube = get_youtube_client()
    published_after = published_after_from_label(period_label)

    cost_used = 0
    breakdown = {"search.list": 0, "videos.list": 0, "channels.list": 0}
    max_fetch = max(1, min(int(max_fetch or 200), 5000))

    # 1) ì±„ë„ ì°¾ê¸°
    try:
        kwargs_ch = dict(part="id,snippet", q=channel_query, type="channel", maxResults=1)
        if region_code: kwargs_ch["regionCode"] = region_code
        if lang_code:   kwargs_ch["relevanceLanguage"] = lang_code
        ch_resp = youtube.search().list(**kwargs_ch).execute()
        cost_used += 100; breakdown["search.list"] += 100
    except HttpError as e:
        raise RuntimeError(f"ì±„ë„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

    items = ch_resp.get("items", [])
    if not items:
        return [], cost_used, breakdown
    channel_id = items[0]["id"]["channelId"]

    # 2) ì±„ë„ ë‚´ ì˜ìƒ ê²€ìƒ‰
    results_tmp = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        try:
            kwargs = dict(part="id", type="video", channelId=channel_id,
                          maxResults=take, order="date")
            if published_after:
                kwargs["publishedAfter"] = published_after
            if region_code:
                kwargs["regionCode"] = region_code
            if lang_code:
                kwargs["relevanceLanguage"] = lang_code
            if next_token:
                kwargs["pageToken"] = next_token

            v_search = youtube.search().list(**kwargs).execute()
            cost_used += 100; breakdown["search.list"] += 100
        except HttpError as e:
            raise RuntimeError(f"ì±„ë„ ì˜ìƒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

        page_ids = [
            it["id"]["videoId"] for it in v_search.get("items", [])
            if "id" in it and "videoId" in it["id"]
        ]
        if not page_ids:
            break

        try:
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(page_ids)
            ).execute()
            cost_used += 1; breakdown["videos.list"] += 1
        except HttpError as e:
            raise RuntimeError(f"Videos API ì˜¤ë¥˜: {e}")

        items2 = video_response.get("items", [])
        for item in items2:
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}
            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            thumbs = snip.get("thumbnails", {})
            thumb_url = (
                (thumbs.get("maxres", {}) or {}).get("url") or
                (thumbs.get("standard", {}) or {}).get("url") or
                (thumbs.get("high", {}) or {}).get("url") or
                (thumbs.get("medium", {}) or {}).get("url") or
                (thumbs.get("default", {}) or {}).get("url") or
                ""
            )
            seconds = parse_duration_iso8601(cdet.get("duration", ""))

            if not duration_filter_ok(seconds, duration_label):
                continue
            if view_count < min_views:
                continue

            results_tmp.append({
                "title": title,
                "views": view_count,
                "published_at_iso": published_at_iso,
                "url": url,
                "thumbnail_url": thumb_url,
                "duration_sec": seconds,
                "channel_id": channel_id,
                "channel_title": snip.get("channelTitle", ""),
            })

        fetched += len(page_ids)
        next_token = v_search.get("nextPageToken")
        if not next_token:
            break

    if not results_tmp:
        return [], cost_used, breakdown

    # ì±„ë„ ë©”íƒ€
    channels_map = {}
    try:
        ch_resp2 = youtube.channels().list(
            part="snippet,statistics",
            id=channel_id
        ).execute()
        cost_used += 1; breakdown["channels.list"] += 1
        for c in ch_resp2.get("items", []):
            cid = c.get("id")
            cstats = c.get("statistics", {}) or {}
            subs = cstats.get("subscriberCount")
            subs_int = int(subs) if subs is not None else None
            channels_map[cid] = {
                "title": (c.get("snippet", {}) or {}).get("title", ""),
                "subs": subs_int,
                "views": int(cstats.get("viewCount", 0)),
                "videos": int(cstats.get("videoCount", 0)),
            }
    except HttpError:
        channels_map = {}

    results = []
    for r in results_tmp:
        cinfo = channels_map.get(r["channel_id"], {})
        r.update({
            "channel_subs": cinfo.get("subs"),
            "channel_total_views": cinfo.get("views", 0),
            "channel_video_count": cinfo.get("videos", 0),
            "channel_title": cinfo.get("title", r.get("channel_title", "")),
        })
        results.append(r)

    results.sort(key=lambda x: x["views"], reverse=True)
    return results, cost_used, breakdown

# ----------------------------
# ê²°ê³¼ í•„í„° + í‘œì‹œìš© ê°€ê³µ
# ----------------------------
def calc_grade_from_ratio(ratio: float | None) -> str:
    if ratio is None: return "F"
    v = ratio
    if v >= 5000: return "S"
    if 2000 <= v <= 4999: return "A+"
    if 1000 <= v <= 1999: return "A"
    if 500  <= v <=  999: return "B"
    if 300  <= v <=  499: return "C"
    if 100  <= v <=  299: return "D"
    if  50  <= v <=   99: return "E"
    return "F"

def filter_results(all_results,
                   min_views_label: str,
                   client_period_label: str,
                   dur_label: str,
                   grade_label: str):
    min_views = parse_min_views(min_views_label)
    cutoff = cutoff_dt_from_label_kst(client_period_label)
    filtered = []
    now_kst = datetime.now(timezone.utc).astimezone(KST)

    for r in all_results:
        if r["views"] < min_views:
            continue
        pub_kst = parse_published_at_to_kst(r["published_at_iso"])
        if pub_kst < cutoff:
            continue
        if not duration_filter_ok(r["duration_sec"], dur_label):
            continue
        subs = r.get("channel_subs")
        cviews = r.get("channel_total_views", 0)
        ratio_val = (cviews / subs) if isinstance(subs, int) and subs and subs > 0 else None
        grade_val = calc_grade_from_ratio(ratio_val)
        if grade_label != "ì „ì²´ë“±ê¸‰" and grade_val != grade_label:
            continue
        filtered.append(r)

    return filtered, now_kst

def build_display_rows(filtered_results, search_dt_kst: datetime):
    rows = []
    for r in filtered_results:
        published_kst = parse_published_at_to_kst(r["published_at_iso"])
        d, h = human_elapsed_days_hours(search_dt_kst, published_kst)
        total_hours = max(1, d*24 + h)
        clicks_per_hour = int(round(r["views"] / total_hours))
        dur_text = format_duration_hms(r["duration_sec"])
        subs = r.get("channel_subs")
        cviews = r.get("channel_total_views", 0)
        ratio_val = (cviews / subs) if isinstance(subs, int) and subs and subs > 0 else None
        grade = calc_grade_from_ratio(ratio_val)

        rows.append({
            "ì œëª©": r["title"],
            "ì±„ë„ëª…": r.get("channel_title", ""),
            "ì¡°íšŒìˆ˜": r["views"],
            "ì‹œê°„ë‹¹ í´ë¦­ìˆ˜": clicks_per_hour,
            "ì˜ìƒê¸¸ì´": dur_text,
            "ì—…ë¡œë“œì¼": published_kst.strftime("%Y-%m-%d"),
            "ê²½ê³¼ì‹œê°„(ì¼)": d,
            "ë“±ê¸‰": grade,
            "ì±„ë„ êµ¬ë…ìžìˆ˜": subs if subs is not None else None,
            "ì±„ë„ ì „ì²´ì¡°íšŒìˆ˜": cviews,
            "ì±„ë„ ì˜ìƒê°œìˆ˜": r.get("channel_video_count", 0),
            "URL": r["url"],
            "ì¸ë„¤ì¼": r["thumbnail_url"],
        })
    return rows

# ----------------------------
# Streamlit UI
# ----------------------------
init_api_keys_state()  # ëª¨ë“ˆ ë¡œë“œ ì‹œ í•œ ë²ˆ

def main():
    st.set_page_config(page_title="YouTube ê²€ìƒ‰ê¸°", layout="centered")
    st.title("ðŸ“º YouTube ê²€ìƒ‰ê¸° (Streamlit)")

    # --- ì‚¬ì´ë“œë°”: API í‚¤ / ì¿¼í„° ---
    with st.sidebar:
        st.header("ðŸ”‘ API í‚¤ ê´€ë¦¬")

        keys = API_KEYS_STATE["keys"]
        if keys:
            def _mask_key(k: str) -> str:
                if len(k) <= 12:
                    return k
                return k[:6] + "..." + k[-6:]

            idx_now = API_KEYS_STATE["index"]
            idx = st.selectbox(
                "ì‚¬ìš©í•  í‚¤ ì„ íƒ",
                options=list(range(len(keys))),
                index=idx_now,
                format_func=lambda i: f"{i+1}. {_mask_key(keys[i])}",
            )
            if idx != idx_now:
                select_api_key(idx)
                st.experimental_rerun()
        else:
            st.warning("API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ëž˜ì—ì„œ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")

        with st.expander("API í‚¤ íŽ¸ì§‘ (í•œ ì¤„ì— í•œ ê°œ)", expanded=not bool(keys)):
            text_default = "\n".join(keys) if keys else ""
            text = st.text_area("API í‚¤ ëª©ë¡", value=text_default, height=120)
            col_a, col_b = st.columns(2)
            if col_a.button("ì €ìž¥", use_container_width=True):
                lines = [l.strip() for l in text.splitlines() if l.strip()]
                if not lines:
                    st.error("ìµœì†Œ 1ê°œ ì´ìƒì˜ í‚¤ë¥¼ ìž…ë ¥í•˜ì„¸ìš”.")
                else:
                    save_api_keys_from_user(lines)
                    st.success("API í‚¤ ì €ìž¥ ì™„ë£Œ (1ë²ˆ í‚¤ í™œì„±í™”)")
                    st.experimental_rerun()
            if col_b.button("ëª¨ë‘ ì‚­ì œ", use_container_width=True):
                clear_all_api_keys()
                st.success("API í‚¤ë¥¼ ëª¨ë‘ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
                st.experimental_rerun()

        st.markdown("---")
        today_quota = get_today_quota_total()
        st.caption(f"ì˜¤ëŠ˜ê¹Œì§€ ì‚¬ìš©í•œ YouTube API ì¿¼í„°: **{today_quota} ë‹¨ìœ„**")

    # --- ë©”ì¸ ê²€ìƒ‰ í¼ ---
    st.markdown("### ðŸ” ê²€ìƒ‰ ì¡°ê±´")

    with st.form("search_form"):
        mode = st.radio(
            "ê²€ìƒ‰ íƒ€ìž…",
            ["í‚¤ì›Œë“œ ê²€ìƒ‰", "ì±„ë„ ë‚´ ê²€ìƒ‰"],
            horizontal=True,
        )

        if mode == "í‚¤ì›Œë“œ ê²€ìƒ‰":
            query = st.text_input("ê²€ìƒ‰ì–´", key="keyword_input")
            channel_query = None
        else:
            channel_query = st.text_input("ì±„ë„ëª…", key="channel_input")
            query = None

        col1, col2 = st.columns(2)
        with col1:
            min_values = ["5,000","10,000","25,000","50,000","100,000",
                          "200,000","500,000","1,000,000"]
            min_label = st.selectbox("ìµœì†Œ ì¡°íšŒìˆ˜", min_values, index=0)
        with col2:
            country_name = st.selectbox("êµ­ê°€/ì–¸ì–´", COUNTRY_LIST, index=COUNTRY_LIST.index("í•œêµ­"))

        with st.expander("ì¶”ê°€ í•„í„°", expanded=False):
            col3, col4 = st.columns(2)
            with col3:
                search_period_values = [
                    "ì œí•œì—†ìŒ",
                    "90ì¼","150ì¼","200ì¼","365ì¼",
                    "730ì¼","1095ì¼","1825ì¼","3650ì¼"
                ]
                api_period = st.selectbox("ì„œë²„ ê²€ìƒ‰ê¸°ê°„", search_period_values, index=1)
                period_values = [
                    "1ì¼","3ì¼","7ì¼","14ì¼","30ì¼","60ì¼","90ì¼",
                    "150ì¼","200ì¼","365ì¼",
                    "730ì¼","1095ì¼","1825ì¼","3650ì¼"
                ]
                period_label = st.selectbox("ì—…ë¡œë“œ ê¸°ê°„ í•„í„°", period_values, index=6)
            with col4:
                dur_values = ["ì „ì²´","ì‡¼ì¸ ","ë¡±í¼","1ë¶„~20ë¶„","20ë¶„~40ë¶„","40ë¶„~60ë¶„","60ë¶„ì´ìƒ"]
                dur_label = st.selectbox("ì˜ìƒ ê¸¸ì´", dur_values, index=0)
                grade_values = ["ì „ì²´ë“±ê¸‰","S","A+","A","B","C","D","E","F"]
                grade_label = st.selectbox("ì±„ë„ ë“±ê¸‰", grade_values, index=0)

            max_fetch = st.number_input("ê°€ì ¸ì˜¬ ìµœëŒ€ ì•„ì´í…œ ìˆ˜ (1~5000)", min_value=1, max_value=5000, value=50, step=10)

        submitted = st.form_submit_button("ê²€ìƒ‰ ì‹¤í–‰")

    # --- ê²€ìƒ‰ ì‹¤í–‰ ---
    if submitted:
        if not API_KEYS_STATE["keys"]:
            st.error("API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ í‚¤ë¥¼ ë¨¼ì € ì„¤ì •í•´ ì£¼ì„¸ìš”.")
            return

        region_code, lang_code = COUNTRY_LANG_MAP.get(country_name, ("KR", "ko"))

        try:
            if mode == "í‚¤ì›Œë“œ ê²€ìƒ‰":
                if not (query or "").strip():
                    st.warning("ê²€ìƒ‰ì–´ë¥¼ ìž…ë ¥í•˜ì„¸ìš”.")
                    return
                add_to_history(query)
                append_keyword_log(query)
                results, cost_used, breakdown = search_videos(
                    query=query.strip(),
                    min_views=5000,           # ì„œë²„ ë‹¨ê³„ ìµœì†Œê°’
                    period_label=api_period,
                    duration_label="ì „ì²´",    # í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë‹¤ì‹œ í•„í„°
                    max_fetch=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
            else:
                if not (channel_query or "").strip():
                    st.warning("ì±„ë„ëª…ì„ ìž…ë ¥í•˜ì„¸ìš”.")
                    return
                key_for_log = f"[channel]{channel_query.strip()}"
                add_to_history(key_for_log)
                append_keyword_log(key_for_log)
                results, cost_used, breakdown = search_videos_in_channel_by_name(
                    channel_query=channel_query.strip(),
                    min_views=5000,
                    period_label=api_period,
                    duration_label="ì „ì²´",
                    max_fetch=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
        except RuntimeError as e:
            st.error(str(e))
            return

        add_quota_usage(cost_used)
        st.info(f"ì´ë²ˆ ê²€ìƒ‰ì—ì„œ ì‚¬ìš©ëœ ì¿¼í„°: **{cost_used} ë‹¨ìœ„** &nbsp;&nbsp;|&nbsp;&nbsp; ì˜¤ëŠ˜ ëˆ„ì : **{get_today_quota_total()} ë‹¨ìœ„**")

        if not results:
            st.warning("ì¡°ê±´ì— ë§žëŠ” ì„œë²„ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.session_state["last_results"] = []
            return

        # í´ë¼ì´ì–¸íŠ¸ í•„í„° ì ìš©
        filtered, search_dt_kst = filter_results(
            results,
            min_views_label=min_label,
            client_period_label=period_label,
            dur_label=dur_label,
            grade_label=grade_label,
        )

        if not filtered:
            st.warning("ì„œë²„ ê²°ê³¼ëŠ” ìžˆì—ˆì§€ë§Œ, í˜„ìž¬ í•„í„° ì¡°ê±´ì— ë§žëŠ” ì˜ìƒì€ ì—†ìŠµë‹ˆë‹¤.")
            st.session_state["last_results"] = []
            return

        display_rows = build_display_rows(filtered, search_dt_kst)
        df = pd.DataFrame(display_rows)

        # URL / ì¸ë„¤ì¼ ì»¬ëŸ¼ì€ í‘œì—ì„œëŠ” ë¹¼ê³ , ê°œë³„ ë¯¸ë¦¬ë³´ê¸°ì—ì„œ ì‚¬ìš©
        df_show = df.drop(columns=["URL", "ì¸ë„¤ì¼"])

        st.markdown(f"### ê²°ê³¼: {len(filtered):,}ê±´")
        st.caption("â€» í‘œëŠ” ì¢Œìš°ë¡œ ìŠ¤í¬ë¡¤í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤ (ëª¨ë°”ì¼).")
        st.dataframe(df_show, use_container_width=True)

        st.session_state["last_results"] = display_rows

    # --- ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° ---
    if "last_results" in st.session_state and st.session_state["last_results"]:
        results = st.session_state["last_results"]
        st.markdown("### ðŸŽ¬ ì„ íƒí•œ ì˜ìƒ ë¯¸ë¦¬ë³´ê¸°")

        options = [f"{i+1}. {row['ì œëª©'][:60]}" for i, row in enumerate(results)]
        idx = st.selectbox("ë¯¸ë¦¬ë³¼ ì˜ìƒ ì„ íƒ", range(len(results)), format_func=lambda i: options[i])

        row = results[idx]
        st.markdown(f"**ì œëª©**: {row['ì œëª©']}")
        st.write(f"ì±„ë„: {row['ì±„ë„ëª…']}")
        st.write(f"ì¡°íšŒìˆ˜: {row['ì¡°íšŒìˆ˜']:,} | ì‹œê°„ë‹¹ í´ë¦­ìˆ˜: {row['ì‹œê°„ë‹¹ í´ë¦­ìˆ˜']:,}")
        st.write(f"ì˜ìƒê¸¸ì´: {row['ì˜ìƒê¸¸ì´']} | ì—…ë¡œë“œì¼: {row['ì—…ë¡œë“œì¼']} | ë“±ê¸‰: {row['ë“±ê¸‰']}")

        if row["ì¸ë„¤ì¼"]:
            st.image(row["ì¸ë„¤ì¼"], use_container_width=True)

        st.markdown(f"[YouTubeì—ì„œ ì—´ê¸° ðŸ”—]({row['URL']})")

    # --- ìµœê·¼ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ---
    st.markdown("### â± ìµœê·¼ ê²€ìƒ‰ í‚¤ì›Œë“œ (14ì¼)")
    history = get_recent_keywords(days=14)
    if not history:
        st.write("ìµœê·¼ ê²€ìƒ‰ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        hist_rows = [{
            "ê²€ìƒ‰ì‹œê°": format_k_datetime_simple(dt),
            "í‚¤ì›Œë“œ": q
        } for dt, q in history]
        hist_df = pd.DataFrame(hist_rows)
        st.table(hist_df)

if __name__ == "__main__":
    main()
