#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
from datetime import datetime, timedelta, timezone

import streamlit as st
import pandas as pd
import requests

from supabase import create_client, Client
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# ============================
# ê¸°ë³¸ ì„¤ì •
# ============================
st.set_page_config(
    page_title="YouTube ê²€ìƒ‰ê¸° (Streamlit)",
    page_icon="ğŸ”",
    layout="wide",
)

st.title("ğŸ” YouTube ê²€ìƒ‰ê¸° (Streamlit)")

# ëª¨ë°”ì¼ì—ì„œ ë³´ê¸° ì‰½ê²Œ ìƒë‹¨ ì—¬ë°± ì¡°ê¸ˆë§Œ
st.markdown(
    """
    <style>
    .block-container { padding-top: 1rem !important; }
    </style>
    """,
    unsafe_allow_html=True,
)

# ============================
# ì‹œê°„ / ìƒìˆ˜
# ============================
KST = timezone(timedelta(hours=9))
ENV_KEY_NAME = "YOUTUBE_API_KEY"

WEEKDAY_KO = ["ì›”ìš”ì¼","í™”ìš”ì¼","ìˆ˜ìš”ì¼","ëª©ìš”ì¼","ê¸ˆìš”ì¼","í† ìš”ì¼","ì¼ìš”ì¼"]

COUNTRY_LANG_MAP = {
    "í•œêµ­": ("KR", "ko"),
    "ì¼ë³¸": ("JP", "ja"),
    "ë¯¸êµ­": ("US", "en"),
    "ì˜êµ­": ("GB", "en"),
    "ë…ì¼": ("DE", "de"),
    "í”„ë‘ìŠ¤": ("FR", "fr"),
    "ìŠ¤í˜ì¸": ("ES", "es"),
    "ì´íƒˆë¦¬ì•„": ("IT", "it"),
    "ë¸Œë¼ì§ˆ": ("BR", "pt"),
    "ì¸ë„": ("IN", "en"),
    "í˜¸ì£¼": ("AU", "en"),
}
COUNTRY_LIST = list(COUNTRY_LANG_MAP.keys())

# ============================
# Supabase í´ë¼ì´ì–¸íŠ¸
# ============================
@st.cache_resource
def get_supabase_client() -> Client | None:
    url = st.secrets.get("SUPABASE_URL")
    key = st.secrets.get("SUPABASE_SERVICE_KEY")
    if not url or not key:
        return None
    return create_client(url, key)

SUPABASE_BUCKET = st.secrets.get("SUPABASE_BUCKET", "yts-config")
supabase = get_supabase_client()

# ============================
# Supabase JSON I/O
# ============================
def _load_json(filename: str, default):
    """Supabase Storageì—ì„œ filenameì„ JSONìœ¼ë¡œ ë¡œë“œ"""
    if supabase is None:
        return default
    try:
        res = supabase.storage.from_(SUPABASE_BUCKET).download(filename)
        if res is None:
            return default
        if isinstance(res, bytes):
            text = res.decode("utf-8")
        else:
            text = str(res)
        return json.loads(text)
    except Exception:
        return default


def _save_json(filename: str, data):
    """Supabase Storageì— filenameì„ JSONìœ¼ë¡œ ì €ì¥ (upsert)"""
    if supabase is None:
        return
    try:
        payload = json.dumps(data, ensure_ascii=False, indent=2).encode("utf-8")
        supabase.storage.from_(SUPABASE_BUCKET).upload(
            path=filename,
            file=payload,
            file_options={"content-type": "application/json", "x-upsert": "true"},
        )
    except Exception as e:
        st.warning(f"Supabase ì €ì¥ ì˜¤ë¥˜({filename}): {e}")


CONFIG_PATH       = "yts_config.json"
KEYWORD_LOG_PATH  = "yts_keyword_log.json"
QUOTA_PATH        = "yts_quota_usage.json"

# ============================
# API í‚¤ ê´€ë¦¬
# ============================
_DEFAULT_API_KEYS = [
    "YOUR_YT_API_KEY_1",
    "YOUR_YT_API_KEY_2",
]

def _load_api_keys_config():
    data = _load_json(CONFIG_PATH, {})
    keys = [k.strip() for k in data.get("api_keys", []) if k.strip()]
    if not keys:
        keys = _DEFAULT_API_KEYS[:]
    sel = data.get("selected_index", 0)
    sel = max(0, min(sel, len(keys)-1))
    return {"api_keys": keys, "selected_index": sel}

def _save_api_keys_config(keys: list[str], selected_index: int):
    keys = [k.strip() for k in keys if k.strip()]
    selected_index = max(0, min(selected_index, len(keys)-1)) if keys else 0
    _save_json(CONFIG_PATH, {"api_keys": keys, "selected_index": selected_index})

if "api_keys_state" not in st.session_state:
    cfg = _load_api_keys_config()
    st.session_state.api_keys_state = {
        "keys": cfg["api_keys"],
        "index": cfg["selected_index"],
    }

def _apply_env_key(key: str):
    if key:
        os.environ[ENV_KEY_NAME] = key
    else:
        os.environ.pop(ENV_KEY_NAME, None)

def get_current_api_key() -> str:
    keys = st.session_state.api_keys_state["keys"]
    idx  = st.session_state.api_keys_state["index"]
    if not keys:
        return ""
    return keys[idx]

def set_current_api_index(idx: int):
    keys = st.session_state.api_keys_state["keys"]
    if not keys:
        return
    idx = max(0, min(idx, len(keys)-1))
    st.session_state.api_keys_state["index"] = idx
    _apply_env_key(keys[idx])
    _save_api_keys_config(keys, idx)

def save_api_keys_from_user(text: str):
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    if not lines:
        return
    st.session_state.api_keys_state["keys"] = lines
    st.session_state.api_keys_state["index"] = 0
    _apply_env_key(lines[0])
    _save_api_keys_config(lines, 0)

# ì²« ë¡œë“œ ì‹œ í™˜ê²½ë³€ìˆ˜ ì ìš©
_apply_env_key(get_current_api_key())

# ============================
# ì¿¼í„° ê´€ë¦¬
# ============================
def _load_quota_map():
    return _load_json(QUOTA_PATH, {})

def _save_quota_map(data: dict):
    _save_json(QUOTA_PATH, data)

def quota_today_key():
    return datetime.now(KST).strftime("%Y-%m-%d")

def get_today_quota_total() -> int:
    data = _load_quota_map()
    return int(data.get(quota_today_key(), 0))

def add_quota_usage(units: int):
    if units <= 0:
        return
    data = _load_quota_map()
    key = quota_today_key()
    data[key] = int(data.get(key, 0)) + int(units)
    _save_quota_map(data)

# ============================
# í‚¤ì›Œë“œ ë¡œê·¸
# ============================
def _load_keyword_log():
    return _load_json(KEYWORD_LOG_PATH, [])

def _save_keyword_log(entries: list):
    _save_json(KEYWORD_LOG_PATH, entries)

def append_keyword_log(query: str):
    q = (query or "").strip()
    if not q:
        return
    entries = _load_keyword_log()
    now = datetime.now(KST).isoformat(timespec="seconds")
    entries.append({"ts": now, "q": q})
    _save_keyword_log(entries)

def get_recent_keywords(limit: int = 30):
    entries = _load_keyword_log()
    out = []
    for item in entries:
        ts = item.get("ts")
        q  = item.get("q")
        if not ts or not q:
            continue
        try:
            dt = datetime.fromisoformat(ts)
        except Exception:
            continue
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=KST)
        out.append((dt, q))
    out.sort(key=lambda x: x[0], reverse=True)
    return out[:limit]

# ============================
# ì‹œê°„/ìœ í‹¸
# ============================
def format_k_datetime(dt_aw: datetime) -> str:
    if dt_aw.tzinfo is None:
        dt_aw = dt_aw.replace(tzinfo=KST)
    dt = dt_aw.astimezone(KST)
    wd = WEEKDAY_KO[dt.weekday()]
    h24 = dt.hour
    ampm = "ì˜¤ì „" if h24 < 12 else "ì˜¤í›„"
    h12 = h24 % 12 or 12
    return f"{dt.month}ì›”{dt.day}ì¼ {wd} {ampm}{h12}ì‹œ {dt.minute}ë¶„"

def parse_published_at_to_kst(published_iso: str) -> datetime:
    dt_utc = datetime.fromisoformat(published_iso.replace("Z", "+00:00"))
    return dt_utc.astimezone(KST)

def human_elapsed_days_hours(later: datetime, earlier: datetime) -> tuple[int, int]:
    delta = later - earlier
    if delta.total_seconds() < 0:
        return 0, 0
    days = delta.days
    hours = delta.seconds // 3600
    return days, hours

def published_after_from_label(label: str):
    label = label.strip()
    now_utc = datetime.now(timezone.utc)
    if label == "ì œí•œì—†ìŒ":
        return None
    if label.endswith("ì¼"):
        days = int(label[:-1])
        dt = now_utc - timedelta(days=days)
    else:
        return None
    return dt.isoformat(timespec="seconds").replace("+00:00", "Z")

def parse_duration_iso8601(iso_dur: str) -> int:
    h = m = s = 0
    if not iso_dur or not iso_dur.startswith("PT"):
        return 0
    num = ""
    for ch in iso_dur[2:]:
        if ch.isdigit():
            num += ch
        else:
            if ch == "H" and num:
                h = int(num); num = ""
            elif ch == "M" and num:
                m = int(num); num = ""
            elif ch == "S" and num:
                s = int(num); num = ""
    return h*3600 + m*60 + s

def format_duration_hms(seconds: int) -> str:
    if seconds <= 0:
        return "0:00"
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    if h > 0:
        return f"{h}:{m:02d}:{s:02d}"
    return f"{m}:{s:02d}"

def duration_filter_ok(seconds: int, label: str) -> bool:
    if label == "ì „ì²´": return True
    if label == "ì‡¼ì¸ ": return seconds < 60
    if label == "ë¡±í¼": return seconds >= 60
    if label == "1~20ë¶„": return 60 <= seconds < 20*60
    if label == "20~40ë¶„": return 20*60 <= seconds < 40*60
    if label == "40~60ë¶„": return 40*60 <= seconds < 60*60
    if label == "60ë¶„ì´ìƒ": return seconds >= 60*60
    return True

def parse_min_views(text: str) -> int:
    digits = text.replace(",", "").replace(" ", "").replace("ë§Œ", "0000")
    try:
        return int(digits)
    except Exception:
        return 0

# ============================
# YouTube í´ë¼ì´ì–¸íŠ¸
# ============================
def get_youtube_client():
    key = get_current_api_key()
    if not key:
        raise RuntimeError("YouTube API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢Œì¸¡ì—ì„œ í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    try:
        return build("youtube", "v3", developerKey=key, cache_discovery=False)
    except TypeError:
        return build("youtube", "v3", developerKey=key)

# ============================
# YouTube ê²€ìƒ‰ í•¨ìˆ˜
# ============================
def search_videos(
    query: str,
    min_views: int,
    api_period_label: str,
    duration_label: str,
    max_fetch: int,
    region_code: str | None,
    lang_code: str | None,
):
    youtube = get_youtube_client()
    published_after = published_after_from_label(api_period_label)

    cost_used = 0
    breakdown = {"search.list": 0, "videos.list": 0}
    max_fetch = max(1, min(int(max_fetch or 100), 5000))

    results_tmp = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        kwargs = dict(
            q=query,
            part="id",
            type="video",
            maxResults=take,
        )
        if published_after:
            kwargs["publishedAfter"] = published_after
        if region_code:
            kwargs["regionCode"] = region_code
        if lang_code:
            kwargs["relevanceLanguage"] = lang_code
        if next_token:
            kwargs["pageToken"] = next_token

        try:
            search_response = youtube.search().list(**kwargs).execute()
            cost_used += 100; breakdown["search.list"] += 100
        except HttpError as e:
            raise RuntimeError(f"Search API ì˜¤ë¥˜: {e}")

        page_ids = [
            it["id"]["videoId"]
            for it in search_response.get("items", [])
            if "id" in it and "videoId" in it["id"]
        ]
        if not page_ids:
            break

        try:
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(page_ids),
            ).execute()
            cost_used += 1; breakdown["videos.list"] += 1
        except HttpError as e:
            raise RuntimeError(f"Videos API ì˜¤ë¥˜: {e}")

        items = video_response.get("items", [])
        for item in items:
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}

            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

            if view_count < min_views:
                continue
            if not duration_filter_ok(duration_sec, duration_label):
                continue

            results_tmp.append({
                "title": title,
                "views": view_count,
                "published_at_iso": published_at_iso,
                "url": url,
                "duration_sec": duration_sec,
                "channel_title": snip.get("channelTitle", ""),
            })

        fetched += len(page_ids)
        next_token = search_response.get("nextPageToken")
        if not next_token:
            break

    return results_tmp, cost_used, breakdown

# ============================
# ì ìˆ˜/ë“±ê¸‰ ê³„ì‚°
# ============================
def calc_grade(clicks_per_hour: int) -> str:
    v = clicks_per_hour
    if v >= 5000: return "S"
    if v >= 2000: return "A+"
    if v >= 1000: return "A"
    if v >= 500:  return "B"
    if v >= 300:  return "C"
    if v >= 100:  return "D"
    if v >= 50:   return "E"
    return "F"

# ============================
# UI - sidebar (ëª¨ë°”ì¼ ì••ì¶•)
# ============================
st.sidebar.header("âš™ï¸ ì„¤ì • / í•„í„°")

# --- API í‚¤ ì„¤ì • ---
with st.sidebar.expander("ğŸ”‘ YouTube API í‚¤", expanded=True):
    keys = st.session_state.api_keys_state["keys"]
    idx  = st.session_state.api_keys_state["index"]

    if keys:
        masked = [f"{i+1}. {k[:6]}...{k[-5:]}" for i,k in enumerate(keys)]
        sel = st.selectbox("ì‚¬ìš©í•  í‚¤ ì„ íƒ", range(len(keys)), format_func=lambda i: masked[i], index=idx)
        if sel != idx:
            set_current_api_index(sel)

    key_text = st.text_area(
        "API í‚¤ë“¤ì„ í•œ ì¤„ì— í•˜ë‚˜ì”© ì…ë ¥ í›„ [ì €ì¥] í´ë¦­",
        value="\n".join(keys) if keys and keys != _DEFAULT_API_KEYS else "",
        height=80,
    )
    if st.button("API í‚¤ ì €ì¥", use_container_width=True):
        save_api_keys_from_user(key_text)
        st.success("API í‚¤ë¥¼ ì €ì¥í•˜ê³  1ë²ˆ í‚¤ë¥¼ í™œì„±í™”í–ˆìŠµë‹ˆë‹¤.")

# --- ê²€ìƒ‰ ì˜µì…˜ ---
st.sidebar.markdown("---")
query = st.sidebar.text_input("ğŸ” ê²€ìƒ‰ì–´", "")
channel_query = st.sidebar.text_input("ğŸ“º ì±„ë„ëª… ê²€ìƒ‰ (ì„ íƒ)", "")

api_period = st.sidebar.selectbox(
    "ì„œë²„ ê²€ìƒ‰ê¸°ê°„",
    ["ì œí•œì—†ìŒ","90ì¼","150ì¼","365ì¼","730ì¼","1095ì¼","1825ì¼","3650ì¼"],
    index=1,
)

upload_period = st.sidebar.selectbox(
    "ì—…ë¡œë“œ ê¸°ê°„(í´ë¼ì´ì–¸íŠ¸ í•„í„°)",
    ["ì œí•œì—†ìŒ","1ì¼","3ì¼","7ì¼","14ì¼","30ì¼","60ì¼","90ì¼","180ì¼","365ì¼"],
    index=6,
)

min_views_label = st.sidebar.selectbox(
    "ìµœì†Œ ì¡°íšŒìˆ˜",
    ["5,000","10,000","25,000","50,000","100,000","200,000","500,000","1,000,000"],
    index=0,
)

duration_label = st.sidebar.selectbox(
    "ì˜ìƒ ê¸¸ì´",
    ["ì „ì²´","ì‡¼ì¸ ","ë¡±í¼","1~20ë¶„","20~40ë¶„","40~60ë¶„","60ë¶„ì´ìƒ"],
    index=0,
)

max_fetch = st.sidebar.number_input("ê°€ì ¸ì˜¬ ìµœëŒ€ ê°œìˆ˜", 1, 5000, 50, step=10)

country_name = st.sidebar.selectbox("êµ­ê°€/ì–¸ì–´", COUNTRY_LIST, index=0)
region_code, lang_code = COUNTRY_LANG_MAP[country_name]

# ìµœê·¼ í‚¤ì›Œë“œ
with st.sidebar.expander("â± ìµœê·¼ ê²€ìƒ‰ í‚¤ì›Œë“œ", expanded=False):
    recents = get_recent_keywords(30)
    if not recents:
        st.write("ìµœê·¼ ê²€ìƒ‰ ì—†ìŒ")
    else:
        for dt, q in recents:
            st.write(f"- {dt.strftime('%m-%d %H:%M')} â€” `{q}`")

# ============================
# ë©”ì¸ ì˜ì—­
# ============================
col_btn, col_quota = st.columns([2,1])

with col_btn:
    do_search = st.button("ê²€ìƒ‰ ì‹¤í–‰", type="primary", use_container_width=True)
with col_quota:
    st.metric("ì˜¤ëŠ˜ ì‚¬ìš©í•œ ì¿¼í„°", f"{get_today_quota_total():,} units")

status_placeholder = st.empty()

if "results_df" not in st.session_state:
    st.session_state.results_df = None
    st.session_state.last_search_time = None

# ============================
# ê²€ìƒ‰ ì‹¤í–‰
# ============================
def apply_client_filters(df: pd.DataFrame) -> pd.DataFrame:
    # ì—…ë¡œë“œ ê¸°ê°„
    if upload_period != "ì œí•œì—†ìŒ":
        days = int(upload_period.replace("ì¼",""))
        cutoff = datetime.now(KST) - timedelta(days=days)
        df = df[df["ì—…ë¡œë“œì‹œê°"] >= cutoff]
    # ìµœì†Œ ì¡°íšŒìˆ˜ (ì¶”ê°€ í•„í„°)
    min_views = parse_min_views(min_views_label)
    df = df[df["ì˜ìƒì¡°íšŒìˆ˜"] >= min_views]
    return df

if do_search:
    if not query.strip() and not channel_query.strip():
        st.warning("ê²€ìƒ‰ì–´ ë˜ëŠ” ì±„ë„ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        try:
            if query.strip():
                base_query = query.strip()
                append_keyword_log(base_query)
                status_placeholder.info("ì¼ë°˜ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
                raw_results, cost_used, breakdown = search_videos(
                    query=base_query,
                    min_views=parse_min_views(min_views_label),
                    api_period_label=api_period,
                    duration_label=duration_label,
                    max_fetch=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
            else:
                st.warning("ì±„ë„ ì „ìš© ê²€ìƒ‰ì€ ê°„ë‹¨ ë²„ì „ì—ì„œ ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\nì¼ë‹¨ ì¼ë°˜ ê²€ìƒ‰ë§Œ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
                raw_results, cost_used, breakdown = [], 0, {}

        except Exception as e:
            st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            raw_results, cost_used, breakdown = [], 0, {}

        add_quota_usage(cost_used)

        if not raw_results:
            st.session_state.results_df = None
            status_placeholder.info("ì„œë²„ ê²°ê³¼ 0ê±´")
        else:
            search_dt = datetime.now(KST)
            rows = []
            for r in raw_results:
                pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                d, h = human_elapsed_days_hours(search_dt, pub_kst)
                total_hours = max(1, d*24 + h)
                cph = int(round(r["views"] / total_hours))

                rows.append({
                    "ì±„ë„ëª…": r["channel_title"],
                    "ë“±ê¸‰": calc_grade(cph),
                    "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                    "ì‹œê°„ë‹¹í´ë¦­": cph,
                    "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                    "ì—…ë¡œë“œì‹œê°": pub_kst,
                    "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                    "ì œëª©": r["title"],
                    "URL": r["url"],
                })

            df = pd.DataFrame(rows)
            if not df.empty:
                df = apply_client_filters(df)

            st.session_state.results_df = df
            st.session_state.last_search_time = search_dt

            status_placeholder.success(f"ì„œë²„ ê²°ê³¼: {len(raw_results):,}ê±´ / í•„í„° í›„: {len(df):,}ê±´ "
                                       f"(ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})")

# ============================
# ê²°ê³¼ í‘œì‹œ
# ============================
df = st.session_state.results_df
if df is None or df.empty:
    st.info("ì•„ì§ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢Œì¸¡ì—ì„œ ì¡°ê±´ì„ ì„¤ì •í•˜ê³  **[ê²€ìƒ‰ ì‹¤í–‰]** ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
else:
    # URL ì»¬ëŸ¼ì„ ë§í¬ë¡œ í‘œì‹œ
    df_display = df.copy()
    df_display["ë§í¬"] = df_display["URL"].apply(lambda u: f"[ì—´ê¸°]({u})")
    df_display = df_display.drop(columns=["URL"])

    st.subheader("ğŸ“Š ê²°ê³¼ ë¦¬ìŠ¤íŠ¸")
    st.dataframe(
        df_display,
        use_container_width=True,
        height=500,
    )

    st.caption("ì—´ê¸° ë§í¬ë¥¼ ëˆ„ë¥´ë©´ ìƒˆ íƒ­ì—ì„œ ì˜ìƒì´ ì—´ë¦½ë‹ˆë‹¤.")
