#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import random
from datetime import datetime, timedelta, timezone

import streamlit as st
import pandas as pd

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

from supabase import create_client, Client

# -----------------------------
# Streamlit ê¸°ë³¸ ì„¤ì •
# -----------------------------
st.set_page_config(
    page_title="YouTubeê²€ìƒ‰ê¸°",
    page_icon="ğŸ”",
    layout="wide",
)

# ìƒë‹¨ ì—¬ë°± ì¡°ì • + DataEditor ì•„ì´ì½˜(3ì  ë©”ë‰´ ë“±) ìˆ¨ê¸°ê¸°
st.markdown(
    """
    <style>
    .block-container { padding-top: 3rem !important; }

    /* DataEditor ë‚´ ì•„ì´ì½˜ ë²„íŠ¼(3ì  ë©”ë‰´ ë“±) ìˆ¨ê¸°ê¸° */
    [data-testid="stDataFrame"] button[kind="icon"] {
        display: none !important;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# -----------------------------
# ë¡œê·¸ì¸ ì„¤ì •
# -----------------------------
LOGIN_ID_ENV = os.getenv("LOGIN_ID", "")
LOGIN_PW_ENV = os.getenv("LOGIN_PW", "")

st.session_state.setdefault("logged_in", False)
st.session_state.setdefault("login_id", LOGIN_ID_ENV or "")
st.session_state.setdefault("login_pw", LOGIN_PW_ENV or "")

def check_login(id_text: str, pw_text: str) -> bool:
    if LOGIN_ID_ENV and LOGIN_PW_ENV:
        return (id_text == LOGIN_ID_ENV) and (pw_text == LOGIN_PW_ENV)
    return True  # í™˜ê²½ë³€ìˆ˜ ì—†ìœ¼ë©´ ê°œë°œìš©ìœ¼ë¡œ í†µê³¼

if not st.session_state["logged_in"]:
    st.markdown("### ğŸ”’ YouTubeê²€ìƒ‰ê¸° ë¡œê·¸ì¸")

    id_input = st.text_input("ë¡œê·¸ì¸ ID", value=st.session_state["login_id"])
    pw_input = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password", value=st.session_state["login_pw"])

    col_l, _ = st.columns([1, 3])
    with col_l:
        login_btn = st.button("ë¡œê·¸ì¸", type="primary", use_container_width=True)

    if login_btn:
        st.session_state["login_id"] = id_input
        st.session_state["login_pw"] = pw_input
        if check_login(id_input, pw_input):
            st.session_state["logged_in"] = True
            st.success("ë¡œê·¸ì¸ ì„±ê³µ!")
            st.rerun()
        else:
            st.error("ë¡œê·¸ì¸ ì‹¤íŒ¨. ID ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    st.stop()

# ë¡œê·¸ì¸ ì´í›„ í™”ë©´ ì œëª© (ì‘ê²Œ)
st.markdown("### ğŸ” YouTubeê²€ìƒ‰ê¸°")

# -----------------------------
# ê³µí†µ ìƒìˆ˜/í™˜ê²½
# -----------------------------
KST = timezone(timedelta(hours=9))
WEEKDAY_KO = ["ì›”ìš”ì¼","í™”ìš”ì¼","ìˆ˜ìš”ì¼","ëª©ìš”ì¼","ê¸ˆìš”ì¼","í† ìš”ì¼","ì¼ìš”ì¼"]

# YouTube ìˆ˜ìµ ìƒìœ„ê¶Œ 20ê°œêµ­(ëŒ€ëµì ì¸ ìˆœì„œ)
COUNTRY_LANG_MAP = {
    "ë¯¸êµ­": ("US", "en"),
    "ì˜êµ­": ("GB", "en"),
    "í•œêµ­": ("KR", "ko"),
    "ì¼ë³¸": ("JP", "ja"),
    "ì¸ë„": ("IN", "en"),
    "ë¸Œë¼ì§ˆ": ("BR", "pt"),
    "ìºë‚˜ë‹¤": ("CA", "en"),
    "ë…ì¼": ("DE", "de"),
    "í”„ë‘ìŠ¤": ("FR", "fr"),
    "ë©•ì‹œì½”": ("MX", "es"),
    "í˜¸ì£¼": ("AU", "en"),
    "ìŠ¤í˜ì¸": ("ES", "es"),
    "ì´íƒˆë¦¬ì•„": ("IT", "it"),
    "ë„¤ëœë€ë“œ": ("NL", "nl"),
    "í„°í‚¤": ("TR", "tr"),
    "ì¸ë„ë„¤ì‹œì•„": ("ID", "id"),
    "íƒœêµ­": ("TH", "th"),
    "ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„": ("SA", "ar"),
    "ì•„ëì—ë¯¸ë¦¬íŠ¸": ("AE", "ar"),
}

COUNTRY_LIST = list(COUNTRY_LANG_MAP.keys())

# íŠ¸ë Œë“œìš© ì¹´í…Œê³ ë¦¬ (videos.list videoCategoryId)
TREND_CATEGORIES = {
    "ì „ì²´": None,
    "ìŒì•…": "10",
    "ê²Œì„": "20",
    "ìŠ¤í¬ì¸ ": "17",
    "ì—”í„°í…Œì¸ë¨¼íŠ¸": "24",
    "ë‰´ìŠ¤/ì‹œì‚¬": "25",
    "ì‚¬ëŒ/ë¸”ë¡œê·¸": "22",
    "ì½”ë¯¸ë””": "23",
    "êµìœ¡": "27",
    "ê³¼í•™/ê¸°ìˆ ": "28",
}

# -----------------------------
# Supabase í´ë¼ì´ì–¸íŠ¸
# -----------------------------
@st.cache_resource
def get_supabase_client() -> Client | None:
    url = st.secrets.get("SUPABASE_URL")
    key = st.secrets.get("SUPABASE_SERVICE_KEY")
    if not url or not key:
        return None
    return create_client(url, key)

SUPABASE_BUCKET = st.secrets.get("SUPABASE_BUCKET", "yts-config")
supabase = get_supabase_client()

def _load_json(filename: str, default):
    if supabase is None:
        return default
    try:
        res = supabase.storage.from_(SUPABASE_BUCKET).download(filename)
        if res is None:
            return default
        if isinstance(res, bytes):
            text = res.decode("utf-8")
        else:
            text = str(res)
        return json.loads(text)
    except Exception:
        return default

def _save_json(filename: str, data):
    if supabase is None:
        return
    try:
        payload = json.dumps(data, ensure_ascii=False, indent=2).encode("utf-8")
        supabase.storage.from_(SUPABASE_BUCKET).upload(
            path=filename,
            file=payload,
            file_options={"content-type": "application/json", "x-upsert": "true"},
        )
    except Exception as e:
        st.warning(f"Supabase ì €ì¥ ì˜¤ë¥˜({filename}): {e}")

# -----------------------------
# ê²½ë¡œ/íŒŒì¼ëª… ì •ì˜ (Supabaseìš©)
# -----------------------------
KEYWORD_LOG_PATH  = "yts_keyword_log.json"
QUOTA_PATH        = "yts_quota_usage.json"

# -----------------------------
# API í‚¤: secretsì—ì„œë§Œ ì‚¬ìš©
# -----------------------------
def get_current_api_key() -> str:
    keys = st.secrets.get("YOUTUBE_API_KEYS")
    if isinstance(keys, list) and keys:
        return str(keys[0]).strip()
    if isinstance(keys, str) and keys.strip():
        first = keys.strip().splitlines()[0]
        return first.strip()

    single = st.secrets.get("YOUTUBE_API_KEY")
    if isinstance(single, str) and single.strip():
        return single.strip()

    return ""

def get_youtube_client():
    key = get_current_api_key()
    if not key:
        raise RuntimeError(
            "YouTube API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
            "â–¶ .streamlit/secrets.toml ì— YOUTUBE_API_KEYS ë˜ëŠ” YOUTUBE_API_KEY ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
        )
    try:
        return build("youtube", "v3", developerKey=key, cache_discovery=False)
    except TypeError:
        return build("youtube", "v3", developerKey=key)

# -----------------------------
# ì¿¼í„° ê¸°ë¡
# -----------------------------
def _load_quota_map():
    return _load_json(QUOTA_PATH, {})

def _save_quota_map(data: dict):
    _save_json(QUOTA_PATH, data)

def quota_today_key():
    return datetime.now(KST).strftime("%Y-%m-%d")

def get_today_quota_total() -> int:
    data = _load_quota_map()
    return int(data.get(quota_today_key(), 0))

def add_quota_usage(units: int):
    if units <= 0:
        return
    data = _load_quota_map()
    key = quota_today_key()
    data[key] = int(data.get(key, 0)) + int(units)
    _save_quota_map(data)

# -----------------------------
# ìµœê·¼ ê²€ìƒ‰ í‚¤ì›Œë“œ
# -----------------------------
def _load_keyword_log():
    return _load_json(KEYWORD_LOG_PATH, [])

def _save_keyword_log(entries: list):
    _save_json(KEYWORD_LOG_PATH, entries)

def append_keyword_log(query: str):
    q = (query or "").strip()
    if not q:
        return
    entries = _load_keyword_log()
    now = datetime.now(KST).isoformat(timespec="seconds")
    entries.append({"ts": now, "q": q})
    _save_keyword_log(entries)

def get_recent_keywords(limit: int = 30):
    entries = _load_keyword_log()
    out = []
    for item in entries:
        ts = item.get("ts")
        q  = item.get("q")
        if not ts or not q:
            continue
        try:
            dt = datetime.fromisoformat(ts)
        except Exception:
            continue
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=KST)
        out.append((dt, q))
    out.sort(key=lambda x: x[0], reverse=True)
    return out[:limit]

# -----------------------------
# ì‹œê°„/í˜•ì‹ ìœ í‹¸
# -----------------------------
def parse_published_at_to_kst(published_iso: str) -> datetime:
    dt_utc = datetime.fromisoformat(published_iso.replace("Z", "+00:00"))
    return dt_utc.astimezone(KST)

def human_elapsed_days_hours(later: datetime, earlier: datetime) -> tuple[int, int]:
    delta = later - earlier
    if delta.total_seconds() < 0:
        return 0, 0
    days = delta.days
    hours = delta.seconds // 3600
    return days, hours

def published_after_from_label(label: str):
    label = label.strip()
    now_utc = datetime.now(timezone.utc)
    if label == "ì œí•œì—†ìŒ":
        return None
    if label.endswith("ì¼"):
        days = int(label[:-1])
        dt = now_utc - timedelta(days=days)
    else:
        return None
    return dt.isoformat(timespec="seconds").replace("+00:00", "Z")

def parse_duration_iso8601(iso_dur: str) -> int:
    h = m = s = 0
    if not iso_dur or not iso_dur.startswith("PT"):
        return 0
    num = ""
    for ch in iso_dur[2:]:
        if ch.isdigit():
            num += ch
        else:
            if ch == "H" and num:
                h = int(num); num = ""
            elif ch == "M" and num:
                m = int(num); num = ""
            elif ch == "S" and num:
                s = int(num); num = ""
    return h*3600 + m*60 + s

def format_duration_hms(seconds: int) -> str:
    if seconds <= 0:
        return "0:00"
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    if h > 0:
        return f"{h}:{m:02d}:{s:02d}"
    return f"{m}:{s:02d}"

def duration_filter_ok(seconds: int, label: str) -> bool:
    if label == "ì „ì²´": return True
    if label == "ì‡¼ì¸ ": return seconds < 60
    if label == "ë¡±í¼": return seconds >= 60
    if label == "1~20ë¶„": return 60 <= seconds < 20*60
    if label == "20~40ë¶„": return 20*60 <= seconds < 40*60
    if label == "40~60ë¶„": return 40*60 <= seconds < 60*60
    if label == "60ë¶„ì´ìƒ": return seconds >= 60*60
    return True

def parse_min_views(text: str) -> int:
    digits = text.replace(",", "").replace(" ", "").replace("ë§Œ", "0000")
    try:
        return int(digits)
    except Exception:
        return 0

# -----------------------------
# ë“±ê¸‰ ê³„ì‚° (ì‹œê°„ë‹¹ í´ë¦­ìˆ˜ ê¸°ì¤€, A~H)
# -----------------------------
def calc_grade(clicks_per_hour: int) -> str:
    v = clicks_per_hour
    if v >= 5000: return "A"
    if v >= 2000: return "B"
    if v >= 1000: return "C"
    if v >= 500:  return "D"
    if v >= 300:  return "E"
    if v >= 100:  return "F"
    if v >= 50:   return "G"
    return "H"

GRADE_ORDER = {"A": 1, "B": 2, "C": 3, "D": 4, "E": 5, "F": 6, "G": 7, "H": 8}

# -----------------------------
# YouTube API í˜¸ì¶œ í•¨ìˆ˜ë“¤
# -----------------------------
def search_videos(
    query: str,
    min_views: int,
    api_period_label: str,
    duration_label: str,
    max_fetch: int,
    region_code: str | None,
    lang_code: str | None,
):
    youtube = get_youtube_client()
    published_after = published_after_from_label(api_period_label)

    cost_used = 0
    breakdown = {"search.list": 0, "videos.list": 0}
    max_fetch = max(1, min(int(max_fetch or 100), 5000))

    results_tmp = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        kwargs = dict(
            q=query,
            part="id",
            type="video",
            maxResults=take,
        )
        if published_after:
            kwargs["publishedAfter"] = published_after
        if region_code:
            kwargs["regionCode"] = region_code
        if lang_code:
            kwargs["relevanceLanguage"] = lang_code
        if next_token:
            kwargs["pageToken"] = next_token

        try:
            search_response = youtube.search().list(**kwargs).execute()
            cost_used += 100; breakdown["search.list"] += 100
        except HttpError as e:
            raise RuntimeError(f"Search API ì˜¤ë¥˜: {e}")

        page_ids = [
            it["id"]["videoId"]
            for it in search_response.get("items", [])
            if "id" in it and "videoId" in it["id"]
        ]
        if not page_ids:
            break

        try:
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(page_ids),
            ).execute()
            cost_used += 1; breakdown["videos.list"] += 1
        except HttpError as e:
            raise RuntimeError(f"Videos API ì˜¤ë¥˜: {e}")

        items = video_response.get("items", [])
        for item in items:
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}

            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

            thumbs = snip.get("thumbnails", {}) or {}
            thumb_url = (
                (thumbs.get("maxres") or {}).get("url")
                or (thumbs.get("standard") or {}).get("url")
                or (thumbs.get("high") or {}).get("url")
                or (thumbs.get("medium") or {}).get("url")
                or (thumbs.get("default") or {}).get("url")
                or ""
            )

            if view_count < min_views:
                continue
            if not duration_filter_ok(duration_sec, duration_label):
                continue

            results_tmp.append({
                "title": title,
                "views": view_count,
                "published_at_iso": published_at_iso,
                "url": url,
                "duration_sec": duration_sec,
                "channel_title": snip.get("channelTitle", ""),
                "thumbnail_url": thumb_url,
            })

        fetched += len(page_ids)
        next_token = search_response.get("nextPageToken")
        if not next_token:
            break

    return results_tmp, cost_used, breakdown

def search_channels_by_keyword(
    keyword: str,
    max_results: int,
    region_code: str | None,
    lang_code: str | None,
):
    youtube = get_youtube_client()
    take = max(1, min(max_results, 50))
    kwargs = dict(
        q=keyword,
        part="id",
        type="channel",
        maxResults=take,
    )
    if region_code:
        kwargs["regionCode"] = region_code
    if lang_code:
        kwargs["relevanceLanguage"] = lang_code

    try:
        search_response = youtube.search().list(**kwargs).execute()
        cost_used = 100
    except HttpError as e:
        raise RuntimeError(f"Channel search API ì˜¤ë¥˜: {e}")

    ch_ids = [
        it["id"]["channelId"]
        for it in search_response.get("items", [])
        if "id" in it and "channelId" in it["id"]
    ]
    if not ch_ids:
        return [], cost_used, {"search.list": 100, "channels.list": 0}

    try:
        ch_resp = youtube.channels().list(
            part="snippet,statistics",
            id=",".join(ch_ids),
        ).execute()
        cost_used += 1
    except HttpError as e:
        raise RuntimeError(f"Channels API ì˜¤ë¥˜: {e}")

    results = []
    for c in ch_resp.get("items", []):
        cid = c.get("id", "")
        sn = c.get("snippet", {}) or {}
        stt = c.get("statistics", {}) or {}
        subs = int(stt.get("subscriberCount", 0)) if stt.get("subscriberCount") is not None else 0
        total_views = int(stt.get("viewCount", 0))
        videos = int(stt.get("videoCount", 0))
        url = f"https://www.youtube.com/channel/{cid}" if cid else ""

        thumbs = sn.get("thumbnails", {}) or {}
        thumb_url = (
            (thumbs.get("high") or {}).get("url")
            or (thumbs.get("medium") or {}).get("url")
            or (thumbs.get("default") or {}).get("url")
            or ""
        )

        results.append({
            "channel_title": sn.get("title", ""),
            "subs": subs,
            "total_views": total_views,
            "videos": videos,
            "url": url,
            "thumbnail_url": thumb_url,
        })

    results.sort(key=lambda r: r["subs"], reverse=True)
    return results, cost_used, {"search.list": 100, "channels.list": 1}

def search_videos_in_channel_by_name(
    channel_name: str,
    min_views: int,
    api_period_label: str,
    duration_label: str,
    max_fetch: int,
    region_code: str | None,
    lang_code: str | None,
):
    youtube = get_youtube_client()
    published_after = published_after_from_label(api_period_label)
    cost_used = 0
    breakdown = {"search.list": 0, "videos.list": 0}

    # 1) ì±„ë„ ID ì°¾ê¸°
    kwargs_ch = dict(
        q=channel_name,
        part="id,snippet",
        type="channel",
        maxResults=1,
    )
    if region_code:
        kwargs_ch["regionCode"] = region_code
    if lang_code:
        kwargs_ch["relevanceLanguage"] = lang_code

    try:
        ch_resp = youtube.search().list(**kwargs_ch).execute()
        cost_used += 100; breakdown["search.list"] += 100
    except HttpError as e:
        raise RuntimeError(f"ì±„ë„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

    items = ch_resp.get("items", [])
    if not items:
        return [], cost_used, breakdown

    channel_id = items[0]["id"]["channelId"]

    # 2) í•´ë‹¹ ì±„ë„ ì˜ìƒ ê²€ìƒ‰
    max_fetch = max(1, min(int(max_fetch or 100), 5000))
    results_tmp = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        kwargs = dict(
            part="id",
            type="video",
            channelId=channel_id,
            maxResults=take,
            order="date",
        )
        if published_after:
            kwargs["publishedAfter"] = published_after
        if region_code:
            kwargs["regionCode"] = region_code
        if lang_code:
            kwargs["relevanceLanguage"] = lang_code
        if next_token:
            kwargs["pageToken"] = next_token

        try:
            v_search = youtube.search().list(**kwargs).execute()
            cost_used += 100; breakdown["search.list"] += 100
        except HttpError as e:
            raise RuntimeError(f"ì±„ë„ ì˜ìƒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

        page_ids = [
            it["id"]["videoId"]
            for it in v_search.get("items", [])
            if "id" in it and "videoId" in it["id"]
        ]
        if not page_ids:
            break

        try:
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(page_ids),
            ).execute()
            cost_used += 1; breakdown["videos.list"] += 1
        except HttpError as e:
            raise RuntimeError(f"Videos API ì˜¤ë¥˜: {e}")

        items2 = video_response.get("items", [])
        for item in items2:
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}

            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

            thumbs = snip.get("thumbnails", {}) or {}
            thumb_url = (
                (thumbs.get("maxres") or {}).get("url")
                or (thumbs.get("standard") or {}).get("url")
                or (thumbs.get("high") or {}).get("url")
                or (thumbs.get("medium") or {}).get("url")
                or (thumbs.get("default") or {}).get("url")
                or ""
            )

            if view_count < min_views:
                continue
            if not duration_filter_ok(duration_sec, duration_label):
                continue

            results_tmp.append({
                "title": title,
                "views": view_count,
                "published_at_iso": published_at_iso,
                "url": url,
                "duration_sec": duration_sec,
                "channel_title": snip.get("channelTitle", ""),
                "thumbnail_url": thumb_url,
            })

        fetched += len(page_ids)
        next_token = v_search.get("nextPageToken")
        if not next_token:
            break

    return results_tmp, cost_used, breakdown

def search_trending_videos(
    max_results: int,
    region_code: str | None,
    video_category_id: str | None,
):
    youtube = get_youtube_client()
    take = max(1, min(max_results, 50))

    base_kwargs = dict(
        part="snippet,statistics,contentDetails",
        chart="mostPopular",
        maxResults=take,
    )
    if region_code:
        base_kwargs["regionCode"] = region_code

    kwargs = base_kwargs.copy()
    if video_category_id:
        kwargs["videoCategoryId"] = video_category_id

    try:
        resp = youtube.videos().list(**kwargs).execute()
        cost_used = 1
    except HttpError as e:
        if e.resp.status == 404 and video_category_id:
            resp = youtube.videos().list(**base_kwargs).execute()
            cost_used = 1
        else:
            raise RuntimeError(f"íŠ¸ë Œë“œ API ì˜¤ë¥˜: {e}")

    results = []
    for item in resp.get("items", []):
        vid = item.get("id", "")
        snip = item.get("snippet", {}) or {}
        stats = item.get("statistics", {}) or {}
        cdet = item.get("contentDetails", {}) or {}

        title = snip.get("title", "")
        published_at_iso = snip.get("publishedAt", "")
        view_count = int(stats.get("viewCount", 0))
        url = f"https://www.youtube.com/watch?v={vid}"
        duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

        thumbs = snip.get("thumbnails", {}) or {}
        thumb_url = (
            (thumbs.get("maxres") or {}).get("url")
            or (thumbs.get("standard") or {}).get("url")
            or (thumbs.get("high") or {}).get("url")
            or (thumbs.get("medium") or {}).get("url")
            or (thumbs.get("default") or {}).get("url")
            or ""
        )

        results.append({
            "title": title,
            "views": view_count,
            "published_at_iso": published_at_iso,
            "url": url,
            "duration_sec": duration_sec,
            "channel_title": snip.get("channelTitle", ""),
            "thumbnail_url": thumb_url,
        })
    return results, cost_used, {"videos.list": 1}

# -----------------------------
# Sidebar UI
# -----------------------------
st.sidebar.subheader("ê²€ìƒ‰")

# ëœë¤ íŠ¸ë Œë“œ ê²€ìƒ‰
random_trend_clicked = st.sidebar.button("ğŸ² ëœë¤ íŠ¸ë Œë“œ ê²€ìƒ‰", use_container_width=True)

# ê³µí†µ ê²€ìƒ‰ì–´ ì…ë ¥
search_query = st.sidebar.text_input("ê²€ìƒ‰ì–´", "")

# 3ê°€ì§€ ê²€ìƒ‰ ë²„íŠ¼
btn_general = st.sidebar.button("ì¼ë°˜ê²€ìƒ‰", use_container_width=True)
btn_channel_videos = st.sidebar.button("ì±„ë„ì˜ìƒê²€ìƒ‰", use_container_width=True)
btn_channel_keyword = st.sidebar.button("í‚¤ì›Œë“œì±„ë„ê²€ìƒ‰", use_container_width=True)

st.sidebar.markdown("---")

# íŠ¸ë Œë“œ ê²€ìƒ‰ (disclosure)
with st.sidebar.expander("ğŸ”¥ íŠ¸ë Œë“œ ê²€ìƒ‰", expanded=False):
    trend_category_label = st.selectbox(
        "ì¹´í…Œê³ ë¦¬",
        list(TREND_CATEGORIES.keys()),
        index=0,
        key="trend_category_label",
    )
    btn_trend = st.button("íŠ¸ë Œë“œ ê²€ìƒ‰ ì‹¤í–‰", use_container_width=True, key="btn_trend")

st.sidebar.markdown("---")

# ë³´ê¸° ëª¨ë“œ (ê¸°ë³¸: ê·¸ë¦¬ë“œ ë·°)
view_mode = st.sidebar.radio(
    "ë³´ê¸° ëª¨ë“œ",
    ["ê·¸ë¦¬ë“œ ë·°", "ë¦¬ìŠ¤íŠ¸ ë·°", "ì‡¼ì¸  ë·°"],
    index=0,
    key="view_mode_radio",
)

# ì •ë ¬ ë°©ì‹
st.session_state.setdefault("sort_key", "ë“±ê¸‰")
st.session_state.setdefault("sort_order", "ì˜¤ë¦„ì°¨ìˆœ")

with st.sidebar.expander("ì •ë ¬ ë°©ì‹", expanded=True):
    sort_options = [
        "ë“±ê¸‰",
        "ì˜ìƒì¡°íšŒìˆ˜",
        "ì‹œê°„ë‹¹í´ë¦­",
        "ì—…ë¡œë“œì‹œê°",
        "ì±„ë„ëª…",
        "ì œëª©",
        "êµ¬ë…ììˆ˜",
        "ì±„ë„ì¡°íšŒìˆ˜",
        "ì±„ë„ì˜ìƒìˆ˜",
    ]
    default_index = sort_options.index(st.session_state["sort_key"]) if st.session_state["sort_key"] in sort_options else 0
    sort_key = st.selectbox("ì •ë ¬ ê¸°ì¤€", sort_options, index=default_index)
    sort_order = st.selectbox("ì •ë ¬ ë°©í–¥", ["ì˜¤ë¦„ì°¨ìˆœ", "ë‚´ë¦¼ì°¨ìˆœ"], index=0)
    st.session_state["sort_key"] = sort_key
    st.session_state["sort_order"] = sort_order

st.sidebar.markdown("---")

# ì„¸ë¶€ í•„í„° (í•­ìƒ í¼ì³ì§„ ìƒíƒœ)
with st.sidebar.expander("âš™ ì„¸ë¶€ í•„í„°", expanded=True):
    api_period = st.selectbox(
        "ì„œë²„ ê²€ìƒ‰ê¸°ê°„ (YouTube API)",
        ["ì œí•œì—†ìŒ","90ì¼","150ì¼","365ì¼","730ì¼","1095ì¼","1825ì¼","3650ì¼"],
        index=1,
        key="api_period",
    )
    upload_period = st.selectbox(
        "ì—…ë¡œë“œ ê¸°ê°„(í´ë¼ì´ì–¸íŠ¸ í•„í„°)",
        ["ì œí•œì—†ìŒ","1ì¼","3ì¼","7ì¼","14ì¼","30ì¼","60ì¼","90ì¼","180ì¼","365ì¼"],
        index=6,
        key="upload_period",
    )
    min_views_label = st.selectbox(
        "ìµœì†Œ ì¡°íšŒìˆ˜",
        ["5,000","10,000","25,000","50,000","100,000","200,000","500,000","1,000,000"],
        index=0,
        key="min_views_label",
    )
    duration_label = st.selectbox(
        "ì˜ìƒ ê¸¸ì´",
        ["ì „ì²´","ì‡¼ì¸ ","ë¡±í¼","1~20ë¶„","20~40ë¶„","40~60ë¶„","60ë¶„ì´ìƒ"],
        index=0,
        key="duration_label",
    )
    max_fetch = st.number_input(
        "ëª¨ë“  ê²€ìƒ‰ì—ì„œ ê°€ì ¸ì˜¬ ìµœëŒ€ ê°œìˆ˜",
        1, 5000, 50, step=10,
        key="max_fetch",
    )
    country_name = st.selectbox("ê²€ìƒ‰ìš© êµ­ê°€/ì–¸ì–´", COUNTRY_LIST, index=0, key="country_for_search")
    region_code, lang_code = COUNTRY_LANG_MAP[country_name]

status_placeholder = st.empty()

# -----------------------------
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# -----------------------------
if "results_df" not in st.session_state:
    st.session_state.results_df = None
    st.session_state.last_search_time = None
    st.session_state.search_mode = None

def apply_client_filters(df: pd.DataFrame, upload_period: str, min_views_label: str) -> pd.DataFrame:
    if upload_period != "ì œí•œì—†ìŒ" and "ì—…ë¡œë“œì‹œê°" in df.columns:
        days = int(upload_period.replace("ì¼",""))
        cutoff = datetime.now(KST) - timedelta(days=days)
        df = df[df["ì—…ë¡œë“œì‹œê°"] >= cutoff]
    min_views = parse_min_views(min_views_label)
    if "ì˜ìƒì¡°íšŒìˆ˜" in df.columns:
        df = df[df["ì˜ìƒì¡°íšŒìˆ˜"] >= min_views]
    return df

def sort_results_df(df: pd.DataFrame, mode: str, sort_key: str, sort_order: str) -> pd.DataFrame:
    col = sort_key
    if col not in df.columns:
        return df
    ascending = (sort_order == "ì˜¤ë¦„ì°¨ìˆœ")
    if col == "ë“±ê¸‰":
        tmp = df.copy()
        tmp["_grade_order"] = tmp["ë“±ê¸‰"].map(GRADE_ORDER).fillna(999)
        tmp = tmp.sort_values("_grade_order", ascending=ascending, ignore_index=True)
        return tmp.drop(columns=["_grade_order"])
    try:
        return df.sort_values(col, ascending=ascending, ignore_index=True)
    except Exception:
        return df

# -----------------------------
# ê²€ìƒ‰ ì‹¤í–‰ ë¡œì§
# -----------------------------
try:
    mode_triggered = None
    is_random_trend = False

    if random_trend_clicked:
        mode_triggered = "trend"
        is_random_trend = True
    elif btn_general:
        mode_triggered = "general"
    elif btn_channel_videos:
        mode_triggered = "channel_videos"
    elif btn_channel_keyword:
        mode_triggered = "channel_list"
    elif btn_trend:
        mode_triggered = "trend"

    if mode_triggered is not None:
        search_dt = datetime.now(KST)

        # ì¼ë°˜ ê²€ìƒ‰
        if mode_triggered == "general":
            base_query = (search_query or "").strip()
            if not base_query:
                st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                append_keyword_log(base_query)
                status_placeholder.info("ì¼ë°˜ ì˜ìƒ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
                raw_results, cost_used, breakdown = search_videos(
                    query=base_query,
                    min_views=parse_min_views(min_views_label),
                    api_period_label=api_period,
                    duration_label=duration_label,
                    max_fetch=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
                add_quota_usage(cost_used)

                if not raw_results:
                    st.session_state.results_df = None
                    st.session_state.search_mode = "general"
                    status_placeholder.info("ì„œë²„ ê²°ê³¼ 0ê±´")
                else:
                    rows = []
                    for r in raw_results:
                        pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                        d, h = human_elapsed_days_hours(search_dt, pub_kst)
                        total_hours = max(1, d*24 + h)
                        cph = int(round(r["views"] / total_hours))
                        rows.append({
                            "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),
                            "ì±„ë„ëª…": r["channel_title"],
                            "ë“±ê¸‰": calc_grade(cph),
                            "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                            "ì‹œê°„ë‹¹í´ë¦­": cph,
                            "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                            "ì—…ë¡œë“œì‹œê°": pub_kst,
                            "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                            "ì œëª©": r["title"],
                            "ë§í¬URL": r["url"],
                        })
                    df = pd.DataFrame(rows)
                    if not df.empty:
                        df = apply_client_filters(df, upload_period, min_views_label)
                    st.session_state.results_df = df
                    st.session_state.last_search_time = search_dt
                    st.session_state.search_mode = "general"
                    status_placeholder.success(
                        f"[ì¼ë°˜ ê²€ìƒ‰] ì„œë²„ ê²°ê³¼: {len(raw_results):,}ê±´ / "
                        f"í•„í„° í›„: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                    )

        # íŠ¸ë Œë“œ ê²€ìƒ‰ (ì¼ë°˜ + ëœë¤)
        elif mode_triggered == "trend":
            if is_random_trend:
                rand_country_name = random.choice(COUNTRY_LIST)
                rand_region_code, _ = COUNTRY_LANG_MAP[rand_country_name]
                cat_labels = [k for k in TREND_CATEGORIES.keys() if k != "ì „ì²´"]
                rand_cat_label = random.choice(cat_labels)
                cat_id = TREND_CATEGORIES[rand_cat_label]
                append_keyword_log(f"[trend-random]{rand_country_name}/{rand_cat_label}")
                status_placeholder.info(f"ëœë¤ íŠ¸ë Œë“œ ê²€ìƒ‰ ì¤‘... ({rand_country_name} Â· {rand_cat_label})")
                trend_region_code = rand_region_code
            else:
                trend_region_code = region_code
                cat_label = trend_category_label
                cat_id = TREND_CATEGORIES.get(cat_label)
                append_keyword_log(f"[trend]{country_name}/{cat_label}")
                status_placeholder.info("íŠ¸ë Œë“œ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")

            raw_results, cost_used, breakdown = search_trending_videos(
                max_results=max_fetch,
                region_code=trend_region_code,
                video_category_id=cat_id,
            )
            add_quota_usage(cost_used)

            if not raw_results:
                st.session_state.results_df = None
                st.session_state.search_mode = "trend"
                status_placeholder.info("íŠ¸ë Œë“œ ê²°ê³¼ 0ê±´")
            else:
                rows = []
                for r in raw_results:
                    pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                    d, h = human_elapsed_days_hours(search_dt, pub_kst)
                    total_hours = max(1, d*24 + h)
                    cph = int(round(r["views"] / total_hours))
                    rows.append({
                        "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),
                        "ì±„ë„ëª…": r["channel_title"],
                        "ë“±ê¸‰": calc_grade(cph),
                        "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                        "ì‹œê°„ë‹¹í´ë¦­": cph,
                        "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                        "ì—…ë¡œë“œì‹œê°": pub_kst,
                        "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                        "ì œëª©": r["title"],
                        "ë§í¬URL": r["url"],
                    })
                df = pd.DataFrame(rows)
                if not df.empty:
                    df = apply_client_filters(df, upload_period, min_views_label)
                st.session_state.results_df = df
                st.session_state.last_search_time = search_dt
                st.session_state.search_mode = "trend"
                status_placeholder.success(
                    f"[íŠ¸ë Œë“œ] ê²°ê³¼: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                )

        # ì±„ë„ ì˜ìƒ ê²€ìƒ‰
        elif mode_triggered == "channel_videos":
            ch_name = (search_query or "").strip()
            if not ch_name:
                st.warning("ê²€ìƒ‰ì–´ì— ì±„ë„ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                append_keyword_log(f"[channel_videos]{ch_name}")
                status_placeholder.info("ì±„ë„ ì˜ìƒ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
                raw_results, cost_used, breakdown = search_videos_in_channel_by_name(
                    channel_name=ch_name,
                    min_views=parse_min_views(min_views_label),
                    api_period_label=api_period,
                    duration_label=duration_label,
                    max_fetch=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
                add_quota_usage(cost_used)

                if not raw_results:
                    st.session_state.results_df = None
                    st.session_state.search_mode = "channel_videos"
                    status_placeholder.info("ì±„ë„ ì˜ìƒ ê²°ê³¼ 0ê±´")
                else:
                    rows = []
                    for r in raw_results:
                        pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                        d, h = human_elapsed_days_hours(search_dt, pub_kst)
                        total_hours = max(1, d*24 + h)
                        cph = int(round(r["views"] / total_hours))
                        rows.append({
                            "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),
                            "ì±„ë„ëª…": r["channel_title"],
                            "ë“±ê¸‰": calc_grade(cph),
                            "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                            "ì‹œê°„ë‹¹í´ë¦­": cph,
                            "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                            "ì—…ë¡œë“œì‹œê°": pub_kst,
                            "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                            "ì œëª©": r["title"],
                            "ë§í¬URL": r["url"],
                        })
                    df = pd.DataFrame(rows)
                    if not df.empty:
                        df = apply_client_filters(df, upload_period, min_views_label)
                    st.session_state.results_df = df
                    st.session_state.last_search_time = search_dt
                    st.session_state.search_mode = "channel_videos"
                    status_placeholder.success(
                        f"[ì±„ë„ ì˜ìƒ] ì„œë²„ ê²°ê³¼: {len(raw_results):,}ê±´ / "
                        f"í•„í„° í›„: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                    )

        # í‚¤ì›Œë“œ ì±„ë„ ê²€ìƒ‰
        elif mode_triggered == "channel_list":
            ch_kw = (search_query or "").strip()
            if not ch_kw:
                st.warning("ê²€ìƒ‰ì–´ì— ì±„ë„ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                append_keyword_log(f"[channel]{ch_kw}")
                status_placeholder.info("ì±„ë„ ëª©ë¡ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
                ch_results, cost_used, breakdown = search_channels_by_keyword(
                    keyword=ch_kw,
                    max_results=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
                add_quota_usage(cost_used)

                if not ch_results:
                    st.session_state.results_df = None
                    st.session_state.search_mode = "channel_list"
                    status_placeholder.info("ì±„ë„ ê²°ê³¼ 0ê±´")
                else:
                    df_rows = []
                    for r in ch_results:
                        df_rows.append({
                            "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),
                            "ì±„ë„ëª…": r["channel_title"],
                            "êµ¬ë…ììˆ˜": r["subs"],
                            "ì±„ë„ì¡°íšŒìˆ˜": r["total_views"],
                            "ì±„ë„ì˜ìƒìˆ˜": r["videos"],
                            "ë§í¬URL": r["url"],
                        })
                    df = pd.DataFrame(df_rows)
                    st.session_state.results_df = df
                    st.session_state.last_search_time = search_dt
                    st.session_state.search_mode = "channel_list"
                    status_placeholder.success(
                        f"[ì±„ë„ ëª©ë¡] ê²°ê³¼: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                    )

except Exception as e:
    st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
    st.session_state.results_df = None

# -----------------------------
# ê²°ê³¼ í‘œì‹œ (ë·° ëª¨ë“œë³„)
# -----------------------------
df = st.session_state.results_df
mode = st.session_state.search_mode

if df is None or df.empty:
    st.info("ì•„ì§ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢Œì¸¡ì—ì„œ ì¡°ê±´ì„ ì„¤ì •í•˜ê³  ë²„íŠ¼ì„ ëˆŒëŸ¬ ê²€ìƒ‰í•˜ì„¸ìš”.")
else:
    df_display = df.copy()

    if "ë§í¬URL" in df_display.columns:
        df_display["ë§í¬"] = df_display["ë§í¬URL"]
        df_display = df_display.drop(columns=["ë§í¬URL"])

    if mode == "general":
        st.subheader("ğŸ“Š ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸")
    elif mode == "trend":
        st.subheader("ğŸ”¥ íŠ¸ë Œë“œ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸")
    elif mode == "channel_videos":
        st.subheader("ğŸ¬ ì±„ë„ ì˜ìƒ ë¦¬ìŠ¤íŠ¸")
    elif mode == "channel_list":
        st.subheader("ğŸ“º ì±„ë„ê²€ìƒ‰ ë¦¬ìŠ¤íŠ¸")
    else:
        st.subheader("ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸")

    df_display = sort_results_df(df_display, mode, sort_key, sort_order)

    # ---------- ì‡¼ì¸  ë·° ----------
    if view_mode == "ì‡¼ì¸  ë·°":
        if mode == "channel_list":
            # ì±„ë„ ì•„ì´ì½˜ 4ì—´ ê·¸ë¦¬ë“œ (100x100 ëŠë‚Œ)
            n_cols = 4
            cols = st.columns(n_cols)
            for idx, (_, row) in enumerate(df_display.iterrows()):
                col = cols[idx % n_cols]
                with col:
                    thumb = row.get("ì¸ë„¤ì¼", "")
                    title = row.get("ì±„ë„ëª…", "")
                    link = row.get("ë§í¬", "")
                    if thumb:
                        st.image(thumb, width=100)
                    st.caption(title[:12])
                    if link:
                        st.markdown(f"[ì±„ë„ ì—´ê¸°]({link})")
                if (idx + 1) % n_cols == 0 and (idx + 1) < len(df_display):
                    cols = st.columns(n_cols)
        else:
            # ì˜ìƒ ì‡¼ì¸  ë·°: 4ì—´ë¡œ ë” ì´˜ì´˜í•˜ê²Œ ë°°ì¹˜
            n_cols = 4
            cols = st.columns(n_cols)
            for idx, (_, row) in enumerate(df_display.iterrows()):
                col = cols[idx % n_cols]
                with col:
                    thumb = row.get("ì¸ë„¤ì¼", "")
                    link = row.get("ë§í¬", "")
                    if thumb:
                        # ê°€ë¡œ 150px ì •ë„ì˜ ì„¸ë¡œ ì´ë¯¸ì§€
                        st.image(thumb, width=150)
                    if link:
                        st.markdown(f"[ì—´ê¸°]({link})", help="ìƒˆ íƒ­ì—ì„œ ì—´ê¸°")
                if (idx + 1) % n_cols == 0 and (idx + 1) < len(df_display):
                    cols = st.columns(n_cols)

        st.caption("ğŸ‘‰ ì´ë¯¸ì§€ë¥¼ ëˆŒëŸ¬ ìƒˆ íƒ­ì—ì„œ ì˜ìƒ(ë˜ëŠ” ì±„ë„)ì„ ì—´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # ---------- ê·¸ë¦¬ë“œ ë·° ----------
    elif view_mode == "ê·¸ë¦¬ë“œ ë·°":
        if mode == "channel_list":
            n_cols = 3
        else:
            n_cols = 3

        cols = st.columns(n_cols)

        for idx, (_, row) in enumerate(df_display.iterrows()):
            col = cols[idx % n_cols]
            with col:
                if "ì¸ë„¤ì¼" in df_display.columns and isinstance(row["ì¸ë„¤ì¼"], str) and row["ì¸ë„¤ì¼"]:
                    st.image(row["ì¸ë„¤ì¼"], use_column_width=True)
                if mode == "channel_list":
                    title = row.get("ì±„ë„ëª…", "")
                    subs = row.get("êµ¬ë…ììˆ˜", 0)
                    total_views = row.get("ì±„ë„ì¡°íšŒìˆ˜", 0)
                    video_count = row.get("ì±„ë„ì˜ìƒìˆ˜", 0)
                    link = row.get("ë§í¬", "")
                    st.markdown(f"**{title}**")
                    st.caption(f"êµ¬ë…ì: {subs:,} Â· ì¡°íšŒìˆ˜: {total_views:,} Â· ì˜ìƒìˆ˜: {video_count:,}")
                    if link:
                        st.markdown(f"[ì±„ë„ ì—´ê¸°]({link})")
                else:
                    title = row.get("ì œëª©", "")
                    ch = row.get("ì±„ë„ëª…", "")
                    views = row.get("ì˜ìƒì¡°íšŒìˆ˜", 0)
                    grade = row.get("ë“±ê¸‰", "")
                    link = row.get("ë§í¬", "")
                    st.markdown(f"**{title}**")
                    st.caption(f"{ch} Â· ì¡°íšŒìˆ˜ {views:,} Â· ë“±ê¸‰ {grade}")
                    if link:
                        st.markdown(f"[ì˜ìƒ ì—´ê¸°]({link})")

            if (idx + 1) % n_cols == 0 and (idx + 1) < len(df_display):
                cols = st.columns(n_cols)

        st.caption("ğŸ‘‰ ì¹´ë“œë¥¼ í´ë¦­í•´ì„œ ë§í¬ë¥¼ ì—´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # ---------- ë¦¬ìŠ¤íŠ¸ ë·° ----------
    else:  # "ë¦¬ìŠ¤íŠ¸ ë·°"
        if mode in ("general", "trend", "channel_videos"):
            base_order = [
                "ë“±ê¸‰",
                "ì¸ë„¤ì¼",
                "ì±„ë„ëª…",
                "ì˜ìƒì¡°íšŒìˆ˜",
                "ì‹œê°„ë‹¹í´ë¦­",
                "ì˜ìƒê¸¸ì´",
                "ì—…ë¡œë“œì‹œê°",
                "ê²½ê³¼ì‹œê°„",
                "ì œëª©",
                "ë§í¬",
            ]
        else:
            base_order = [
                "ì¸ë„¤ì¼",
                "ì±„ë„ëª…",
                "êµ¬ë…ììˆ˜",
                "ì±„ë„ì¡°íšŒìˆ˜",
                "ì±„ë„ì˜ìƒìˆ˜",
                "ë§í¬",
            ]
        column_order = [c for c in base_order if c in df_display.columns]

        column_config = {}
        if "ë§í¬" in df_display.columns:
            column_config["ë§í¬"] = st.column_config.LinkColumn(
                "ì—´ê¸°",
                display_text="ì—´ê¸°",
            )
        if "ì¸ë„¤ì¼" in df_display.columns:
            column_config["ì¸ë„¤ì¼"] = st.column_config.ImageColumn(
                "ì¸ë„¤ì¼",
                help="ì¸ë„¤ì¼ ì´ë¯¸ì§€",
                width="small",
            )

        if mode == "general":
            editor_key = "video_results_editor_general"
        elif mode == "trend":
            editor_key = "video_results_editor_trend"
        elif mode == "channel_videos":
            editor_key = "video_results_editor_channel_videos"
        elif mode == "channel_list":
            editor_key = "channel_results_editor_keyword"
        else:
            editor_key = "results_editor_default"

        st.data_editor(
            df_display,
            use_container_width=True,
            height=680,
            hide_index=True,
            column_order=column_order if column_order else None,
            column_config=column_config,
            key=editor_key,
            disabled=True,
            num_rows="fixed",
        )

        st.caption("ğŸ‘‰ 'ì—´ê¸°' ë§í¬ë¥¼ ëˆ„ë¥´ë©´ ìƒˆ íƒ­ì—ì„œ ì˜ìƒ ë˜ëŠ” ì±„ë„ì´ ì—´ë¦½ë‹ˆë‹¤.")

# -----------------------------
# ì‚¬ì´ë“œë°” í•˜ë‹¨: ì¿¼í„° / ìµœê·¼ í‚¤ì›Œë“œ / ë¡œê·¸ì•„ì›ƒ
# -----------------------------
st.sidebar.markdown("---")

quota_today = get_today_quota_total()
st.sidebar.caption(f"ì˜¤ëŠ˜ ì‚¬ìš© ì¿¼í„°: {quota_today:,} units")

recents = get_recent_keywords(7)
if recents:
    keywords = [q for _, q in recents]
    labels = [f"`{k}`" for k in keywords]
    st.sidebar.caption("ìµœê·¼ í‚¤ì›Œë“œ: " + " Â· ".join(labels))
else:
    st.sidebar.caption("ìµœê·¼ í‚¤ì›Œë“œ: ì—†ìŒ")

if st.sidebar.button("ë¡œê·¸ì•„ì›ƒ", use_container_width=True):
    st.session_state["logged_in"] = False
    st.rerun()
