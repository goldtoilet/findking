#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
from datetime import datetime, timedelta, timezone

import streamlit as st
import pandas as pd

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

from supabase import create_client, Client

# -----------------------------
# Streamlit ê¸°ë³¸ ì„¤ì •
# -----------------------------
st.set_page_config(
    page_title="YouTubeê²€ìƒ‰ê¸°",
    page_icon="ğŸ”",
    layout="wide",
)

# ìƒë‹¨ ì—¬ë°± ì¡°ì • + DataEditor ì•„ì´ì½˜(3ì  ë©”ë‰´ ë“±) ìˆ¨ê¸°ê¸° (ê°€ëŠ¥í•œ ì„ ì—ì„œ)
st.markdown(
    """
    <style>
    .block-container { padding-top: 3rem !important; }
    /* DataEditor ë‚´ ì•„ì´ì½˜ ë²„íŠ¼(3ì  ë©”ë‰´ ë“±) ìˆ¨ê¸°ê¸° ì‹œë„ */
    [data-testid="stDataEditor"] button[kind="icon"] {
        display: none !important;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# -----------------------------
# ë¡œê·¸ì¸ ì„¤ì •
# -----------------------------
LOGIN_ID_ENV = os.getenv("LOGIN_ID", "")
LOGIN_PW_ENV = os.getenv("LOGIN_PW", "")

st.session_state.setdefault("logged_in", False)
st.session_state.setdefault("login_id", LOGIN_ID_ENV or "")
st.session_state.setdefault("login_pw", LOGIN_PW_ENV or "")

def check_login(id_text: str, pw_text: str) -> bool:
    # í™˜ê²½ë³€ìˆ˜ì— ê°’ì´ ìˆìœ¼ë©´ ê·¸ê²ƒê³¼ ë¹„êµ
    if LOGIN_ID_ENV and LOGIN_PW_ENV:
        return (id_text == LOGIN_ID_ENV) and (pw_text == LOGIN_PW_ENV)
    # í™˜ê²½ë³€ìˆ˜ê°€ ë¹„ì–´ ìˆìœ¼ë©´ ê·¸ëƒ¥ í†µê³¼(ê°œë°œìš©)
    return True

if not st.session_state["logged_in"]:
    st.markdown("### ğŸ”’ YouTubeê²€ìƒ‰ê¸° ë¡œê·¸ì¸")

    id_input = st.text_input("ë¡œê·¸ì¸ ID", value=st.session_state["login_id"])
    pw_input = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password", value=st.session_state["login_pw"])

    col_l, col_r = st.columns([1, 3])
    with col_l:
        login_btn = st.button("ë¡œê·¸ì¸", type="primary", use_container_width=True)

    if login_btn:
        st.session_state["login_id"] = id_input
        st.session_state["login_pw"] = pw_input
        if check_login(id_input, pw_input):
            st.session_state["logged_in"] = True
            st.success("ë¡œê·¸ì¸ ì„±ê³µ!")
            st.rerun()
        else:
            st.error("ë¡œê·¸ì¸ ì‹¤íŒ¨. ID ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    st.stop()

# ë¡œê·¸ì¸ ì´í›„ í™”ë©´ ì œëª© (ì‘ê²Œ)
st.markdown("### ğŸ” YouTubeê²€ìƒ‰ê¸°")

# -----------------------------
# ê³µí†µ ìƒìˆ˜/í™˜ê²½
# -----------------------------
KST = timezone(timedelta(hours=9))
WEEKDAY_KO = ["ì›”ìš”ì¼","í™”ìš”ì¼","ìˆ˜ìš”ì¼","ëª©ìš”ì¼","ê¸ˆìš”ì¼","í† ìš”ì¼","ì¼ìš”ì¼"]

COUNTRY_LANG_MAP = {
    "í•œêµ­": ("KR", "ko"),
    "ì¼ë³¸": ("JP", "ja"),
    "ë¯¸êµ­": ("US", "en"),
    "ì˜êµ­": ("GB", "en"),
    "ë…ì¼": ("DE", "de"),
    "í”„ë‘ìŠ¤": ("FR", "fr"),
    "ìŠ¤í˜ì¸": ("ES", "es"),
    "ì´íƒˆë¦¬ì•„": ("IT", "it"),
    "ë¸Œë¼ì§ˆ": ("BR", "pt"),
    "ì¸ë„": ("IN", "en"),
    "í˜¸ì£¼": ("AU", "en"),
}
COUNTRY_LIST = list(COUNTRY_LANG_MAP.keys())

# -----------------------------
# Supabase í´ë¼ì´ì–¸íŠ¸
# -----------------------------
@st.cache_resource
def get_supabase_client() -> Client | None:
    url = st.secrets.get("SUPABASE_URL")
    key = st.secrets.get("SUPABASE_SERVICE_KEY")
    if not url or not key:
        return None
    return create_client(url, key)

SUPABASE_BUCKET = st.secrets.get("SUPABASE_BUCKET", "yts-config")
supabase = get_supabase_client()

def _load_json(filename: str, default):
    if supabase is None:
        return default
    try:
        res = supabase.storage.from_(SUPABASE_BUCKET).download(filename)
        if res is None:
            return default
        if isinstance(res, bytes):
            text = res.decode("utf-8")
        else:
            text = str(res)
        return json.loads(text)
    except Exception:
        return default

def _save_json(filename: str, data):
    if supabase is None:
        return
    try:
        payload = json.dumps(data, ensure_ascii=False, indent=2).encode("utf-8")
        supabase.storage.from_(SUPABASE_BUCKET).upload(
            path=filename,
            file=payload,
            file_options={"content-type": "application/json", "x-upsert": "true"},
        )
    except Exception as e:
        st.warning(f"Supabase ì €ì¥ ì˜¤ë¥˜({filename}): {e}")

# -----------------------------
# ê²½ë¡œ/íŒŒì¼ëª… ì •ì˜ (Supabaseìš©)
# -----------------------------
KEYWORD_LOG_PATH  = "yts_keyword_log.json"
QUOTA_PATH        = "yts_quota_usage.json"

# -----------------------------
# API í‚¤: secretsì—ì„œë§Œ ì‚¬ìš©
# -----------------------------
def get_current_api_key() -> str:
    """
    ìš°ì„ ìˆœìœ„:
    1) YOUTUBE_API_KEYS (ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ì¤„ë°”ê¿ˆ ë¬¸ìì—´) ì˜ ì²« ë²ˆì§¸
    2) YOUTUBE_API_KEY (ë‹¨ì¼ ë¬¸ìì—´)
    """
    keys = st.secrets.get("YOUTUBE_API_KEYS")
    if isinstance(keys, list) and keys:
        return str(keys[0]).strip()
    if isinstance(keys, str) and keys.strip():
        first = keys.strip().splitlines()[0]
        return first.strip()

    single = st.secrets.get("YOUTUBE_API_KEY")
    if isinstance(single, str) and single.strip():
        return single.strip()

    return ""

def get_youtube_client():
    key = get_current_api_key()
    if not key:
        raise RuntimeError(
            "YouTube API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
            "â–¶ .streamlit/secrets.toml ì— YOUTUBE_API_KEYS ë˜ëŠ” YOUTUBE_API_KEY ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
        )
    try:
        return build("youtube", "v3", developerKey=key, cache_discovery=False)
    except TypeError:
        return build("youtube", "v3", developerKey=key)

# -----------------------------
# ì¿¼í„° ê¸°ë¡
# -----------------------------
def _load_quota_map():
    return _load_json(QUOTA_PATH, {})

def _save_quota_map(data: dict):
    _save_json(QUOTA_PATH, data)

def quota_today_key():
    return datetime.now(KST).strftime("%Y-%m-%d")

def get_today_quota_total() -> int:
    data = _load_quota_map()
    return int(data.get(quota_today_key(), 0))

def add_quota_usage(units: int):
    if units <= 0:
        return
    data = _load_quota_map()
    key = quota_today_key()
    data[key] = int(data.get(key, 0)) + int(units)
    _save_quota_map(data)

# -----------------------------
# ìµœê·¼ ê²€ìƒ‰ í‚¤ì›Œë“œ
# -----------------------------
def _load_keyword_log():
    return _load_json(KEYWORD_LOG_PATH, [])

def _save_keyword_log(entries: list):
    _save_json(KEYWORD_LOG_PATH, entries)

def append_keyword_log(query: str):
    q = (query or "").strip()
    if not q:
        return
    entries = _load_keyword_log()
    now = datetime.now(KST).isoformat(timespec="seconds")
    entries.append({"ts": now, "q": q})
    _save_keyword_log(entries)

def get_recent_keywords(limit: int = 30):
    entries = _load_keyword_log()
    out = []
    for item in entries:
        ts = item.get("ts")
        q  = item.get("q")
        if not ts or not q:
            continue
        try:
            dt = datetime.fromisoformat(ts)
        except Exception:
            continue
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=KST)
        out.append((dt, q))
    out.sort(key=lambda x: x[0], reverse=True)
    return out[:limit]

# -----------------------------
# ì‹œê°„/í˜•ì‹ ìœ í‹¸
# -----------------------------
def parse_published_at_to_kst(published_iso: str) -> datetime:
    dt_utc = datetime.fromisoformat(published_iso.replace("Z", "+00:00"))
    return dt_utc.astimezone(KST)

def human_elapsed_days_hours(later: datetime, earlier: datetime) -> tuple[int, int]:
    delta = later - earlier
    if delta.total_seconds() < 0:
        return 0, 0
    days = delta.days
    hours = delta.seconds // 3600
    return days, hours

def published_after_from_label(label: str):
    label = label.strip()
    now_utc = datetime.now(timezone.utc)
    if label == "ì œí•œì—†ìŒ":
        return None
    if label.endswith("ì¼"):
        days = int(label[:-1])
        dt = now_utc - timedelta(days=days)
    else:
        return None
    return dt.isoformat(timespec="seconds").replace("+00:00", "Z")

def parse_duration_iso8601(iso_dur: str) -> int:
    h = m = s = 0
    if not iso_dur or not iso_dur.startswith("PT"):
        return 0
    num = ""
    for ch in iso_dur[2:]:
        if ch.isdigit():
            num += ch
        else:
            if ch == "H" and num:
                h = int(num); num = ""
            elif ch == "M" and num:
                m = int(num); num = ""
            elif ch == "S" and num:
                s = int(num); num = ""
    return h*3600 + m*60 + s

def format_duration_hms(seconds: int) -> str:
    if seconds <= 0:
        return "0:00"
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    if h > 0:
        return f"{h}:{m:02d}:{s:02d}"
    return f"{m}:{s:02d}"

def duration_filter_ok(seconds: int, label: str) -> bool:
    if label == "ì „ì²´": return True
    if label == "ì‡¼ì¸ ": return seconds < 60
    if label == "ë¡±í¼": return seconds >= 60
    if label == "1~20ë¶„": return 60 <= seconds < 20*60
    if label == "20~40ë¶„": return 20*60 <= seconds < 40*60
    if label == "40~60ë¶„": return 40*60 <= seconds < 60*60
    if label == "60ë¶„ì´ìƒ": return seconds >= 60*60
    return True

def parse_min_views(text: str) -> int:
    digits = text.replace(",", "").replace(" ", "").replace("ë§Œ", "0000")
    try:
        return int(digits)
    except Exception:
        return 0

# -----------------------------
# ë“±ê¸‰ ê³„ì‚° (ì‹œê°„ë‹¹ í´ë¦­ìˆ˜ ê¸°ì¤€) A~H ì¬ë§¤í•‘
# -----------------------------
def calc_grade(clicks_per_hour: int) -> str:
    v = clicks_per_hour
    if v >= 5000: return "A"
    if v >= 2000: return "B"
    if v >= 1000: return "C"
    if v >= 500:  return "D"
    if v >= 300:  return "E"
    if v >= 100:  return "F"
    if v >= 50:   return "G"
    return "H"

# -----------------------------
# YouTube API í˜¸ì¶œ í•¨ìˆ˜ë“¤
# -----------------------------
def search_videos(
    query: str,
    min_views: int,
    api_period_label: str,
    duration_label: str,
    max_fetch: int,
    region_code: str | None,
    lang_code: str | None,
):
    """ì¼ë°˜ í‚¤ì›Œë“œë¡œ ì˜ìƒ ê²€ìƒ‰"""
    youtube = get_youtube_client()
    published_after = published_after_from_label(api_period_label)

    cost_used = 0
    breakdown = {"search.list": 0, "videos.list": 0}
    max_fetch = max(1, min(int(max_fetch or 100), 5000))

    results_tmp = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        kwargs = dict(
            q=query,
            part="id",
            type="video",
            maxResults=take,
        )
        if published_after:
            kwargs["publishedAfter"] = published_after
        if region_code:
            kwargs["regionCode"] = region_code
        if lang_code:
            kwargs["relevanceLanguage"] = lang_code
        if next_token:
            kwargs["pageToken"] = next_token

        try:
            search_response = youtube.search().list(**kwargs).execute()
            cost_used += 100; breakdown["search.list"] += 100
        except HttpError as e:
            raise RuntimeError(f"Search API ì˜¤ë¥˜: {e}")

        page_ids = [
            it["id"]["videoId"]
            for it in search_response.get("items", [])
            if "id" in it and "videoId" in it["id"]
        ]
        if not page_ids:
            break

        try:
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(page_ids),
            ).execute()
            cost_used += 1; breakdown["videos.list"] += 1
        except HttpError as e:
            raise RuntimeError(f"Videos API ì˜¤ë¥˜: {e}")

        items = video_response.get("items", [])
        for item in items:
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}

            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

            # ì¸ë„¤ì¼ URL
            thumbs = snip.get("thumbnails", {}) or {}
            thumb_url = (
                (thumbs.get("maxres") or {}).get("url")
                or (thumbs.get("standard") or {}).get("url")
                or (thumbs.get("high") or {}).get("url")
                or (thumbs.get("medium") or {}).get("url")
                or (thumbs.get("default") or {}).get("url")
                or ""
            )

            if view_count < min_views:
                continue
            if not duration_filter_ok(duration_sec, duration_label):
                continue

            results_tmp.append({
                "title": title,
                "views": view_count,
                "published_at_iso": published_at_iso,
                "url": url,
                "duration_sec": duration_sec,
                "channel_title": snip.get("channelTitle", ""),
                "thumbnail_url": thumb_url,
            })

        fetched += len(page_ids)
        next_token = search_response.get("nextPageToken")
        if not next_token:
            break

    return results_tmp, cost_used, breakdown

def search_channels_by_keyword(
    keyword: str,
    max_results: int,
    region_code: str | None,
    lang_code: str | None,
):
    """ì±„ë„ í‚¤ì›Œë“œë¡œ ì±„ë„ ëª©ë¡ ê²€ìƒ‰ (ì±„ë„ ì•„ì´ì½˜ ì¸ë„¤ì¼ê¹Œì§€)"""
    youtube = get_youtube_client()
    take = max(1, min(max_results, 50))
    kwargs = dict(
        q=keyword,
        part="id",
        type="channel",
        maxResults=take,
    )
    if region_code:
        kwargs["regionCode"] = region_code
    if lang_code:
        kwargs["relevanceLanguage"] = lang_code

    try:
        search_response = youtube.search().list(**kwargs).execute()
        cost_used = 100
    except HttpError as e:
        raise RuntimeError(f"Channel search API ì˜¤ë¥˜: {e}")

    ch_ids = [
        it["id"]["channelId"]
        for it in search_response.get("items", [])
        if "id" in it and "channelId" in it["id"]
    ]
    if not ch_ids:
        return [], cost_used, {"search.list": 100, "channels.list": 0}

    try:
        ch_resp = youtube.channels().list(
            part="snippet,statistics",
            id=",".join(ch_ids),
        ).execute()
        cost_used += 1
    except HttpError as e:
        raise RuntimeError(f"Channels API ì˜¤ë¥˜: {e}")

    results = []
    for c in ch_resp.get("items", []):
        cid = c.get("id", "")
        sn = c.get("snippet", {}) or {}
        stt = c.get("statistics", {}) or {}
        subs = int(stt.get("subscriberCount", 0)) if stt.get("subscriberCount") is not None else None
        total_views = int(stt.get("viewCount", 0))
        videos = int(stt.get("videoCount", 0))
        url = f"https://www.youtube.com/channel/{cid}" if cid else ""

        thumbs = sn.get("thumbnails", {}) or {}
        thumb_url = (
            (thumbs.get("high") or {}).get("url")
            or (thumbs.get("medium") or {}).get("url")
            or (thumbs.get("default") or {}).get("url")
            or ""
        )

        results.append({
            "channel_title": sn.get("title", ""),
            "subs": subs,
            "total_views": total_views,
            "videos": videos,
            "url": url,
            "thumbnail_url": thumb_url,  # ì±„ë„ ì•„ì´ì½˜
        })

    results.sort(key=lambda r: (r["subs"] or 0), reverse=True)
    return results, cost_used, {"search.list": 100, "channels.list": 1}

def search_videos_in_channel_by_name(
    channel_name: str,
    min_views: int,
    api_period_label: str,
    duration_label: str,
    max_fetch: int,
    region_code: str | None,
    lang_code: str | None,
):
    """ì±„ë„ ì´ë¦„ìœ¼ë¡œ ì±„ë„ì„ ì°¾ê³ , í•´ë‹¹ ì±„ë„ì˜ ì˜ìƒë“¤ì„ ê²€ìƒ‰"""
    youtube = get_youtube_client()
    published_after = published_after_from_label(api_period_label)
    cost_used = 0
    breakdown = {"search.list": 0, "videos.list": 0}

    # 1) ì±„ë„ ID ì°¾ê¸°
    kwargs_ch = dict(
        q=channel_name,
        part="id,snippet",
        type="channel",
        maxResults=1,
    )
    if region_code:
        kwargs_ch["regionCode"] = region_code
    if lang_code:
        kwargs_ch["relevanceLanguage"] = lang_code

    try:
        ch_resp = youtube.search().list(**kwargs_ch).execute()
        cost_used += 100; breakdown["search.list"] += 100
    except HttpError as e:
        raise RuntimeError(f"ì±„ë„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

    items = ch_resp.get("items", [])
    if not items:
        return [], cost_used, breakdown

    channel_id = items[0]["id"]["channelId"]

    # 2) í•´ë‹¹ ì±„ë„ ì˜ìƒ ê²€ìƒ‰
    max_fetch = max(1, min(int(max_fetch or 100), 5000))
    results_tmp = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        kwargs = dict(
            part="id",
            type="video",
            channelId=channel_id,
            maxResults=take,
            order="date",
        )
        if published_after:
            kwargs["publishedAfter"] = published_after
        if region_code:
            kwargs["regionCode"] = region_code
        if lang_code:
            kwargs["relevanceLanguage"] = lang_code
        if next_token:
            kwargs["pageToken"] = next_token

        try:
            v_search = youtube.search().list(**kwargs).execute()
            cost_used += 100; breakdown["search.list"] += 100
        except HttpError as e:
            raise RuntimeError(f"ì±„ë„ ì˜ìƒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

        page_ids = [
            it["id"]["videoId"]
            for it in v_search.get("items", [])
            if "id" in it and "videoId" in it["id"]
        ]
        if not page_ids:
            break

        try:
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(page_ids),
            ).execute()
            cost_used += 1; breakdown["videos.list"] += 1
        except HttpError as e:
            raise RuntimeError(f"Videos API ì˜¤ë¥˜: {e}")

        items2 = video_response.get("items", [])
        for item in items2:
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}

            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

            thumbs = snip.get("thumbnails", {}) or {}
            thumb_url = (
                (thumbs.get("maxres") or {}).get("url")
                or (thumbs.get("standard") or {}).get("url")
                or (thumbs.get("high") or {}).get("url")
                or (thumbs.get("medium") or {}).get("url")
                or (thumbs.get("default") or {}).get("url")
                or ""
            )

            if view_count < min_views:
                continue
            if not duration_filter_ok(duration_sec, duration_label):
                continue

            results_tmp.append({
                "title": title,
                "views": view_count,
                "published_at_iso": published_at_iso,
                "url": url,
                "duration_sec": duration_sec,
                "channel_title": snip.get("channelTitle", ""),
                "thumbnail_url": thumb_url,
            })

        fetched += len(page_ids)
        next_token = v_search.get("nextPageToken")
        if not next_token:
            break

    return results_tmp, cost_used, breakdown

def search_trending_videos(
    max_results: int,
    region_code: str | None,
):
    """
    ë‹¨ìˆœ íŠ¸ë Œë“œ(ì¸ê¸° ë™ì˜ìƒ) ê°€ì ¸ì˜¤ê¸°.
    YouTube Data API: videos().list(chart="mostPopular")
    """
    youtube = get_youtube_client()
    take = max(1, min(max_results, 50))
    kwargs = dict(
        part="snippet,statistics,contentDetails",
        chart="mostPopular",
        maxResults=take,
    )
    if region_code:
        kwargs["regionCode"] = region_code

    try:
        resp = youtube.videos().list(**kwargs).execute()
        cost_used = 1
    except HttpError as e:
        raise RuntimeError(f"íŠ¸ë Œë“œ API ì˜¤ë¥˜: {e}")

    results = []
    for item in resp.get("items", []):
        vid = item.get("id", "")
        snip = item.get("snippet", {}) or {}
        stats = item.get("statistics", {}) or {}
        cdet = item.get("contentDetails", {}) or {}

        title = snip.get("title", "")
        published_at_iso = snip.get("publishedAt", "")
        view_count = int(stats.get("viewCount", 0))
        url = f"https://www.youtube.com/watch?v={vid}"
        duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

        thumbs = snip.get("thumbnails", {}) or {}
        thumb_url = (
            (thumbs.get("maxres") or {}).get("url")
            or (thumbs.get("standard") or {}).get("url")
            or (thumbs.get("high") or {}).get("url")
            or (thumbs.get("medium") or {}).get("url")
            or (thumbs.get("default") or {}).get("url")
            or ""
        )

        results.append({
            "title": title,
            "views": view_count,
            "published_at_iso": published_at_iso,
            "url": url,
            "duration_sec": duration_sec,
            "channel_title": snip.get("channelTitle", ""),
            "thumbnail_url": thumb_url,
        })
    return results, cost_used, {"videos.list": 1}

# -----------------------------
# Sidebar UI
# -----------------------------
st.sidebar.header("ê²€ìƒ‰")

# â‘  ì¼ë°˜ ê²€ìƒ‰ì–´ + ë²„íŠ¼
general_query = st.sidebar.text_input("ğŸ” ì¼ë°˜ ê²€ìƒ‰ì–´", "")
do_general_search = st.sidebar.button("ì¼ë°˜ ê²€ìƒ‰ ì‹¤í–‰", type="primary", use_container_width=True)

# ë³´ê¸° ëª¨ë“œ: ë¦¬ìŠ¤íŠ¸ / ê·¸ë¦¬ë“œ / ì‡¼ì¸  (ë‹¨ì¼ ì„ íƒ)
view_mode = st.sidebar.selectbox(
    "ë³´ê¸° ëª¨ë“œ",
    ["ë¦¬ìŠ¤íŠ¸ë·°", "ê·¸ë¦¬ë“œë·°", "ì‡¼ì¸ ë·°"],
    index=1,   # ê¸°ë³¸ê°’: ê·¸ë¦¬ë“œë·°
)

# ì •ë ¬ ë°©ì‹ (ë¦¬ìŠ¤íŠ¸ë·°/ê·¸ë¦¬ë“œë·°ì—ì„œ ì‚¬ìš©)
with st.sidebar.expander("ì •ë ¬ ë°©ì‹", expanded=False):
    sort_target = st.selectbox(
        "ì •ë ¬ ê¸°ì¤€",
        ["ìë™", "ë“±ê¸‰", "ì˜ìƒì¡°íšŒìˆ˜", "ì‹œê°„ë‹¹í´ë¦­", "ì—…ë¡œë“œì‹œê°", "ì œëª©", "ì±„ë„ëª…", "êµ¬ë…ììˆ˜"],
        key="sort_target",
    )
    sort_order = st.radio(
        "ì •ë ¬ ë°©í–¥",
        ["ë‚´ë¦¼ì°¨ìˆœ", "ì˜¤ë¦„ì°¨ìˆœ"],
        index=0,
        horizontal=True,
        key="sort_order",
    )

st.sidebar.markdown("---")

# â‘¡ íŠ¸ë Œë“œ ê²€ìƒ‰
with st.sidebar.expander("ğŸ”¥ íŠ¸ë Œë“œ ê²€ìƒ‰", expanded=False):
    trend_region_name = st.selectbox(
        "íŠ¸ë Œë“œ ì§€ì—­ (êµ­ê°€)",
        COUNTRY_LIST,
        index=0,
        key="trend_region",
    )
    do_trend_search = st.button("íŠ¸ë Œë“œ ë¶ˆëŸ¬ì˜¤ê¸°", use_container_width=True, key="btn_trend")

# â‘¢ ì±„ë„ì˜ìƒê²€ìƒ‰
with st.sidebar.expander("ğŸ¬ ì±„ë„ì˜ìƒê²€ìƒ‰", expanded=False):
    channel_name_for_videos = st.text_input("ì±„ë„ ì´ë¦„", key="channel_name_videos")
    do_channel_videos_search = st.button(
        "ì±„ë„ ì˜ìƒ ê²€ìƒ‰ ì‹¤í–‰",
        use_container_width=True,
        key="btn_channel_videos"
    )

# â‘£ í‚¤ì›Œë“œì±„ë„ê²€ìƒ‰
with st.sidebar.expander("ğŸ“ˆ í‚¤ì›Œë“œì±„ë„ê²€ìƒ‰", expanded=False):
    channel_keyword = st.text_input("ì±„ë„ í‚¤ì›Œë“œ", key="channel_keyword")
    do_channel_list_search = st.button(
        "ì±„ë„ ëª©ë¡ ê²€ìƒ‰ ì‹¤í–‰",
        use_container_width=True,
        key="btn_channel_list"
    )

st.sidebar.markdown("---")

# ì„¸ë¶€ í•„í„° (í•­ìƒ í¼ì³ì§„ ìƒíƒœë¡œ ì‹œì‘)
with st.sidebar.expander("âš™ ì„¸ë¶€ í•„í„°", expanded=True):
    api_period = st.selectbox(
        "ì„œë²„ ê²€ìƒ‰ê¸°ê°„ (YouTube API)",
        ["ì œí•œì—†ìŒ","90ì¼","150ì¼","365ì¼","730ì¼","1095ì¼","1825ì¼","3650ì¼"],
        index=1,
        key="api_period",
    )
    upload_period = st.selectbox(
        "ì—…ë¡œë“œ ê¸°ê°„(í´ë¼ì´ì–¸íŠ¸ í•„í„°)",
        ["ì œí•œì—†ìŒ","1ì¼","3ì¼","7ì¼","14ì¼","30ì¼","60ì¼","90ì¼","180ì¼","365ì¼"],
        index=6,
        key="upload_period",
    )
    min_views_label = st.selectbox(
        "ìµœì†Œ ì¡°íšŒìˆ˜",
        ["5,000","10,000","25,000","50,000","100,000","200,000","500,000","1,000,000"],
        index=0,
        key="min_views_label",
    )
    duration_label = st.selectbox(
        "ì˜ìƒ ê¸¸ì´",
        ["ì „ì²´","ì‡¼ì¸ ","ë¡±í¼","1~20ë¶„","20~40ë¶„","40~60ë¶„","60ë¶„ì´ìƒ"],
        index=0,
        key="duration_label",
    )
    max_fetch = st.number_input(
        "ëª¨ë“  ê²€ìƒ‰ì—ì„œ ê°€ì ¸ì˜¬ ìµœëŒ€ ê°œìˆ˜",
        1, 5000, 50, step=10,
        key="max_fetch",
    )
    country_name = st.selectbox("ê²€ìƒ‰ìš© êµ­ê°€/ì–¸ì–´", COUNTRY_LIST, index=0, key="country_for_search")
    region_code, lang_code = COUNTRY_LANG_MAP[country_name]

status_placeholder = st.empty()

# -----------------------------
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# -----------------------------
if "results_df" not in st.session_state:
    st.session_state.results_df = None
    st.session_state.last_search_time = None
    st.session_state.search_mode = None  # "general", "trend", "channel_videos", "channel_list"

# í´ë¼ì´ì–¸íŠ¸ í•„í„°
def apply_client_filters(df: pd.DataFrame, upload_period: str, min_views_label: str) -> pd.DataFrame:
    if upload_period != "ì œí•œì—†ìŒ" and "ì—…ë¡œë“œì‹œê°" in df.columns:
        days = int(upload_period.replace("ì¼",""))
        cutoff = datetime.now(KST) - timedelta(days=days)
        df = df[df["ì—…ë¡œë“œì‹œê°"] >= cutoff]
    min_views = parse_min_views(min_views_label)
    if "ì˜ìƒì¡°íšŒìˆ˜" in df.columns:
        df = df[df["ì˜ìƒì¡°íšŒìˆ˜"] >= min_views]
    return df

# -----------------------------
# ê²€ìƒ‰ ì‹¤í–‰ ë¡œì§
# -----------------------------
try:
    # ì–´ë–¤ ë²„íŠ¼ì´ ëˆŒë ¸ëŠ”ì§€ íŒë‹¨
    mode_triggered = None
    if do_general_search:
        mode_triggered = "general"
    elif do_trend_search:
        mode_triggered = "trend"
    elif do_channel_videos_search:
        mode_triggered = "channel_videos"
    elif do_channel_list_search:
        mode_triggered = "channel_list"

    if mode_triggered is not None:
        search_dt = datetime.now(KST)

        # ------ ì¼ë°˜ ê²€ìƒ‰ ------
        if mode_triggered == "general":
            base_query = (general_query or "").strip()
            if not base_query:
                st.warning("ì¼ë°˜ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                append_keyword_log(base_query)
                status_placeholder.info("ì¼ë°˜ ì˜ìƒ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
                raw_results, cost_used, breakdown = search_videos(
                    query=base_query,
                    min_views=parse_min_views(min_views_label),
                    api_period_label=api_period,
                    duration_label=duration_label,
                    max_fetch=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
                add_quota_usage(cost_used)

                if not raw_results:
                    st.session_state.results_df = None
                    st.session_state.search_mode = "general"
                    status_placeholder.info("ì„œë²„ ê²°ê³¼ 0ê±´")
                else:
                    rows = []
                    for r in raw_results:
                        pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                        d, h = human_elapsed_days_hours(search_dt, pub_kst)
                        total_hours = max(1, d*24 + h)
                        cph = int(round(r["views"] / total_hours))
                        rows.append({
                            "ë“±ê¸‰": calc_grade(cph),
                            "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),
                            "ì±„ë„ëª…": r["channel_title"],
                            "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                            "ì‹œê°„ë‹¹í´ë¦­": cph,
                            "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                            "ì—…ë¡œë“œì‹œê°": pub_kst,
                            "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                            "ì œëª©": r["title"],
                            "ë§í¬URL": r["url"],
                        })
                    df = pd.DataFrame(rows)
                    if not df.empty:
                        df = apply_client_filters(df, upload_period, min_views_label)
                    st.session_state.results_df = df
                    st.session_state.last_search_time = search_dt
                    st.session_state.search_mode = "general"
                    status_placeholder.success(
                        f"[ì¼ë°˜ ê²€ìƒ‰] ì„œë²„ ê²°ê³¼: {len(raw_results):,}ê±´ / "
                        f"í•„í„° í›„: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                    )

        # ------ íŠ¸ë Œë“œ ê²€ìƒ‰ ------
        elif mode_triggered == "trend":
            trend_rc, _ = COUNTRY_LANG_MAP[trend_region_name]
            append_keyword_log(f"[trend]{trend_region_name}")
            status_placeholder.info("íŠ¸ë Œë“œ(ì¸ê¸° ë™ì˜ìƒ) ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
            raw_results, cost_used, breakdown = search_trending_videos(
                max_results=max_fetch,
                region_code=trend_rc,
            )
            add_quota_usage(cost_used)

            if not raw_results:
                st.session_state.results_df = None
                st.session_state.search_mode = "trend"
                status_placeholder.info("íŠ¸ë Œë“œ ê²°ê³¼ 0ê±´")
            else:
                rows = []
                for r in raw_results:
                    pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                    d, h = human_elapsed_days_hours(search_dt, pub_kst)
                    total_hours = max(1, d*24 + h)
                    cph = int(round(r["views"] / total_hours))
                    rows.append({
                        "ë“±ê¸‰": calc_grade(cph),
                        "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),
                        "ì±„ë„ëª…": r["channel_title"],
                        "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                        "ì‹œê°„ë‹¹í´ë¦­": cph,
                        "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                        "ì—…ë¡œë“œì‹œê°": pub_kst,
                        "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                        "ì œëª©": r["title"],
                        "ë§í¬URL": r["url"],
                    })
                df = pd.DataFrame(rows)
                if not df.empty:
                    df = apply_client_filters(df, upload_period, min_views_label)
                st.session_state.results_df = df
                st.session_state.last_search_time = search_dt
                st.session_state.search_mode = "trend"
                status_placeholder.success(
                    f"[íŠ¸ë Œë“œ] ê²°ê³¼: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                )

        # ------ ì±„ë„ì˜ìƒê²€ìƒ‰ ------
        elif mode_triggered == "channel_videos":
            ch_name = (channel_name_for_videos or "").strip()
            if not ch_name:
                st.warning("ì±„ë„ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                append_keyword_log(f"[channel_videos]{ch_name}")
                status_placeholder.info("ì±„ë„ ì˜ìƒ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
                raw_results, cost_used, breakdown = search_videos_in_channel_by_name(
                    channel_name=ch_name,
                    min_views=parse_min_views(min_views_label),
                    api_period_label=api_period,
                    duration_label=duration_label,
                    max_fetch=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
                add_quota_usage(cost_used)

                if not raw_results:
                    st.session_state.results_df = None
                    st.session_state.search_mode = "channel_videos"
                    status_placeholder.info("ì±„ë„ ì˜ìƒ ê²°ê³¼ 0ê±´")
                else:
                    rows = []
                    for r in raw_results:
                        pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                        d, h = human_elapsed_days_hours(search_dt, pub_kst)
                        total_hours = max(1, d*24 + h)
                        cph = int(round(r["views"] / total_hours))
                        rows.append({
                            "ë“±ê¸‰": calc_grade(cph),
                            "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),
                            "ì±„ë„ëª…": r["channel_title"],
                            "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                            "ì‹œê°„ë‹¹í´ë¦­": cph,
                            "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                            "ì—…ë¡œë“œì‹œê°": pub_kst,
                            "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                            "ì œëª©": r["title"],
                            "ë§í¬URL": r["url"],
                        })
                    df = pd.DataFrame(rows)
                    if not df.empty:
                        df = apply_client_filters(df, upload_period, min_views_label)
                    st.session_state.results_df = df
                    st.session_state.last_search_time = search_dt
                    st.session_state.search_mode = "channel_videos"
                    status_placeholder.success(
                        f"[ì±„ë„ ì˜ìƒ] ì„œë²„ ê²°ê³¼: {len(raw_results):,}ê±´ / "
                        f"í•„í„° í›„: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                    )

        # ------ í‚¤ì›Œë“œì±„ë„ê²€ìƒ‰ ------
        elif mode_triggered == "channel_list":
            ch_kw = (channel_keyword or "").strip()
            if not ch_kw:
                st.warning("ì±„ë„ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                append_keyword_log(f"[channel]{ch_kw}")
                status_placeholder.info("ì±„ë„ ëª©ë¡ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
                ch_results, cost_used, breakdown = search_channels_by_keyword(
                    keyword=ch_kw,
                    max_results=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
                add_quota_usage(cost_used)

                if not ch_results:
                    st.session_state.results_df = None
                    st.session_state.search_mode = "channel_list"
                    status_placeholder.info("ì±„ë„ ê²°ê³¼ 0ê±´")
                else:
                    df_rows = []
                    for r in ch_results:
                        subs = r["subs"]
                        subs_text = f"{subs:,}" if isinstance(subs, int) else "-"
                        df_rows.append({
                            "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),  # ì±„ë„ ì•„ì´ì½˜
                            "ì±„ë„ëª…": r["channel_title"],
                            "êµ¬ë…ììˆ˜": subs_text,
                            "ì±„ë„ì¡°íšŒìˆ˜": f"{r['total_views']:,}",
                            "ì±„ë„ì˜ìƒìˆ˜": f"{r['videos']:,}",
                            "ë§í¬URL": r["url"],
                        })
                    df = pd.DataFrame(df_rows)
                    st.session_state.results_df = df
                    st.session_state.last_search_time = search_dt
                    st.session_state.search_mode = "channel_list"
                    status_placeholder.success(
                        f"[ì±„ë„ ëª©ë¡] ê²°ê³¼: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                    )

except Exception as e:
    st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
    st.session_state.results_df = None

# -----------------------------
# ê²°ê³¼ í‘œì‹œ (ë¦¬ìŠ¤íŠ¸ / ê·¸ë¦¬ë“œ / ì‡¼ì¸ )
# -----------------------------
df = st.session_state.results_df
mode = st.session_state.search_mode

if df is None or df.empty:
    st.info("ì•„ì§ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢Œì¸¡ì—ì„œ ì¡°ê±´ì„ ì„¤ì •í•˜ê³  ë²„íŠ¼ì„ ëˆŒëŸ¬ ê²€ìƒ‰í•˜ì„¸ìš”.")
else:
    df_display = df.copy()

    # URL ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬
    if "ë§í¬URL" in df_display.columns:
        df_display["ë§í¬"] = df_display["ë§í¬URL"]
        df_display = df_display.drop(columns=["ë§í¬URL"])

    # ----- ì •ë ¬ ì ìš© (ë¦¬ìŠ¤íŠ¸ë·° / ê·¸ë¦¬ë“œë·°ì¼ ë•Œë§Œ) -----
    if view_mode in ("ë¦¬ìŠ¤íŠ¸ë·°", "ê·¸ë¦¬ë“œë·°") and not df_display.empty:
        if sort_target != "ìë™":
            col_map = {
                "ë“±ê¸‰": "ë“±ê¸‰",
                "ì˜ìƒì¡°íšŒìˆ˜": "ì˜ìƒì¡°íšŒìˆ˜",
                "ì‹œê°„ë‹¹í´ë¦­": "ì‹œê°„ë‹¹í´ë¦­",
                "ì—…ë¡œë“œì‹œê°": "ì—…ë¡œë“œì‹œê°",
                "ì œëª©": "ì œëª©",
                "ì±„ë„ëª…": "ì±„ë„ëª…",
                "êµ¬ë…ììˆ˜": "êµ¬ë…ììˆ˜",
            }
            sort_col = col_map.get(sort_target)
            if sort_col and sort_col in df_display.columns:
                ascending = (sort_order == "ì˜¤ë¦„ì°¨ìˆœ")
                df_display = df_display.sort_values(
                    by=sort_col,
                    ascending=ascending,
                    kind="mergesort"  # ì•ˆì • ì •ë ¬
                )

    # ëª¨ë“œë³„ ì œëª©
    if mode == "general":
        st.subheader("ğŸ“Š ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸")
    elif mode == "trend":
        st.subheader("ğŸ”¥ íŠ¸ë Œë“œ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸")
    elif mode == "channel_videos":
        st.subheader("ğŸ¬ ì±„ë„ ì˜ìƒ ë¦¬ìŠ¤íŠ¸")
    elif mode == "channel_list":
        st.subheader("ğŸ“º ì±„ë„ê²€ìƒ‰ ë¦¬ìŠ¤íŠ¸")
    else:
        st.subheader("ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸")

    # ==================================================
    # 1) ì‡¼ì¸ ë·° (ì´ë¯¸ì§€ë§Œ ì´˜ì´˜í•˜ê²Œ)
    # ==================================================
    if view_mode == "ì‡¼ì¸ ë·°":
        if mode == "channel_list":
            # ì±„ë„ ì•„ì´ì½˜ ê·¸ë¦¬ë“œ (100x100, ë§ì´ ë³´ê¸°) - 4ì—´
            n_cols = 4
            cols = st.columns(n_cols)
            for idx, (_, row) in enumerate(df_display.iterrows()):
                col = cols[idx % n_cols]
                with col:
                    thumb = row.get("ì¸ë„¤ì¼", "")
                    if isinstance(thumb, str) and thumb:
                        st.image(thumb, width=100)
                    # ìº¡ì…˜ì€ ì‘ê²Œ
                    ch_name = row.get("ì±„ë„ëª…", "")
                    if ch_name:
                        st.caption(ch_name)
                if (idx + 1) % n_cols == 0 and (idx + 1) < len(df_display):
                    cols = st.columns(n_cols)
            st.caption("ğŸ“º ì±„ë„ ì•„ì´ì½˜ì„ í•œëˆˆì— ë³´ëŠ” ì‡¼ì¸  ë·°ì…ë‹ˆë‹¤.")
        else:
            # ì¼ë°˜/íŠ¸ë Œë“œ/ì±„ë„ì˜ìƒê²€ìƒ‰: 9:16 ë¹„ìœ¨, ì‘ì€ ì¸ë„¤ì¼ì„ ì´˜ì´˜í•˜ê²Œ (3ì—´)
            n_cols = 3
            cols = st.columns(n_cols)
            for idx, (_, row) in enumerate(df_display.iterrows()):
                col = cols[idx % n_cols]
                with col:
                    thumb = row.get("ì¸ë„¤ì¼", "")
                    if isinstance(thumb, str) and thumb:
                        # 9:16 ë¹„ìœ¨, ê°€ë¡œ 110px, ì„¸ë¡œ ~196px, ì—¬ë°± ìµœì†Œ
                        html = f"""
                        <div style="width:110px;height:196px;overflow:hidden;border-radius:8px;margin:2px auto;">
                          <img src="{thumb}" style="width:100%;height:100%;object-fit:cover;" />
                        </div>
                        """
                        st.markdown(html, unsafe_allow_html=True)
                if (idx + 1) % n_cols == 0 and (idx + 1) < len(df_display):
                    cols = st.columns(n_cols)
            st.caption("ğŸ ì‡¼ì¸  ì „ìš© ë·°: 9:16 ë¹„ìœ¨ë¡œ ì„¸ë¡œ ì¸ë„¤ì¼ì„ í™”ë©´ì— ë§ì´ ë³´ì—¬ì¤ë‹ˆë‹¤.")

    # ==================================================
    # 2) ê·¸ë¦¬ë“œë·° (ì¹´ë“œ í˜•ì‹) - ê¸°ë³¸ë·°
    # ==================================================
    elif view_mode == "ê·¸ë¦¬ë“œë·°":
        if mode == "channel_list":
            n_cols = 3
        else:
            n_cols = 3

        cols = st.columns(n_cols)

        for idx, (_, row) in enumerate(df_display.iterrows()):
            col = cols[idx % n_cols]
            with col:
                thumb = row.get("ì¸ë„¤ì¼", "")
                grade = row.get("ë“±ê¸‰", "")
                if grade and mode != "channel_list":
                    st.markdown(f"**ë“±ê¸‰: {grade}**")
                if isinstance(thumb, str) and thumb:
                    st.image(thumb, use_column_width=True)

                if mode == "channel_list":
                    # ì±„ë„ ì¹´ë“œ
                    title = row.get("ì±„ë„ëª…", "")
                    subs = row.get("êµ¬ë…ììˆ˜", "")
                    total_views = row.get("ì±„ë„ì¡°íšŒìˆ˜", "")
                    video_count = row.get("ì±„ë„ì˜ìƒìˆ˜", "")
                    link = row.get("ë§í¬", "")
                    st.markdown(f"**{title}**")
                    st.caption(f"êµ¬ë…ì: {subs} Â· ì¡°íšŒìˆ˜: {total_views} Â· ì˜ìƒìˆ˜: {video_count}")
                    if link:
                        st.markdown(f"[ì±„ë„ ì—´ê¸°]({link})")
                else:
                    # ì˜ìƒ ì¹´ë“œ
                    title = row.get("ì œëª©", "")
                    ch = row.get("ì±„ë„ëª…", "")
                    views = row.get("ì˜ìƒì¡°íšŒìˆ˜", "")
                    link = row.get("ë§í¬", "")
                    st.markdown(f"**{title}**")
                    if isinstance(views, (int, float)):
                        views_text = f"{views:,}"
                    else:
                        views_text = str(views)
                    st.caption(f"{ch} Â· ì¡°íšŒìˆ˜ {views_text}")
                    if link:
                        st.markdown(f"[ì˜ìƒ ì—´ê¸°]({link})")

            if (idx + 1) % n_cols == 0 and (idx + 1) < len(df_display):
                cols = st.columns(n_cols)

        st.caption("ğŸ‘‰ ì¹´ë“œì˜ ë§í¬ í…ìŠ¤íŠ¸ë¥¼ ëˆŒëŸ¬ì„œ ìƒˆ íƒ­ì—ì„œ ì—´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # ==================================================
    # 3) ë¦¬ìŠ¤íŠ¸ë·° (í…Œì´ë¸”)
    # ==================================================
    else:
        # ì»¬ëŸ¼ ìˆœì„œ (ë“±ê¸‰ì„ ì¸ë„¤ì¼ ì™¼ìª½ìœ¼ë¡œ)
        if mode in ("general", "trend", "channel_videos"):
            base_order = [
                "ë“±ê¸‰",
                "ì¸ë„¤ì¼",
                "ì±„ë„ëª…",
                "ì˜ìƒì¡°íšŒìˆ˜",
                "ì‹œê°„ë‹¹í´ë¦­",
                "ì˜ìƒê¸¸ì´",
                "ì—…ë¡œë“œì‹œê°",
                "ê²½ê³¼ì‹œê°„",
                "ì œëª©",
                "ë§í¬",
            ]
        else:
            base_order = [
                "ì¸ë„¤ì¼",
                "ì±„ë„ëª…",
                "êµ¬ë…ììˆ˜",
                "ì±„ë„ì¡°íšŒìˆ˜",
                "ì±„ë„ì˜ìƒìˆ˜",
                "ë§í¬",
            ]
        column_order = [c for c in base_order if c in df_display.columns]

        # ì»¬ëŸ¼ ì„¤ì • (ì¸ë„¤ì¼ì€ í•­ìƒ í‘œì‹œ)
        column_config = {}
        if "ë§í¬" in df_display.columns:
            column_config["ë§í¬"] = st.column_config.LinkColumn(
                "ì—´ê¸°",
                display_text="ì—´ê¸°",
            )
        if "ì¸ë„¤ì¼" in df_display.columns:
            column_config["ì¸ë„¤ì¼"] = st.column_config.ImageColumn(
                "ì¸ë„¤ì¼",
                help="ì¸ë„¤ì¼ ì´ë¯¸ì§€",
                width="small",
            )

        # ëª¨ë“œë³„ key (ì •ë ¬/ëˆˆì•„ì´ì½˜ ìƒíƒœ ë¶„ë¦¬)
        if mode == "general":
            editor_key = "video_results_editor_general"
        elif mode == "trend":
            editor_key = "video_results_editor_trend"
        elif mode == "channel_videos":
            editor_key = "video_results_editor_channel_videos"
        elif mode == "channel_list":
            editor_key = "channel_results_editor_keyword"
        else:
            editor_key = "results_editor_default"

        st.data_editor(
            df_display,
            use_container_width=True,
            height=650,    # âœ… í…Œì´ë¸” ë†’ì´ ì¦ê°€(í•œ í™”ë©´ì— ë” ë§ì€ í–‰ ë³´ì´ê²Œ)
            hide_index=True,
            column_order=column_order if column_order else None,
            column_config=column_config,
            key=editor_key,
            disabled=True,          # í¸ì§‘ ë¶ˆê°€
            num_rows="fixed",       # í–‰ ì¶”ê°€/ì‚­ì œ ë¶ˆê°€
        )

        st.caption("ğŸ‘‰ 'ì—´ê¸°' ë§í¬ë¥¼ ëˆ„ë¥´ë©´ ìƒˆ íƒ­ì—ì„œ ì˜ìƒ ë˜ëŠ” ì±„ë„ì´ ì—´ë¦½ë‹ˆë‹¤.")

# -----------------------------
# ì‚¬ì´ë“œë°” í•˜ë‹¨: ì¿¼í„° / ìµœê·¼ í‚¤ì›Œë“œ / ë¡œê·¸ì•„ì›ƒ
# -----------------------------
st.sidebar.markdown("---")

# ì˜¤ëŠ˜ ì‚¬ìš©í•œ ì¿¼í„° (ì‘ê²Œ)
quota_today = get_today_quota_total()
st.sidebar.caption(f"ì˜¤ëŠ˜ ì‚¬ìš© ì¿¼í„°: {quota_today:,} units")

# ìµœê·¼ í‚¤ì›Œë“œ (ìµœëŒ€ 7ê°œ, ë‚ ì§œ ì—†ì´)
recents = get_recent_keywords(7)
if recents:
    keywords = [q for _, q in recents]
    labels = [f"`{k}`" for k in keywords]
    st.sidebar.caption("ìµœê·¼ í‚¤ì›Œë“œ: " + " Â· ".join(labels))
else:
    st.sidebar.caption("ìµœê·¼ í‚¤ì›Œë“œ: ì—†ìŒ")

# ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼
if st.sidebar.button("ë¡œê·¸ì•„ì›ƒ", use_container_width=True):
    st.session_state["logged_in"] = False
    st.rerun()
