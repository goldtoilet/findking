#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
from datetime import datetime, timedelta, timezone

import streamlit as st
import pandas as pd

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

from supabase import create_client, Client

# -----------------------------
# ê¸°ë³¸ ì„¤ì •
# -----------------------------
st.set_page_config(
    page_title="youtubeê²€ìƒ‰ê¸°",
    page_icon="ğŸ”",
    layout="wide",
)

# ì œëª©ì„ ì¡°ê¸ˆ ì‘ê²Œ
st.markdown("### youtubeê²€ìƒ‰ê¸°")

# ì—¬ë°± ì‚´ì§ ì¡°ì •
st.markdown(
    """
    <style>
    .block-container { padding-top: 2rem !important; }
    </style>
    """,
    unsafe_allow_html=True,
)

# -----------------------------
# ê³µí†µ ìƒìˆ˜/í™˜ê²½
# -----------------------------
KST = timezone(timedelta(hours=9))
ENV_KEY_NAME = "YOUTUBE_API_KEY"
WEEKDAY_KO = ["ì›”ìš”ì¼","í™”ìš”ì¼","ìˆ˜ìš”ì¼","ëª©ìš”ì¼","ê¸ˆìš”ì¼","í† ìš”ì¼","ì¼ìš”ì¼"]

COUNTRY_LANG_MAP = {
    "í•œêµ­": ("KR", "ko"),
    "ì¼ë³¸": ("JP", "ja"),
    "ë¯¸êµ­": ("US", "en"),
    "ì˜êµ­": ("GB", "en"),
    "ë…ì¼": ("DE", "de"),
    "í”„ë‘ìŠ¤": ("FR", "fr"),
    "ìŠ¤í˜ì¸": ("ES", "es"),
    "ì´íƒˆë¦¬ì•„": ("IT", "it"),
    "ë¸Œë¼ì§ˆ": ("BR", "pt"),
    "ì¸ë„": ("IN", "en"),
    "í˜¸ì£¼": ("AU", "en"),
}
COUNTRY_LIST = list(COUNTRY_LANG_MAP.keys())

# -----------------------------
# Supabase í´ë¼ì´ì–¸íŠ¸
# -----------------------------
@st.cache_resource
def get_supabase_client() -> Client | None:
    url = st.secrets.get("SUPABASE_URL")
    key = st.secrets.get("SUPABASE_SERVICE_KEY")
    if not url or not key:
        return None
    return create_client(url, key)

SUPABASE_BUCKET = st.secrets.get("SUPABASE_BUCKET", "yts-config")
supabase = get_supabase_client()

def _load_json(filename: str, default):
    """Supabase Storageì—ì„œ JSON ë¡œë“œ"""
    if supabase is None:
        return default
    try:
        res = supabase.storage.from_(SUPABASE_BUCKET).download(filename)
        if res is None:
            return default
        if isinstance(res, bytes):
            text = res.decode("utf-8")
        else:
            text = str(res)
        return json.loads(text)
    except Exception:
        return default

def _save_json(filename: str, data):
    """Supabase Storageì— JSON ì €ì¥ (upsert)"""
    if supabase is None:
        return
    try:
        payload = json.dumps(data, ensure_ascii=False, indent=2).encode("utf-8")
        supabase.storage.from_(SUPABASE_BUCKET).upload(
            path=filename,
            file=payload,
            file_options={"content-type": "application/json", "x-upsert": "true"},
        )
    except Exception as e:
        st.warning(f"Supabase ì €ì¥ ì˜¤ë¥˜({filename}): {e}")

CONFIG_PATH       = "yts_config.json"
KEYWORD_LOG_PATH  = "yts_keyword_log.json"
QUOTA_PATH        = "yts_quota_usage.json"

# -----------------------------
# ë¡œê·¸ì¸ ìƒíƒœ
# -----------------------------
LOGIN_ID_ENV = st.secrets.get("LOGIN_ID", "")
LOGIN_PW_ENV = st.secrets.get("LOGIN_PW", "")

if "logged_in" not in st.session_state:
    st.session_state.logged_in = False

def login_form():
    st.sidebar.subheader("ë¡œê·¸ì¸")
    with st.sidebar.form("login_form"):
        user_id = st.text_input("ì•„ì´ë””", value="", key="login_id")
        user_pw = st.text_input("ë¹„ë°€ë²ˆí˜¸", value="", type="password", key="login_pw")
        submitted = st.form_submit_button("ë¡œê·¸ì¸")
    if submitted:
        if (user_id == LOGIN_ID_ENV) and (user_pw == LOGIN_PW_ENV):
            st.session_state.logged_in = True
            st.success("ë¡œê·¸ì¸ ì„±ê³µ!")
            st.experimental_rerun()
        else:
            st.error("ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")

def logout_button():
    if st.sidebar.button("ë¡œê·¸ì•„ì›ƒ", use_container_width=True):
        st.session_state.logged_in = False
        st.experimental_rerun()

# -----------------------------
# API í‚¤ ì„¤ì • (secrets ê¸°ë°˜)
# -----------------------------
DEFAULT_KEYS_FROM_SECRETS = st.secrets.get("YOUTUBE_API_KEYS", [])
if isinstance(DEFAULT_KEYS_FROM_SECRETS, str):
    DEFAULT_KEYS_FROM_SECRETS = [DEFAULT_KEYS_FROM_SECRETS]
_DEFAULT_API_KEYS = list(DEFAULT_KEYS_FROM_SECRETS)

def _load_api_keys_config():
    data = _load_json(CONFIG_PATH, {})
    keys = [k.strip() for k in data.get("api_keys", []) if k.strip()]
    if not keys:
        keys = _DEFAULT_API_KEYS[:]
    sel = data.get("selected_index", 0)
    sel = max(0, min(sel, len(keys)-1)) if keys else 0
    return {"api_keys": keys, "selected_index": sel}

def _save_api_keys_config(keys: list[str], selected_index: int):
    keys = [k.strip() for k in keys if k.strip()]
    selected_index = max(0, min(selected_index, len(keys)-1)) if keys else 0
    _save_json(CONFIG_PATH, {"api_keys": keys, "selected_index": selected_index})

if "api_keys_state" not in st.session_state:
    cfg = _load_api_keys_config()
    st.session_state.api_keys_state = {
        "keys": cfg["api_keys"],
        "index": cfg["selected_index"],
    }

def _apply_env_key(key: str):
    if key:
        os.environ[ENV_KEY_NAME] = key
    else:
        os.environ.pop(ENV_KEY_NAME, None)

def get_current_api_key() -> str:
    keys = st.session_state.api_keys_state["keys"]
    idx  = st.session_state.api_keys_state["index"]
    if not keys:
        return ""
    return keys[idx]

def set_current_api_index(idx: int):
    keys = st.session_state.api_keys_state["keys"]
    if not keys:
        return
    idx = max(0, min(idx, len(keys)-1))
    st.session_state.api_keys_state["index"] = idx
    _apply_env_key(keys[idx])
    _save_api_keys_config(keys, idx)

# ì•± ì‹œì‘ ì‹œ í˜„ì¬ ì¸ë±ìŠ¤ í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ì— ë°˜ì˜
_apply_env_key(get_current_api_key())

# -----------------------------
# ì¿¼í„° ê´€ë¦¬
# -----------------------------
def _load_quota_map():
    return _load_json(QUOTA_PATH, {})

def _save_quota_map(data: dict):
    _save_json(QUOTA_PATH, data)

def quota_today_key():
    return datetime.now(KST).strftime("%Y-%m-%d")

def get_today_quota_total() -> int:
    data = _load_quota_map()
    return int(data.get(quota_today_key(), 0))

def add_quota_usage(units: int):
    if units <= 0:
        return
    data = _load_quota_map()
    key = quota_today_key()
    data[key] = int(data.get(key, 0)) + int(units)
    _save_quota_map(data)

# -----------------------------
# í‚¤ì›Œë“œ ë¡œê·¸
# -----------------------------
def _load_keyword_log():
    return _load_json(KEYWORD_LOG_PATH, [])

def _save_keyword_log(entries: list):
    _save_json(KEYWORD_LOG_PATH, entries)

def append_keyword_log(query: str):
    q = (query or "").strip()
    if not q:
        return
    entries = _load_keyword_log()
    now = datetime.now(KST).isoformat(timespec="seconds")
    entries.append({"ts": now, "q": q})
    _save_keyword_log(entries)

def get_recent_keywords_simple(limit: int = 7):
    """ìµœê·¼ í‚¤ì›Œë“œ ë¬¸ìì—´ë§Œ ë°˜í™˜ (ë‚ ì§œ ì—†ì´, ìµœì‹ ìˆœ)"""
    entries = _load_keyword_log()
    out = []
    for item in entries:
        q = item.get("q")
        if q:
            out.append(q)
    # ë’¤ì—ì„œë¶€í„° ìµœê·¼ì´ë¯€ë¡œ ì—­ìˆœ
    out = out[::-1]
    return out[:limit]

# -----------------------------
# ì‹œê°„/í¬ë§· ìœ í‹¸
# -----------------------------
def format_k_datetime(dt_aw: datetime) -> str:
    if dt_aw.tzinfo is None:
        dt_aw = dt_aw.replace(tzinfo=KST)
    dt = dt_aw.astimezone(KST)
    wd = WEEKDAY_KO[dt.weekday()]
    h24 = dt.hour
    ampm = "ì˜¤ì „" if h24 < 12 else "ì˜¤í›„"
    h12 = h24 % 12 or 12
    return f"{dt.month}ì›”{dt.day}ì¼ {wd} {ampm}{h12}ì‹œ {dt.minute}ë¶„"

def parse_published_at_to_kst(published_iso: str) -> datetime:
    dt_utc = datetime.fromisoformat(published_iso.replace("Z", "+00:00"))
    return dt_utc.astimezone(KST)

def human_elapsed_days_hours(later: datetime, earlier: datetime) -> tuple[int, int]:
    delta = later - earlier
    if delta.total_seconds() < 0:
        return 0, 0
    days = delta.days
    hours = delta.seconds // 3600
    return days, hours

def published_after_from_label(label: str):
    label = label.strip()
    now_utc = datetime.now(timezone.utc)
    if label == "ì œí•œì—†ìŒ":
        return None
    if label.endswith("ì¼"):
        days = int(label[:-1])
        dt = now_utc - timedelta(days=days)
    else:
        return None
    return dt.isoformat(timespec="seconds").replace("+00:00", "Z")

def parse_duration_iso8601(iso_dur: str) -> int:
    h = m = s = 0
    if not iso_dur or not iso_dur.startswith("PT"):
        return 0
    num = ""
    for ch in iso_dur[2:]:
        if ch.isdigit():
            num += ch
        else:
            if ch == "H" and num:
                h = int(num); num = ""
            elif ch == "M" and num:
                m = int(num); num = ""
            elif ch == "S" and num:
                s = int(num); num = ""
    return h*3600 + m*60 + s

def format_duration_hms(seconds: int) -> str:
    if seconds <= 0:
        return "0:00"
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    if h > 0:
        return f"{h}:{m:02d}:{s:02d}"
    return f"{m}:{s:02d}"

def duration_filter_ok(seconds: int, label: str) -> bool:
    if label == "ì „ì²´": return True
    if label == "ì‡¼ì¸ ": return seconds < 60
    if label == "ë¡±í¼": return seconds >= 60
    if label == "1~20ë¶„": return 60 <= seconds < 20*60
    if label == "20~40ë¶„": return 20*60 <= seconds < 40*60
    if label == "40~60ë¶„": return 40*60 <= seconds < 60*60
    if label == "60ë¶„ì´ìƒ": return seconds >= 60*60
    return True

def parse_min_views(text: str) -> int:
    digits = text.replace(",", "").replace(" ", "").replace("ë§Œ", "0000")
    try:
        return int(digits)
    except Exception:
        return 0

# -----------------------------
# ë“±ê¸‰ ê³„ì‚° (A~Hë¡œ ë³€ê²½)
# -----------------------------
def calc_grade(clicks_per_hour: int) -> str:
    v = clicks_per_hour
    if v >= 5000: return "A"
    if v >= 2000: return "B"
    if v >= 1000: return "C"
    if v >= 500:  return "D"
    if v >= 300:  return "E"
    if v >= 100:  return "F"
    if v >= 50:   return "G"
    return "H"

# -----------------------------
# YouTube Client
# -----------------------------
def get_youtube_client():
    key = get_current_api_key()
    if not key:
        raise RuntimeError("YouTube API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
    try:
        return build("youtube", "v3", developerKey=key, cache_discovery=False)
    except TypeError:
        return build("youtube", "v3", developerKey=key)

# -----------------------------
# ê²€ìƒ‰ í•¨ìˆ˜ë“¤
# -----------------------------
def search_videos(
    query: str,
    min_views: int,
    api_period_label: str,
    duration_label: str,
    max_fetch: int,
    region_code: str | None,
    lang_code: str | None,
):
    """ì¼ë°˜ í‚¤ì›Œë“œë¡œ ì˜ìƒ ê²€ìƒ‰"""
    youtube = get_youtube_client()
    published_after = published_after_from_label(api_period_label)

    cost_used = 0
    breakdown = {"search.list": 0, "videos.list": 0}
    max_fetch = max(1, min(int(max_fetch or 100), 5000))

    results_tmp = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        kwargs = dict(
            q=query,
            part="id",
            type="video",
            maxResults=take,
        )
        if published_after:
            kwargs["publishedAfter"] = published_after
        if region_code:
            kwargs["regionCode"] = region_code
        if lang_code:
            kwargs["relevanceLanguage"] = lang_code
        if next_token:
            kwargs["pageToken"] = next_token

        try:
            search_response = youtube.search().list(**kwargs).execute()
            cost_used += 100; breakdown["search.list"] += 100
        except HttpError as e:
            raise RuntimeError(f"Search API ì˜¤ë¥˜: {e}")

        page_ids = [
            it["id"]["videoId"]
            for it in search_response.get("items", [])
            if "id" in it and "videoId" in it["id"]
        ]
        if not page_ids:
            break

        try:
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(page_ids),
            ).execute()
            cost_used += 1; breakdown["videos.list"] += 1
        except HttpError as e:
            raise RuntimeError(f"Videos API ì˜¤ë¥˜: {e}")

        items = video_response.get("items", [])
        for item in items:
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}

            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

            thumbs = snip.get("thumbnails", {}) or {}
            thumb_url = (
                (thumbs.get("maxres") or {}).get("url") or
                (thumbs.get("standard") or {}).get("url") or
                (thumbs.get("high") or {}).get("url") or
                (thumbs.get("medium") or {}).get("url") or
                (thumbs.get("default") or {}).get("url") or
                ""
            )

            if view_count < min_views:
                continue
            if not duration_filter_ok(duration_sec, duration_label):
                continue

            results_tmp.append({
                "title": title,
                "views": view_count,
                "published_at_iso": published_at_iso,
                "url": url,
                "duration_sec": duration_sec,
                "channel_title": snip.get("channelTitle", ""),
                "thumb_url": thumb_url,
            })

        fetched += len(page_ids)
        next_token = search_response.get("nextPageToken")
        if not next_token:
            break

    return results_tmp, cost_used, breakdown

def search_trending_videos(
    max_fetch: int,
    region_code: str | None,
):
    """íŠ¸ë Œë“œ(ì¸ê¸° ë™ì˜ìƒ) ê²€ìƒ‰ - chart=mostPopular"""
    youtube = get_youtube_client()
    max_fetch = max(1, min(int(max_fetch or 50), 50))  # YouTube ì œí•œ 50
    kwargs = dict(
        part="snippet,statistics,contentDetails",
        chart="mostPopular",
        maxResults=max_fetch,
    )
    if region_code:
        kwargs["regionCode"] = region_code

    try:
        resp = youtube.videos().list(**kwargs).execute()
        cost_used = 1
    except HttpError as e:
        raise RuntimeError(f"íŠ¸ë Œë“œ ê²€ìƒ‰ API ì˜¤ë¥˜: {e}")

    results = []
    for item in resp.get("items", []):
        vid = item.get("id", "")
        snip = item.get("snippet", {}) or {}
        stats = item.get("statistics", {}) or {}
        cdet = item.get("contentDetails", {}) or {}

        title = snip.get("title", "")
        published_at_iso = snip.get("publishedAt", "")
        view_count = int(stats.get("viewCount", 0))
        url = f"https://www.youtube.com/watch?v={vid}"
        duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

        thumbs = snip.get("thumbnails", {}) or {}
        thumb_url = (
            (thumbs.get("maxres") or {}).get("url") or
            (thumbs.get("standard") or {}).get("url") or
            (thumbs.get("high") or {}).get("url") or
            (thumbs.get("medium") or {}).get("url") or
            (thumbs.get("default") or {}).get("url") or
            ""
        )

        results.append({
            "title": title,
            "views": view_count,
            "published_at_iso": published_at_iso,
            "url": url,
            "duration_sec": duration_sec,
            "channel_title": snip.get("channelTitle", ""),
            "thumb_url": thumb_url,
        })

    return results, cost_used

def search_channels_by_keyword(
    keyword: str,
    max_results: int,
    region_code: str | None,
    lang_code: str | None,
):
    """í‚¤ì›Œë“œë¡œ ì±„ë„ ì°¾ê¸°"""
    youtube = get_youtube_client()
    take = max(1, min(max_results, 50))
    kwargs = dict(
        q=keyword,
        part="id",
        type="channel",
        maxResults=take,
    )
    if region_code:
        kwargs["regionCode"] = region_code
    if lang_code:
        kwargs["relevanceLanguage"] = lang_code

    try:
        search_response = youtube.search().list(**kwargs).execute()
        cost_used = 100
    except HttpError as e:
        raise RuntimeError(f"Channel search API ì˜¤ë¥˜: {e}")

    ch_ids = [
        it["id"]["channelId"]
        for it in search_response.get("items", [])
        if "id" in it and "channelId" in it["id"]
    ]
    if not ch_ids:
        return [], cost_used

    try:
        ch_resp = youtube.channels().list(
            part="snippet,statistics",
            id=",".join(ch_ids),
        ).execute()
        cost_used += 1
    except HttpError as e:
        raise RuntimeError(f"Channels API ì˜¤ë¥˜: {e}")

    results = []
    for c in ch_resp.get("items", []):
        cid = c.get("id", "")
        sn = c.get("snippet", {}) or {}
        stt = c.get("statistics", {}) or {}
        subs = int(stt.get("subscriberCount", 0)) if stt.get("subscriberCount") is not None else None
        total_views = int(stt.get("viewCount", 0))
        videos = int(stt.get("videoCount", 0))
        url = f"https://www.youtube.com/channel/{cid}" if cid else ""

        thumbs = sn.get("thumbnails", {}) or {}
        thumb_url = (
            (thumbs.get("high") or {}).get("url") or
            (thumbs.get("medium") or {}).get("url") or
            (thumbs.get("default") or {}).get("url") or
            ""
        )

        results.append({
            "channel_title": sn.get("title", ""),
            "subs": subs,
            "total_views": total_views,
            "videos": videos,
            "url": url,
            "thumb_url": thumb_url,
        })

    results.sort(key=lambda r: (r["subs"] or 0), reverse=True)
    return results, cost_used

def search_videos_in_channel(
    channel_name: str,
    api_period_label: str,
    duration_label: str,
    max_fetch: int,
    region_code: str | None,
    lang_code: str | None,
):
    """ì±„ë„ ì´ë¦„ìœ¼ë¡œ ì±„ë„ì„ ì°¾ê³ , ê·¸ ì±„ë„ì˜ ì˜ìƒì„ ê²€ìƒ‰"""
    youtube = get_youtube_client()
    # 1) ì±„ë„ ì°¾ê¸°
    kwargs_ch = dict(
        q=channel_name,
        part="id",
        type="channel",
        maxResults=1,
    )
    if region_code:
        kwargs_ch["regionCode"] = region_code
    if lang_code:
        kwargs_ch["relevanceLanguage"] = lang_code

    try:
        ch_resp = youtube.search().list(**kwargs_ch).execute()
        cost_used = 100
    except HttpError as e:
        raise RuntimeError(f"ì±„ë„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

    items = ch_resp.get("items", [])
    if not items:
        return [], cost_used

    channel_id = items[0]["id"]["channelId"]
    published_after = published_after_from_label(api_period_label)

    # 2) í•´ë‹¹ ì±„ë„ì˜ ì˜ìƒ ê²€ìƒ‰
    max_fetch = max(1, min(int(max_fetch or 100), 5000))
    results_tmp = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        kwargs = dict(
            part="id",
            type="video",
            channelId=channel_id,
            maxResults=take,
            order="date",
        )
        if published_after:
            kwargs["publishedAfter"] = published_after
        if region_code:
            kwargs["regionCode"] = region_code
        if lang_code:
            kwargs["relevanceLanguage"] = lang_code
        if next_token:
            kwargs["pageToken"] = next_token

        try:
            v_search = youtube.search().list(**kwargs).execute()
            cost_used += 100
        except HttpError as e:
            raise RuntimeError(f"ì±„ë„ ì˜ìƒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

        page_ids = [
            it["id"]["videoId"]
            for it in v_search.get("items", [])
            if "id" in it and "videoId" in it["id"]
        ]
        if not page_ids:
            break

        try:
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(page_ids),
            ).execute()
            cost_used += 1
        except HttpError as e:
            raise RuntimeError(f"Videos API ì˜¤ë¥˜: {e}")

        for item in video_response.get("items", []):
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}

            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

            thumbs = snip.get("thumbnails", {}) or {}
            thumb_url = (
                (thumbs.get("maxres") or {}).get("url") or
                (thumbs.get("standard") or {}).get("url") or
                (thumbs.get("high") or {}).get("url") or
                (thumbs.get("medium") or {}).get("url") or
                (thumbs.get("default") or {}).get("url") or
                ""
            )

            if not duration_filter_ok(duration_sec, duration_label):
                continue

            results_tmp.append({
                "title": title,
                "views": view_count,
                "published_at_iso": published_at_iso,
                "url": url,
                "duration_sec": duration_sec,
                "channel_title": snip.get("channelTitle", ""),
                "thumb_url": thumb_url,
            })

        fetched += len(page_ids)
        next_token = v_search.get("nextPageToken")
        if not next_token:
            break

    return results_tmp, cost_used

# -----------------------------
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# -----------------------------
if "results_df" not in st.session_state:
    st.session_state.results_df = None
    st.session_state.last_search_time = None
    st.session_state.search_mode = None   # "normal", "trend", "channel_videos", "channel_keyword"

# ============================================================
# ğŸ” ë¡œê·¸ì¸ ì²˜ë¦¬
# ============================================================
if not st.session_state.logged_in:
    login_form()
    st.stop()

# ============================================================
# ğŸ§­ ì‚¬ì´ë“œë°” UI
# ============================================================
# 1) ê²€ìƒ‰ì–´ ì…ë ¥ë“¤
st.sidebar.subheader("ê²€ìƒ‰")

query = st.sidebar.text_input("ì¼ë°˜ ê²€ìƒ‰ì–´", "")
st.sidebar.markdown("---")

# íŠ¸ë Œë“œ, ì±„ë„ì˜ìƒ, í‚¤ì›Œë“œì±„ë„ ê²€ìƒ‰ì€ disclosure(ì ‘ê¸°) ë°©ì‹
with st.sidebar.expander("íŠ¸ë Œë“œ ê²€ìƒ‰", expanded=False):
    use_trend = st.checkbox("íŠ¸ë Œë“œ ê²€ìƒ‰ ì‹¤í–‰", value=False, key="use_trend_flag")

with st.sidebar.expander("ì±„ë„ì˜ìƒê²€ìƒ‰", expanded=False):
    channel_name_for_videos = st.text_input("ì±„ë„ ì´ë¦„", key="channel_name_for_videos")

with st.sidebar.expander("í‚¤ì›Œë“œì±„ë„ê²€ìƒ‰", expanded=False):
    channel_keyword = st.text_input("ì±„ë„ í‚¤ì›Œë“œ", key="channel_keyword_for_search")

st.sidebar.markdown("---")

# 2) ì„¸ë¶€ í•„í„°
with st.sidebar.expander("ì„¸ë¶€ í•„í„°", expanded=False):
    api_period = st.selectbox(
        "ì„œë²„ ê²€ìƒ‰ê¸°ê°„",
        ["ì œí•œì—†ìŒ","90ì¼","150ì¼","365ì¼","730ì¼","1095ì¼","1825ì¼","3650ì¼"],
        index=1,
    )
    upload_period = st.selectbox(
        "ì—…ë¡œë“œ ê¸°ê°„(í´ë¼ì´ì–¸íŠ¸ í•„í„°)",
        ["ì œí•œì—†ìŒ","1ì¼","3ì¼","7ì¼","14ì¼","30ì¼","60ì¼","90ì¼","180ì¼","365ì¼"],
        index=6,
    )
    min_views_label = st.selectbox(
        "ìµœì†Œ ì¡°íšŒìˆ˜",
        ["5,000","10,000","25,000","50,000","100,000","200,000","500,000","1,000,000"],
        index=0,
    )
    duration_label = st.selectbox(
        "ì˜ìƒ ê¸¸ì´",
        ["ì „ì²´","ì‡¼ì¸ ","ë¡±í¼","1~20ë¶„","20~40ë¶„","40~60ë¶„","60ë¶„ì´ìƒ"],
        index=0,
    )
    max_fetch = st.number_input("ê°€ì ¸ì˜¬ ìµœëŒ€ ê°œìˆ˜", 1, 5000, 50, step=10)
    country_name = st.selectbox("êµ­ê°€/ì–¸ì–´", COUNTRY_LIST, index=0)
    region_code, lang_code = COUNTRY_LANG_MAP[country_name]

# 3) ì¸ë„¤ì¼ / ë·° ì˜µì…˜
st.sidebar.markdown("---")
show_thumbs = st.sidebar.checkbox("ì¸ë„¤ì¼ ë³´ê¸°", value=True, key="show_thumbnails")
grid_view = st.sidebar.checkbox("ê·¸ë¦¬ë“œ ë³´ê¸°", value=False, key="grid_view")
shorts_view = st.sidebar.checkbox("ì‡¼ì¸  ë³´ê¸°", value=False, key="shorts_view")

# 4) ì˜¤ëŠ˜ ì¿¼í„° (ì•„ë˜ìª½ì— ì‘ê²Œ)
st.sidebar.markdown("---")
st.sidebar.caption(f"ì˜¤ëŠ˜ ì‚¬ìš©í•œ ì¿¼í„°: {get_today_quota_total():,} units")

# 5) ìµœê·¼ í‚¤ì›Œë“œ (ë§¨ ì•„ë˜ ê·¼ì²˜)
with st.sidebar.expander("ìµœê·¼ í‚¤ì›Œë“œ", expanded=False):
    recent_qs = get_recent_keywords_simple(limit=7)
    if not recent_qs:
        st.caption("ìµœê·¼ ê²€ìƒ‰ ì—†ìŒ")
    else:
        for q in recent_qs:
            st.caption(f"- {q}")

# ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼
logout_button()

# ============================================================
# ğŸ” ê²€ìƒ‰ ì‹¤í–‰ ë²„íŠ¼ (ì¼ë°˜ê²€ìƒ‰ìš©)
# ============================================================
col_left, col_right = st.columns([2, 1])

with col_left:
    do_search = st.button("ê²€ìƒ‰ ì‹¤í–‰", type="primary", use_container_width=True)
with col_right:
    status_placeholder = st.empty()

# ============================================================
# ğŸ§ª í´ë¼ì´ì–¸íŠ¸ í•„í„° í•¨ìˆ˜
# ============================================================
def apply_client_filters(df: pd.DataFrame, upload_period: str, min_views_label: str) -> pd.DataFrame:
    # ì—…ë¡œë“œ ê¸°ê°„
    if upload_period != "ì œí•œì—†ìŒ" and "ì—…ë¡œë“œì‹œê°" in df.columns:
        days = int(upload_period.replace("ì¼",""))
        cutoff = datetime.now(KST) - timedelta(days=days)
        df = df[df["ì—…ë¡œë“œì‹œê°"] >= cutoff]
    # ìµœì†Œ ì¡°íšŒìˆ˜
    min_views = parse_min_views(min_views_label)
    if "ì˜ìƒì¡°íšŒìˆ˜" in df.columns:
        df = df[df["ì˜ìƒì¡°íšŒìˆ˜"] >= min_views]
    return df

# ============================================================
# ğŸ” ê²€ìƒ‰ ë¡œì§
# ============================================================
if do_search:
    base_query = (query or "").strip()
    ch_keyword = (channel_keyword or "").strip()
    ch_name_for_videos = (channel_name_for_videos or "").strip()

    # ì–´ë–¤ ëª¨ë“œë¡œ ê²€ìƒ‰í• ì§€ ìš°ì„ ìˆœìœ„:
    # 1) íŠ¸ë Œë“œ ê²€ìƒ‰ ì²´í¬
    # 2) ì±„ë„ì˜ìƒê²€ìƒ‰ (ì±„ë„ ì´ë¦„)
    # 3) í‚¤ì›Œë“œì±„ë„ê²€ìƒ‰
    # 4) ì¼ë°˜ ê²€ìƒ‰ (query)
    try:
        if st.session_state.get("use_trend_flag", False):
            # íŠ¸ë Œë“œ ê²€ìƒ‰
            append_keyword_log("[trend]")
            status_placeholder.info("íŠ¸ë Œë“œ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
            raw_results, cost_used = search_trending_videos(
                max_fetch=max_fetch,
                region_code=region_code,
            )
            search_dt = datetime.now(KST)
            rows = []
            for r in raw_results:
                pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                d, h = human_elapsed_days_hours(search_dt, pub_kst)
                total_hours = max(1, d*24 + h)
                cph = int(round(r["views"] / total_hours)) if total_hours > 0 else 0
                rows.append({
                    "ì¸ë„¤ì¼URL": r["thumb_url"],
                    "ì±„ë„ëª…": r["channel_title"],
                    "ë“±ê¸‰": calc_grade(cph),
                    "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                    "ì‹œê°„ë‹¹í´ë¦­": cph,
                    "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                    "ì—…ë¡œë“œì‹œê°": pub_kst,
                    "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                    "ì œëª©": r["title"],
                    "URL": r["url"],
                })
            df = pd.DataFrame(rows)
            if not df.empty:
                df = apply_client_filters(df, upload_period, min_views_label)
            st.session_state.results_df = df
            st.session_state.last_search_time = search_dt
            st.session_state.search_mode = "trend"
            status_placeholder.success(
                f"íŠ¸ë Œë“œ ì„œë²„ ê²°ê³¼: {len(raw_results):,}ê±´ / í•„í„° í›„: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
            )
            add_quota_usage(cost_used)

        elif ch_name_for_videos:
            # ì±„ë„ì˜ìƒê²€ìƒ‰
            append_keyword_log(f"[channel_videos]{ch_name_for_videos}")
            status_placeholder.info("ì±„ë„ ì˜ìƒ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
            raw_results, cost_used = search_videos_in_channel(
                channel_name=ch_name_for_videos,
                api_period_label=api_period,
                duration_label=duration_label,
                max_fetch=max_fetch,
                region_code=region_code,
                lang_code=lang_code,
            )
            if not raw_results:
                st.session_state.results_df = None
                st.session_state.search_mode = "channel_videos"
                status_placeholder.info("ì±„ë„ ì˜ìƒ ê²°ê³¼ 0ê±´")
            else:
                search_dt = datetime.now(KST)
                rows = []
                for r in raw_results:
                    pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                    d, h = human_elapsed_days_hours(search_dt, pub_kst)
                    total_hours = max(1, d*24 + h)
                    cph = int(round(r["views"] / total_hours)) if total_hours > 0 else 0
                    rows.append({
                        "ì¸ë„¤ì¼URL": r["thumb_url"],
                        "ì±„ë„ëª…": r["channel_title"],
                        "ë“±ê¸‰": calc_grade(cph),
                        "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                        "ì‹œê°„ë‹¹í´ë¦­": cph,
                        "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                        "ì—…ë¡œë“œì‹œê°": pub_kst,
                        "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                        "ì œëª©": r["title"],
                        "URL": r["url"],
                    })
                df = pd.DataFrame(rows)
                if not df.empty:
                    df = apply_client_filters(df, upload_period, min_views_label)
                st.session_state.results_df = df
                st.session_state.last_search_time = search_dt
                st.session_state.search_mode = "channel_videos"
                status_placeholder.success(
                    f"ì±„ë„ ì˜ìƒ ì„œë²„ ê²°ê³¼: {len(raw_results):,}ê±´ / í•„í„° í›„: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                )
            add_quota_usage(cost_used)

        elif ch_keyword:
            # í‚¤ì›Œë“œì±„ë„ê²€ìƒ‰
            append_keyword_log(f"[channel]{ch_keyword}")
            status_placeholder.info("í‚¤ì›Œë“œë¡œ ì±„ë„ ê²€ìƒ‰ ì¤‘...")
            ch_results, cost_used = search_channels_by_keyword(
                keyword=ch_keyword,
                max_results=max_fetch,
                region_code=region_code,
                lang_code=lang_code,
            )
            rows = []
            for r in ch_results:
                subs = r["subs"]
                subs_text = f"{subs:,}" if isinstance(subs, int) else "-"
                rows.append({
                    "ì¸ë„¤ì¼URL": r["thumb_url"],
                    "ì±„ë„ëª…": r["channel_title"],
                    "êµ¬ë…ììˆ˜": subs_text,
                    "ì±„ë„ì¡°íšŒìˆ˜": f"{r['total_views']:,}",
                    "ì±„ë„ì˜ìƒìˆ˜": f"{r['videos']:,}",
                    "URL": r["url"],
                })
            df = pd.DataFrame(rows)
            st.session_state.results_df = df
            st.session_state.last_search_time = datetime.now(KST)
            st.session_state.search_mode = "channel_keyword"
            status_placeholder.success(
                f"í‚¤ì›Œë“œ ì±„ë„ ê²°ê³¼: {len(ch_results):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
            )
            add_quota_usage(cost_used)

        else:
            # ì¼ë°˜ ê²€ìƒ‰
            if not base_query:
                st.warning("ì¼ë°˜ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ê±°ë‚˜, ë‹¤ë¥¸ ê²€ìƒ‰ ëª¨ë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            else:
                append_keyword_log(base_query)
                status_placeholder.info("ì¼ë°˜ ì˜ìƒ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
                raw_results, cost_used, breakdown = search_videos(
                    query=base_query,
                    min_views=parse_min_views(min_views_label),
                    api_period_label=api_period,
                    duration_label=duration_label,
                    max_fetch=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
                if not raw_results:
                    st.session_state.results_df = None
                    st.session_state.search_mode = "normal"
                    status_placeholder.info("ì„œë²„ ê²°ê³¼ 0ê±´")
                else:
                    search_dt = datetime.now(KST)
                    rows = []
                    for r in raw_results:
                        pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                        d, h = human_elapsed_days_hours(search_dt, pub_kst)
                        total_hours = max(1, d*24 + h)
                        cph = int(round(r["views"] / total_hours)) if total_hours > 0 else 0
                        rows.append({
                            "ì¸ë„¤ì¼URL": r["thumb_url"],
                            "ì±„ë„ëª…": r["channel_title"],
                            "ë“±ê¸‰": calc_grade(cph),
                            "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                            "ì‹œê°„ë‹¹í´ë¦­": cph,
                            "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                            "ì—…ë¡œë“œì‹œê°": pub_kst,
                            "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                            "ì œëª©": r["title"],
                            "URL": r["url"],
                        })
                    df = pd.DataFrame(rows)
                    if not df.empty:
                        df = apply_client_filters(df, upload_period, min_views_label)
                    st.session_state.results_df = df
                    st.session_state.last_search_time = search_dt
                    st.session_state.search_mode = "normal"
                    status_placeholder.success(
                        f"ì¼ë°˜ ê²€ìƒ‰ ì„œë²„ ê²°ê³¼: {len(raw_results):,}ê±´ / í•„í„° í›„: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                    )
                add_quota_usage(cost_used)

    except Exception as e:
        st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        st.session_state.results_df = None

# ============================================================
# ğŸ“Š ê²°ê³¼ í‘œì‹œ (í…Œì´ë¸” / ê·¸ë¦¬ë“œ / ì‡¼ì¸ )
# ============================================================
df = st.session_state.results_df
mode = st.session_state.search_mode

if df is None or df.empty:
    st.info("ì•„ì§ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢Œì¸¡ì—ì„œ ì¡°ê±´ì„ ì„¤ì •í•˜ê³  **[ê²€ìƒ‰ ì‹¤í–‰]** ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
else:
    # ì–´ë–¤ ë·° ëª¨ë“œì¸ì§€ ê²°ì •
    view_mode = "table"   # ê¸°ë³¸: í…Œì´ë¸”
    if show_thumbs:
        if shorts_view:
            view_mode = "shorts"
        elif grid_view:
            view_mode = "grid"

    # ëª¨ë“œë³„ ì œëª©
    if mode == "normal":
        title_text = "ğŸ“Š ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸"
    elif mode == "trend":
        title_text = "ğŸ“Š íŠ¸ë Œë“œ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸"
    elif mode == "channel_videos":
        title_text = "ğŸ“Š ì±„ë„ ì˜ìƒ ë¦¬ìŠ¤íŠ¸"
    elif mode == "channel_keyword":
        title_text = "ğŸ“Š ì±„ë„ê²€ìƒ‰ ë¦¬ìŠ¤íŠ¸"
    else:
        title_text = "ğŸ“Š ê²°ê³¼ ë¦¬ìŠ¤íŠ¸"

    st.subheader(title_text)

    # ===== 1) í…Œì´ë¸” ë·° =====
    if view_mode == "table":
        df_display = df.copy()

        # ì¸ë„¤ì¼URLì€ í…Œì´ë¸” ëª¨ë“œì—ì„œëŠ” ì¼ë‹¨ ìˆ¨ê¸°ê³  ì‹¶ë‹¤ë©´ drop ê°€ëŠ¥
        if "ì¸ë„¤ì¼URL" in df_display.columns:
            df_display = df_display.drop(columns=["ì¸ë„¤ì¼URL"])

        # URL ì»¬ëŸ¼ì„ ê·¸ëŒ€ë¡œ ë‘ì–´ ëª¨ë°”ì¼ì—ì„œë„ ë§í¬ íƒ­ìœ¼ë¡œ ì´ë™ ê°€ëŠ¥
        # í¸ì§‘ì€ ì•ˆë˜ë„ë¡ dataframe ì‚¬ìš©
        st.dataframe(
            df_display,
            use_container_width=True,
        )
        st.caption("URL ì»¬ëŸ¼ì„ í´ë¦­í•˜ë©´ ìƒˆ íƒ­ì—ì„œ ì˜ìƒ ë˜ëŠ” ì±„ë„ì„ ì—´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # ===== 2) ê·¸ë¦¬ë“œ ë·° =====
    elif view_mode == "grid":
        # ì˜ìƒ/ì±„ë„ ê³µí†µ
        if "ì¸ë„¤ì¼URL" not in df.columns:
            st.warning("ì¸ë„¤ì¼ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. í…Œì´ë¸” ë³´ê¸°ë¡œ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            # ì˜ìƒ/ì±„ë„ì— ë”°ë¼ ì„¤ëª… í…ìŠ¤íŠ¸ ë‹¤ë¥´ê²Œ êµ¬ì„±
            if mode in ("normal", "trend", "channel_videos"):
                st.caption("ì¹´ë“œë¥¼ ëˆŒëŸ¬ë„ ì•„ë¬´ ë™ì‘ë„ í•˜ì§€ ì•Šê³ , ì•„ë˜ ë§í¬ë¥¼ í†µí•´ ì—´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                cols_per_row = 3
                cols = st.columns(cols_per_row)
                for i, row in df.iterrows():
                    thumb = row.get("ì¸ë„¤ì¼URL", "")
                    title = row.get("ì œëª©", "")
                    ch = row.get("ì±„ë„ëª…", "")
                    url = row.get("URL", "")
                    views = row.get("ì˜ìƒì¡°íšŒìˆ˜", "")
                    grade = row.get("ë“±ê¸‰", "")
                    with cols[i % cols_per_row]:
                        if thumb:
                            st.image(thumb, use_column_width=True)
                        if title:
                            st.markdown(f"**{title}**")
                        if ch:
                            st.caption(f"ì±„ë„: {ch}")
                        extra = []
                        if grade:
                            extra.append(f"ë“±ê¸‰ {grade}")
                        if isinstance(views, (int, float)):
                            extra.append(f"ì¡°íšŒìˆ˜ {int(views):,}")
                        elif isinstance(views, str) and views:
                            extra.append(f"ì¡°íšŒìˆ˜ {views}")
                        if extra:
                            st.caption(" Â· ".join(extra))
                        if url:
                            st.markdown(f"[ì—´ê¸°]({url})")
            else:
                # í‚¤ì›Œë“œì±„ë„ê²€ìƒ‰: ì±„ë„ í”„ë¡œí•„ ì¸ë„¤ì¼
                st.caption("í‚¤ì›Œë“œë¡œ ì°¾ì€ ì±„ë„ë“¤ì…ë‹ˆë‹¤. ì¹´ë“œë¥¼ ëˆŒëŸ¬ë„ ì•„ë¬´ ë™ì‘ë„ í•˜ì§€ ì•Šê³ , ì•„ë˜ ë§í¬ë¥¼ í†µí•´ ì—´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                cols_per_row = 3
                cols = st.columns(cols_per_row)
                for i, row in df.iterrows():
                    thumb = row.get("ì¸ë„¤ì¼URL", "")
                    name = row.get("ì±„ë„ëª…", "")
                    subs = row.get("êµ¬ë…ììˆ˜", "")
                    total_views = row.get("ì±„ë„ì¡°íšŒìˆ˜", "")
                    videos = row.get("ì±„ë„ì˜ìƒìˆ˜", "")
                    url = row.get("URL", "")
                    with cols[i % cols_per_row]:
                        if thumb:
                            st.image(thumb, use_column_width=True)
                        if name:
                            st.markdown(f"**{name}**")
                        detail = []
                        if subs: detail.append(f"êµ¬ë…ì {subs}")
                        if total_views: detail.append(f"ì¡°íšŒìˆ˜ {total_views}")
                        if videos: detail.append(f"ì˜ìƒ {videos}")
                        if detail:
                            st.caption(" Â· ".join(detail))
                        if url:
                            st.markdown(f"[ì±„ë„ ì—´ê¸°]({url})")

    # ===== 3) ì‡¼ì¸  ë·° =====
    elif view_mode == "shorts":
        # ì‡¼ì¸  ëŠë‚Œ: ë” ë§ì€ ì¸ë„¤ì¼ì„ í•œ í™”ë©´ì— (4ì—´ ì •ë„)
        if "ì¸ë„¤ì¼URL" not in df.columns:
            st.warning("ì¸ë„¤ì¼ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. í…Œì´ë¸” ë³´ê¸°ë¡œ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            if mode in ("normal", "trend", "channel_videos"):
                st.caption("ì‡¼ì¸  ë³´ê¸°: ì„¸ë¡œ ìŠ¤í¬ë¡¤ë¡œ ë§ì€ ì˜ìƒì„ í•œ ë²ˆì— í›‘ì–´ë³´ëŠ” ë ˆì´ì•„ì›ƒì…ë‹ˆë‹¤.")
                cols_per_row = 4
                cols = st.columns(cols_per_row)
                for i, row in df.iterrows():
                    thumb = row.get("ì¸ë„¤ì¼URL", "")
                    title = row.get("ì œëª©", "")
                    url = row.get("URL", "")
                    with cols[i % cols_per_row]:
                        if thumb:
                            # ì‹¤ì œ ì¸ë„¤ì¼ì€ 16:9 ì´ì§€ë§Œ, ëª¨ë°”ì¼ì—ì„œë„ ì´˜ì´˜íˆ ë³´ì´ë„ë¡ í­ë§Œ ë§ì¶°ì„œ í‘œì‹œ
                            st.image(thumb, use_column_width=True)
                        # ì œëª©ì€ 1~2ì¤„ ì •ë„ë§Œ ë³´ì´ë„ë¡ ì§§ê²Œ
                        if title:
                            short_title = title if len(title) <= 40 else title[:37] + "..."
                            st.caption(short_title)
                        if url:
                            st.markdown(f"[ì—´ê¸°]({url})")
            else:
                # í‚¤ì›Œë“œì±„ë„ê²€ìƒ‰ì˜ ì‡¼ì¸ ë·°: ì±„ë„ í”„ë¡œí•„ ê·¸ë¦¬ë“œ
                st.caption("í‚¤ì›Œë“œë¡œ ì°¾ì€ ì±„ë„ë“¤ì˜ ì‡¼ì¸ í˜• ê·¸ë¦¬ë“œì…ë‹ˆë‹¤.")
                cols_per_row = 4
                cols = st.columns(cols_per_row)
                for i, row in df.iterrows():
                    thumb = row.get("ì¸ë„¤ì¼URL", "")
                    name = row.get("ì±„ë„ëª…", "")
                    url = row.get("URL", "")
                    with cols[i % cols_per_row]:
                        if thumb:
                            st.image(thumb, use_column_width=True)
                        if name:
                            short_name = name if len(name) <= 24 else name[:21] + "..."
                            st.caption(short_name)
                        if url:
                            st.markdown(f"[ì±„ë„ ì—´ê¸°]({url})")
